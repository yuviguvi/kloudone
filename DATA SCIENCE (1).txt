Data Science

Basic Level-Q&A:LEVEL-1
1. Can you enumerate the various differences between Supervised and Unsupervised Learning?
Answer: Supervised learning is a type of machine learning where a function is inferred from labeled training data. The training data contains a set of training examples.
Unsupervised learning, on the other hand, is a type of machine learning where inferences are drawn from datasets containing input data without labeled responses. Following are the various other differences between the two types of machine learning:
•	Algorithms Used – Supervised learning makes use of Decision Trees, K-nearest Neighbor algorithm, Neural Networks, Regression, and Support Vector Machines. Unsupervised learning uses Anomaly Detection, Clustering, Latent Variable Models, and Neural Networks.
•	Enables – Supervised learning enables classification and regression, whereas unsupervised learning enables classification, dimension reduction, and density estimation
•	Use – While supervised learning is used for prediction, unsupervised learning finds use in analysis
2. What do you understand by the Selection Bias? What are its various types?
Answer: Selection bias is typically associated with research that doesn’t have a random selection of participants. It is a type of error that occurs when a researcher decides who is going to be studied. On some occasions, selection bias is also referred to as the selection effect.
In other words, selection bias is a distortion of statistical analysis that results from the sample collecting method. When selection bias is not taken into account, some conclusions made by a research study might not be accurate. Following are the various types of selection bias:
•	Sampling Bias – A systematic error resulting due to a non-random sample of a populace causing certain members of the same to be less likely included than others that results in a biased sample.
•	Time Interval – A trial might be ended at an extreme value, usually due to ethical reasons, but the extreme value is most likely to be reached by the variable with the most variance, even though all variables have a similar mean.
•	Data – Results when specific data subsets are selected for supporting a conclusion or rejection of bad data arbitrarily.
•	Attrition – Caused due to attrition, i.e. loss of participants, discounting trial subjects or tests that didn’t run to completion.
3. Please explain the goal of A/B Testing.
Answer: A/B Testing is a statistical hypothesis testing meant for a randomized experiment with two variables, A and B. The goal of A/B Testing is to maximize the likelihood of an outcome of some interest by identifying any changes to a webpage.
A highly reliable method for finding out the best online marketing and promotional strategies for a business, A/B Testing can be employed for testing everything, ranging from sales emails to search ads and website copy.
4. How will you calculate the Sensitivity of machine learning models?
Answer: In machine learning, Sensitivity is used for validating the accuracy of a classifier, such as Logistic, Random Forest, and SVM. It is also known as REC (recall) or TPR (true positive rate).
Sensitivity can be defined as the ratio of predicted true events and total events i.e.:
Sensitivity = True Positives / Positives in Actual Dependent Variable
Here, true events are those events that were true as predicted by a machine learning model. The best sensitivity is 1.0 and the worst sensitivity is 0.0.
5. Could you draw a comparison between overfitting and underfitting?
Answer: In order to make reliable predictions on general untrained data in machine learning and statistics, it is required to fit a (machine learning) model to a set of training data. Overfitting and underfitting are two of the most common modeling errors that occur while doing so.
Following are the various differences between overfitting and underfitting:
•	Definition - A statistical model suffering from overfitting describes some random error or noise in place of the underlying relationship. When underfitting occurs, a statistical model or machine learning algorithm fails in capturing the underlying trend of the data.
•	Occurrence – When a statistical model or machine learning algorithm is excessively complex, it can result in overfitting. Example of a complex model is one having too many parameters when compared to the total number of observations. Underfitting occurs when trying to fit a linear model to non-linear data.
•	Poor Predictive Performance – Although both overfitting and underfitting yield poor predictive performance, the way in which each one of them does so is different. While the overfitted model overreacts to minor fluctuations in the training data, the underfit model under-reacts to even bigger fluctuations.
6. Between Python and R, which one would you pick for text analytics and why?
Answer: For text analytics, Python will gain an upper hand over R due to these reasons:
•	The Pandas library in Python offers easy-to-use data structures as well as high-performance data analysis tools
•	Python has a faster performance for all types of text analytics
•	R is a best-fit for machine learning than mere text analysis
7. Please explain the role of data cleaning in data analysis.
Answer: Data cleaning can be a daunting task due to the fact that with the increase in the number of data sources, the time required for cleaning the data increases at an exponential rate.
This is due to the vast volume of data generated by additional sources. Also, data cleaning can solely take up to 80% of the total time required for carrying out a data analysis task.
Nevertheless, there are several reasons for using data cleaning in data analysis. Two of the most important ones are:
•	Cleaning data from different sources helps in transforming the data into a format that is easy to work with
•	Data cleaning increases the accuracy of a machine learning model
8. What do you mean by cluster sampling and systematic sampling?
Answer: When studying the target population spread throughout a wide area becomes difficult and applying simple random sampling becomes ineffective, the technique of cluster sampling is used. A cluster sample is a probability sample, in which each of the sampling units is a collection or cluster of elements.
Following the technique of systematic sampling, elements are chosen from an ordered sampling frame. The list is advanced in a circular fashion. This is done in such a way so that once the end of the list is reached, the same is progressed from the start, or top, again.
9. Please explain Eigenvectors and Eigenvalues.
Answer: Eigenvectors help in understanding linear transformations. They are calculated typically for a correlation or covariance matrix in data analysis.
In other words, eigenvectors are those directions along which some particular linear transformation acts by compressing, flipping, or stretching.
Eigenvalues can be understood either as the strengths of the transformation in the direction of the eigenvectors or the factors by which the compressions happens.
10. Can you compare the validation set with the test set?
Answer: A validation set is part of the training set used for parameter selection as well as for avoiding overfitting of the machine learning model being developed. On the contrary, a test set is meant for evaluating or testing the performance of a trained machine learning model.
11. What do you understand by linear regression and logistic regression?
Answer: Linear regression is a form of statistical technique in which the score of some variable Y is predicted on the basis of the score of a second variable X, referred to as the predictor variable. The Y variable is known as the criterion variable.
Also known as the logit model, logistic regression is a statistical technique for predicting the binary outcome from a linear combination of predictor variables.
12. Please explain Recommender Systems along with an application.
Answer: Recommender Systems is a subclass of information filtering systems, meant for predicting the preferences or ratings awarded by a user to some product.
An application of a recommender system is the product recommendations section in Amazon. This section contains items based on the user’s search history and past orders.
13. What are outlier values and how do you treat them?
Answer: Outlier values, or simply outliers, are data points in statistics that don’t belong to a certain population. An outlier value is an abnormal observation that is very much different from other values belonging to the set.
Identification of outlier values can be done by using univariate or some other graphical analysis method. Few outlier values can be assessed individually but assessing a large set of outlier values require the substitution of the same with either the 99th or the 1st percentile values.
There are two popular ways of treating outlier values:
1.	To change the value so that it can be brought within a range
2.	To simply remove the value
Note: - Not all extreme values are outlier values.
14. Please enumerate the various steps involved in an analytics project.
Answer: Following are the numerous steps involved in an analytics project:
•	Understanding the business problem
•	Exploring the data and familiarizing with the same
•	Preparing the data for modeling by means of detecting outlier values, transforming variables, treating missing values, et cetera
•	Running the model and analyzing the result for making appropriate changes or modifications to the model (an iterative step that repeats until the best possible outcome is gained)
•	Validating the model using a new dataset
•	Implementing the model and tracking the result for analyzing the performance of the same
15. Could you explain how to define the number of clusters in a clustering algorithm?
Answer: The primary objective of clustering is to group together similar identities in such a way that while entities within a group are similar to each other, the groups remain different from one another.
Generally, Within Sum of Squares is used for explaining the homogeneity within a cluster. For defining the number of clusters in a clustering algorithm, WSS is plotted for a range pertaining to a number of clusters. The resultant graph is known as the Elbow Curve.
The Elbow Curve graph contains a point that represents the point post in which there aren’t any decrements in the WSS. This is known as the bending point and represents K in K–Means.
Although the aforementioned is the widely-used approach, another important approach is the Hierarchical clustering. In this approach, dendrograms are created first and then distinct groups are identified from there.
16. What do you understand by Deep Learning?
Answer: Deep Learning is a paradigm of machine learning that displays a great degree of analogy with the functioning of the human brain. It is a neural network method based on convolutional neural networks (CNN).
Deep learning has a wide array of uses, ranging from social network filtering to medical image analysis and speech recognition. Although Deep Learning has been present for a long time, it’s only recently that it has gained worldwide acclaim. This is mainly due to:
•	An increase in the amount of data generation via various sources
•	The growth in hardware resources required for running Deep Learning models
17. Please explain Gradient Descent.
Answer: The degree of change in the output of a function relating to the changes made to the inputs is known as a gradient. It measures the change in all weights with respect to the change in error. A gradient can also be comprehended as the slope of a function.
Gradient Descent refers to escalating down to the bottom of a valley. Simply, consider this something as opposed to climbing up a hill. It is a minimization algorithm meant for minimizing a given activation function.
18. How does Backpropagation work? Also, it state its various variants.
Answer: Backpropagation refers to a training algorithm used for multilayer neural networks. Following the backpropagation algorithm, the error is moved from an end of the network to all weights inside the network. Doing so allows for efficient computation of the gradient.
Backpropagation works in the following way:
•	Forward propagation of training data
•	Output and target is used for computing derivatives
•	Backpropagate for computing the derivative of the error with respect to the output activation
•	Using previously calculated derivatives for output generation
•	Updating the weights
Following are the various variants of Backpropagation:
•	Batch Gradient Descent – The gradient is calculated for the complete dataset and update is performed on each iteration
•	Mini-batch Gradient Descent – Mini-batch samples are used for calculating gradient and updating parameters (a variant of the Stochastic Gradient Descent approach)
•	Stochastic Gradient Descent – Only a single training example is used to calculate gradient and updating parameters
19. What do you know about Autoencoders?
Answer: Autoencoders are simplistic learning networks used for transforming inputs into outputs with minimum possible error. It means that the outputs resulted are very close to the inputs.
A couple of layers are added between the input and the output with the size of each layer smaller than the size pertaining to the input layer. An autoencoder receives unlabeled input that is encoded for reconstructing the output.
20. Please explain the concept of a Boltzmann Machine.
Answer: A Boltzmann Machine features a simple learning algorithm that enables the same to discover fascinating features representing complex regularities present in the training data. It is basically used for optimizing the quantity and weight for some given problem.
The simple learning algorithm involved in a Boltzmann Machine is very slow in networks that have many layers of feature detectors.
21. What are the skills required as a Data Scientist that could help in using Python for data analysis purposes?
Answer: The skills required as a Data Scientist that could help in using Python for data analysis purposes are stated under:
1.	Expertize in Pandas Dataframes, Scikit-learn, and N-dimensional NumPy Arrays.
2.	Skills to apply element-wise vector and matrix operations on NumPy arrays.
3.	Able to understand built-in data types, including tuples, sets, dictionaries, and various others.
4.	It is equipped with Anaconda distribution and the Conda package manager.
5.	Capability in writing efficient list comprehensions, small, clean functions, and avoid traditional for loops.
6.	Knowledge of Python script and optimizing bottlenecks
22. What is the full form of GAN? Explain GAN?
Answer: The full form of GAN is Generative Adversarial Network. Its task is to take inputs from the noise vector and send it forward to the Generator and then to Discriminator to identify and differentiate the unique and fake inputs.
23. What are the vital components of GAN?
Answer: There are two vital components of GAN. These include the following:
1.	Generator: The Generator act as a Forger, which creates fake copies.
2.	Discriminator: The Discriminator act as a recognizer for fake and unique (real) copies.
24. What is the Computational Graph?
Answer: A computational graph is a graphical presentation that is based on TensorFlow. It has a wide network of different kinds of nodes wherein each node represents a particular mathematical operation. The edges in these nodes are called tensors. This is the reason the computational graph is called a TensorFlow of inputs. The computational graph is characterized by data flows in the form of a graph; therefore, it is also called the DataFlow Graph.
25. What are tensors?
Answer: Tensors are the mathematical objects that represent the collection of higher dimensions of data inputs in the form of alphabets, numerals, and rank fed as inputs to the neural network.
26. Why are Tensorflow considered a high priority in learning Data Science?
Answer: Tensorflow is considered a high priority in learning Data Science because it provides support to using computer languages such as C++ and Python. This way, it makes various processes under data science to achieve faster compilation and completion within the stipulated time frame and faster than the conventional Keras and Torch libraries. Tensorflow supports the computing devices, including the CPU and GPU for faster inputs, editing, and analysis of the data.
27. What is Dropout in Data Science?
Answer: Dropout is a toll in Data Science, which is used for dropping out the hidden and visible units of a network on a random basis. They prevent the overfitting of the data by dropping as much as 20% of the nodes so that the required space can be arranged for iterations needed to converge the network.
28. What is Batch normalization in Data Science?
Answer: Batch Normalization in Data Science is a technique through which attempts could be made to improve the performance and stability of the neural network. This can be done by normalizing the inputs in each layer so that the mean output activation remains 0 with the standard deviation at 1.
29. What is the difference between Batch and Stochastic Gradient Descent?
Answer: The difference between Batch and Stochastic Gradient Descent can be displayed as follows:
Batch Gradient Descent	Stochastic Gradient Descent
It helps in computing the gradient using the complete data set available.	It helps in computing the gradient using only the single sample.
It takes time to converge.	It takes less time to converge.
The volume is huge for analysis purpose	The volume is lesser for analysis purposes.
It updates the weight slowly.	It updates the weight more frequently.
30.What are Auto-Encoders?
Answer: Auto-Encoders are learning networks that are meant to change inputs into output with the lowest chance of getting an error. They intend to keep the output closer to the input. The process of Autoencoders is needed to be done through the development of layers between the input and output. However, efforts are made to keep the size of these layers smaller for faster processing.
31. What are the various Machine Learning Libraries and their benefits?
Answer: The various machine learning libraries and their benefits are as follows.
1.	Numpy: It is used for scientific computation.
2.	Statsmodels: It is used for time-series analysis.
3.	Pandas: It is used for tubular data analysis.
4.	Scikit learns: It is used for data modeling and pre-processing.
5.	Tensorflow: It is used for the deep learning process.
6.	Regular Expressions: It is used for text processing.
7.	Pytorch: It is used for the deep learning process.
8.	NLTK: It is used for text processing.
32. What is an Activation function?
Answer: An Activation function helps in introducing the non-linearity in the neural network. This is done to help the learning process for complex functions. Without the activation function, the neural network will be unable to perform only the linear function and apply linear combinations. Activation function, therefore, offers complex functions and combinations by applying artificial neurons, which helps in delivering output based on the inputs.
33. What are the different types of Deep Learning Frameworks?
Answer: The different types of Deep Learning Framework includes the following:
1.	Caffe
2.	Keras
3.	TensorFlow
4.	Pytorch
5.	Chainer
6.	Microsoft Cognitive Toolkit
34. What are vanishing gradients?
Answer: The vanishing gradients is a condition when the slope is too small during the training process of RNN. The result of vanishing gradients is poor performance outcomes, low accuracy, and long term training processes.
35. What are exploding gradients?
Answer: The exploding gradients are a condition when the errors grow at an exponential rate or high rate during the training of RNN. This error gradient accumulates and results in applying large updates to the neural network, causes an overflow, and results in NaN values.
36. What is the full form of LSTM? What is its function?
Answer: LSTM stands for Long Short Term Memory. It is a recurrent neural network that is capable of learning long term dependencies and recalling information for the longer period as part of its default behavior.
37. What are the different steps in LSTM?
Answer: The different steps in LSTM include the following.
•	Step 1: The network helps in deciding the things that need to be remembered while others that need to be forgotten.
•	Step 2: The selection is made for cell state values that can be updated.
•	Step 3: The network decides as to what can be made as part of the current output.
38. What is Pooling on CNN?
Answer: Polling is a method that is used with the purpose to reduce the spatial dimensions of a CNN. It helps in performing downsampling operations for reducing dimensionality and creating pooled feature maps. Pooling in CNN helps in sliding the filter matrix over the input matrix.
49. What is RNN?
Answer: The RNN stands for Recurrent Neural Networks. They are an artificial neural network that is a sequence of data, including stock markets, sequence of data including stock markets, time series, and various others. The main idea behind the RNN application is to understand the basics of the feedforward nets.
40. What are the different layers on CNN?
Answer: There are four different layers on CNN. These include the following.
1.	Convolutional Layer: In this layer, several small picture windows are created to go over the data.
2.	ReLU Layer: This layer helps in bringing non-linearity to the network and converts the negative pixels to zero so that the output becomes a rectified feature map.
3.	Pooling Layer: This layer reduces the dimensionality of the feature map.
4.	Fully Connected Layer: This layer recognizes and classifies the objects in the image.
41. What is an Epoch in Data Science?
Answer: Epoch in Data Science represents one of the iterations over the entire dataset. It includes everything that is applied to the learning model.
42. What is a Batch in Data Science?
Answer: Batch is referred to as a different dataset that is divided into the form of different batches to help to pass the information into the system. It is developed in the situation when the developer cannot pass the entire dataset into the neural network at once.
43. What is the iteration in Data Science? Give an example?
Answer: Iteration in Data Science is applied by Epoch for analysis of data. The iteration is, therefore, classification of the data into different groups. For example, when there are 50,000 images, and the batch size is 100, then in such a case, the Epoch will run about 500 iterations.
44. What is the cost function?
Answer: Cost functions are a tool to evaluate how good the model performance has been made. It takes into consideration the errors and losses that are made in the output layer during the backpropagation process. In such a case, the errors are moved backward in the neural network, and various other training functions are applied.
45. What are hyperparameters?
Answer: Hyperparameter is a kind of parameter whose value is set before the learning process so that the network training requirements can be identified and the structure of the network can be improved. This process includes recognizing the hidden units, learning rate, epochs, and various others associated.
46. Which skills are important to become a certified Data Scientist?
Answer: The important skills to become a certified Data Scientist include the following:
1.	Knowledge of built-in data types including lists, tuples, sets and related.
2.	Expertize in N-dimensional NumPy Arrays.
3.	Ability to apply Pandas Dataframes.
4.	Strong hold over performance in element wise vectors.
5.	Knowledge of matrix operations on NumPy arrays.
47. What is an Artificial Neural Network in Data Science?
Answer: Artificial Neural Network in Data Science is the specific set of algorithms that are inspired by the biological neural network meant to adapt the changes in the input so that the best output can be achieved. It helps in generating the best possible results without the need to redesign the output methods.
48. What is Deep Learning in Data Science?
Answer: Deep Learning in Data Science is a name given to machine learning, which requires a great level of analogy with the functioning of the human brain. This way, it is a paradigm of machine learning.
49. Are there differences between Deep Learning and Machine Learning?
Answer: Yes, there are differences between Deep Learning and Machine learning. These are stated as under:
Deep Learning	Machine Learning
It gives computers the ability to learn without being explicitly programmed.	It gives computers a limited to unlimited ability wherein nothing major can be done without getting programmed, and many things can be done without the prior programming. It includes supervised, unsupervised, and reinforcement machine learning processes.
It is a subcomponent of machine learning that is concerned with algorithms that are inspired by the structure and functions of the human brains called the Artificial Neural Networks.	It includes Deep Learning as one of its components.
50.What is Ensemble learning?
Answer: Ensemble learning is a process of combining the diverse set of learners that is the individual models with each other. It helps in improving the stability and predictive power of the model.
Question: What are the different kinds of Ensemble learning?
Answer: The different kinds of Ensemble learning includes the following.
1.	Bagging: It implements simple learners on one small population and takes mean for estimation purposes.
2.	Boosting: It adjusts the weight of the observation and thereby classifies the population in different sets before the outcome prediction is made.

51. What is Naive Bayes?
Naive Bayes Machine Learning algorithm is a powerful algorithm for predictive modeling. It is a set of algorithms with a common principle based on Bayes Theorem. The fundamental Naive Bayes assumption is that each feature makes an independent and equal contribution to the outcome.
52. What is perceptron in Machine Learning?
Perceptron is an algorithm that is able to simulate the ability of the human brain to understand and discard; it is used for the supervised classification of the input into one of the several possible non-binary outputs.
53. List the extraction techniques used for dimensionality reduction.
•	Independent component analysis
•	Principal component analysis
•	Kernel-based principal component analysis
54. Is KNN different from K-means Clustering?
KNN	K-means Clustering
Supervised	Unsupervised
Classification algorithms	Clustering algorithms
Minimal training model	Exhaustive training model
Used in the classification and regression of the known data	Used in population demographics, market segmentation, social media trends, anomaly detection, etc.
55.What is ensemble learning?
Ensemble learning is a computational technique in which classifiers or experts are strategically formed and combined. It is used to improve classification, prediction, function approximation, etc. of a model.
56. List the steps involved in Machine Learning.
•	Data collection
•	Data preparation
•	Choosing an appropriate model
•	Training the dataset
•	Evaluation
•	Parameter tuning
•	Predictions
57. What is a hash table?
A hash table is a data structure that is used to produce an associative array which is mostly used for database indexing.
58. What is regularization in Machine Learning?
Regularization comes into the picture when a model is either overfit or underfit. It is basically used to minimize the error in a dataset. A new piece of information is fit into the dataset to avoid fitting issues.
59.What are the components of relational evaluation techniques?
•	Data acquisition
•	Ground truth acquisition
•	Cross validation technique
•	Query type
•	Scoring metric
•	Significance test
60.What is model accuracy and model performance?
Model accuracy, a subset of model performance, is based on the model performance of an algorithm. Whereas, model performance is based on the datasets we feed as inputs to the algorithm.
61. Define F1 score.
F1 score is the weighted average of precision and recall. It considers both false positive and false negative values into account. It is used to measure a model’s performance.
62. List the applications of Machine Learning.
•	Image, speech, and face detection
•	Bioinformatics
•	Market segmentation
•	Manufacturing and inventory management
•	Fraud detection, and so on
63. Can you name three feature selection techniques in Machine Learning?
1.	Univariate Selection
2.	Feature Importance
3.	Correlation Matrix with Heatmap
64.What is a recommendation system?
A recommendation system is an information filtering system that is used to predict user preference based on choice patterns followed by the user while browsing/using the system.
65.What methods are used for reducing dimensionality?
Dimensionality reduction is the process of reducing the number of random variables. We can reduce dimensionality using techniques such as missing values ratio, low variance filter, high correlation filter, random forest, principal component analysis, etc.
66. List different methods for sequential supervised learning.
•	Sliding window methods
•	Recurrent sliding windows methods
•	Hidden Markov models
•	Maximum entropy Markov models
•	Conditional random fields
•	Graph transformer networks
67. What are the advantages of neural networks?
•	Require less formal statistical training
•	Have the ability to detect nonlinear relationships between variables
•	Detect all possible interactions between predictor variables
•	Availability of multiple training algorithms
68. What is Bias–Variance tradeoff?
Bias error is used to measure how much on an average the predicted values vary from the actual values. In case a high-bias error occurs, we have an under-performing model.
Variance is used to measure how the predictions made on the same observation differ from each other. A high-variance model will overfit the dataset and perform badly on any observation.

69. What is TensorFlow?
TensorFlow is an open-source Machine Learning library. It is a fast, flexible, and low-level toolkit for doing complex algorithms and offers users customizability to build experimental learning architectures and to work on them to produce desired outputs.
70.How to install TensorFlow?
TensorFlow Installation Guide:
CPU : pip install tensorflow-cpu
GPU : pip install tensorflow-gpu
71.What are the TensorFlow objects?
1.	Constants
2.	Variables
3.	Placeholder
4.	Graph
5.	Session
72. What is a cost function?
A cost function is a scalar function that quantifies the error factor of the neural network. Lower the cost function better the neural network. For example, while classifying the image in the MNIST dataset, the input image is digit 2, but the neural network wrongly predicts it to be 3.
73. List different activation neurons or functions.
1.	Linear neuron
2.	Binary threshold neuron
3.	Stochastic binary neuron
4.	Sigmoid neuron
5.	Tanh function
6.	Rectified linear unit (ReLU)
74. What are the hyper parameters of ANN?
•	Learning rate: The learning rate is how fast the network learns its parameters.
•	Momentum: It is a parameter that helps to come out of the local minima and smoothen the jumps while gradient descent.
•	Number of epochs: The number of times the entire training data is fed to the network while training is referred to as the number of epochs. We increase the number of epochs until the validation accuracy starts decreasing, even if the training accuracy is increasing (overfitting).
75. What is vanishing gradient?
As we add more and more hidden layers, backpropagation becomes less useful in passing information to the lower layers. In effect, as information is passed back, the gradients begin to vanish and become small relative to the weights of the network.
76. What are dropouts?
Dropout is a simple way to prevent a neural network from overfitting. It is the dropping out of some of the units in a neural network. It is similar to the natural reproduction process, where the nature produces offsprings by combining distinct genes (dropping out others) rather than strengthening the co-adapting of them.
77. Define LSTM.
Long short-term memory (LSTM) is explicitly designed to address the long-term dependency problem, by maintaining a state of what to remember and what to forget.
78. List the key components of LSTM.
•	Gates (forget, Memory, update, and Read)
•	Tanh(x) (values between -1 and 1)
•	Sigmoid(x) (values between 0 and 1)
79. List the variants of RNN.
•	LSTM: Long Short-term Memory
•	GRU: Gated Recurrent Unit
•	End-to-end Network
•	Memory Network
80. What is an autoencoder? Name a few applications.
An autoencoder is basically used to learn a compressed form of the given data. A few applications of an autoencoder are given below:
1.	Data denoising
2.	Dimensionality reduction
3.	Image reconstruction
4.	Image colorization
81. What are the components of the generative adversarial network (GAN)? How do you deploy it?
Components of GAN:
•	Generator
•	Discriminator
Deployment Steps:
•	Train the model
•	Validate and finalize the model
•	Save the model
•	Load the saved model for the next prediction
82. What are the steps involved in the gradient descent algorithm?
Gradient descent is an optimization algorithm that is used to find the coefficients of parameters that are used to reduce the cost function to a minimum.
Step 1: Allocate weights (x,y) with random values and calculate the error (SSE)
Step 2: Calculate the gradient, i.e., the variation in SSE when the weights (x,y) are changed by a very small value. This helps us move the values of x and y in the direction in which SSE is minimized
Step 3: Adjust the weights with the gradients to move toward the optimal values where SSE is minimized
Step 4: Use new weights for prediction and calculating the new SSE
Step 5: Repeat Steps 2 and 3 until further adjustments to the weights do not significantly reduce the error
83. What do you understand by session in TensorFlow?
Syntax: Class Session
It is a class for running TensorFlow operations. The environment is encapsulated in the session object wherein the operation objects are executed and Tensor objects are evaluated.
# Build a graph
x = tf.constant(2.0)
y = tf.constant(5.0)
z = x * y
# Launch the graph in a session
sess = tf.Session()
# Evaluate the tensor `z`
print(sess.run(z))
84. What do you mean by TensorFlow cluster?
TensorFlow cluster is a set of ‘tasks’ that participate in the distributed execution of a TensorFlow graph. Each task is associated with a TensorFlow server, which contains a ‘master’ that can be used to create sessions and a ‘worker’ that executes operations in the graph. A cluster can also be divided into one or more ‘jobs’, where each job contains one or more tasks.

85. How to run TensorFlow on Hadoop?
To use HDFS with TensorFlow, we need to change the file path for reading and writing data to an HDFS path. For example:
filename_queue = tf.train.string_input_producer([
“hdfs://namenode:8020/path/to/file1.csv”,
“hdfs://namenode:8020/path/to/file2.csv”,
])
86. What are intermediate tensors? Do sessions have lifetime?
The intermediate tensors are tensors that are neither inputs nor outputs of the Session.run() call, but are in the path leading from the inputs to the outputs; they will be freed at or before the end of the call.
Sessions can own resources, few classes like tf.Variable, tf.QueueBase, and tf.ReaderBase, and they use a significant amount of memory. These resources (and the associated memory) are released when the session is closed, by calling tf.Session.close.

87. What is the lifetime of a variable?
When we first run the tf.Variable.initializer operation for a variable in a session, it is started. It is destroyed when we run the tf.Session.close operation.




Data Science

Basic Level-Q&A:
1. Can you enumerate the various differences between Supervised and Unsupervised Learning?
Answer: Supervised learning is a type of machine learning where a function is inferred from labeled training data. The training data contains a set of training examples.
Unsupervised learning, on the other hand, is a type of machine learning where inferences are drawn from datasets containing input data without labeled responses. Following are the various other differences between the two types of machine learning:
•	Algorithms Used – Supervised learning makes use of Decision Trees, K-nearest Neighbor algorithm, Neural Networks, Regression, and Support Vector Machines. Unsupervised learning uses Anomaly Detection, Clustering, Latent Variable Models, and Neural Networks.
•	Enables – Supervised learning enables classification and regression, whereas unsupervised learning enables classification, dimension reduction, and density estimation
•	Use – While supervised learning is used for prediction, unsupervised learning finds use in analysis
2. What do you understand by the Selection Bias? What are its various types?
Answer: Selection bias is typically associated with research that doesn’t have a random selection of participants. It is a type of error that occurs when a researcher decides who is going to be studied. On some occasions, selection bias is also referred to as the selection effect.
In other words, selection bias is a distortion of statistical analysis that results from the sample collecting method. When selection bias is not taken into account, some conclusions made by a research study might not be accurate. Following are the various types of selection bias:
•	Sampling Bias – A systematic error resulting due to a non-random sample of a populace causing certain members of the same to be less likely included than others that results in a biased sample.
•	Time Interval – A trial might be ended at an extreme value, usually due to ethical reasons, but the extreme value is most likely to be reached by the variable with the most variance, even though all variables have a similar mean.
•	Data – Results when specific data subsets are selected for supporting a conclusion or rejection of bad data arbitrarily.
•	Attrition – Caused due to attrition, i.e. loss of participants, discounting trial subjects or tests that didn’t run to completion.
3. Please explain the goal of A/B Testing.
Answer: A/B Testing is a statistical hypothesis testing meant for a randomized experiment with two variables, A and B. The goal of A/B Testing is to maximize the likelihood of an outcome of some interest by identifying any changes to a webpage.
A highly reliable method for finding out the best online marketing and promotional strategies for a business, A/B Testing can be employed for testing everything, ranging from sales emails to search ads and website copy.
4. How will you calculate the Sensitivity of machine learning models?
Answer: In machine learning, Sensitivity is used for validating the accuracy of a classifier, such as Logistic, Random Forest, and SVM. It is also known as REC (recall) or TPR (true positive rate).
Sensitivity can be defined as the ratio of predicted true events and total events i.e.:
Sensitivity = True Positives / Positives in Actual Dependent Variable
Here, true events are those events that were true as predicted by a machine learning model. The best sensitivity is 1.0 and the worst sensitivity is 0.0.
5. Could you draw a comparison between overfitting and underfitting?
Answer: In order to make reliable predictions on general untrained data in machine learning and statistics, it is required to fit a (machine learning) model to a set of training data. Overfitting and underfitting are two of the most common modeling errors that occur while doing so.
Following are the various differences between overfitting and underfitting:
•	Definition - A statistical model suffering from overfitting describes some random error or noise in place of the underlying relationship. When underfitting occurs, a statistical model or machine learning algorithm fails in capturing the underlying trend of the data.
•	Occurrence – When a statistical model or machine learning algorithm is excessively complex, it can result in overfitting. Example of a complex model is one having too many parameters when compared to the total number of observations. Underfitting occurs when trying to fit a linear model to non-linear data.
•	Poor Predictive Performance – Although both overfitting and underfitting yield poor predictive performance, the way in which each one of them does so is different. While the overfitted model overreacts to minor fluctuations in the training data, the underfit model under-reacts to even bigger fluctuations.
6. Between Python and R, which one would you pick for text analytics and why?
Answer: For text analytics, Python will gain an upper hand over R due to these reasons:
•	The Pandas library in Python offers easy-to-use data structures as well as high-performance data analysis tools
•	Python has a faster performance for all types of text analytics
•	R is a best-fit for machine learning than mere text analysis
7. Please explain the role of data cleaning in data analysis.
Answer: Data cleaning can be a daunting task due to the fact that with the increase in the number of data sources, the time required for cleaning the data increases at an exponential rate.
This is due to the vast volume of data generated by additional sources. Also, data cleaning can solely take up to 80% of the total time required for carrying out a data analysis task.
Nevertheless, there are several reasons for using data cleaning in data analysis. Two of the most important ones are:
•	Cleaning data from different sources helps in transforming the data into a format that is easy to work with
•	Data cleaning increases the accuracy of a machine learning model
8. What do you mean by cluster sampling and systematic sampling?
Answer: When studying the target population spread throughout a wide area becomes difficult and applying simple random sampling becomes ineffective, the technique of cluster sampling is used. A cluster sample is a probability sample, in which each of the sampling units is a collection or cluster of elements.
Following the technique of systematic sampling, elements are chosen from an ordered sampling frame. The list is advanced in a circular fashion. This is done in such a way so that once the end of the list is reached, the same is progressed from the start, or top, again.
9. Please explain Eigenvectors and Eigenvalues.
Answer: Eigenvectors help in understanding linear transformations. They are calculated typically for a correlation or covariance matrix in data analysis.
In other words, eigenvectors are those directions along which some particular linear transformation acts by compressing, flipping, or stretching.
Eigenvalues can be understood either as the strengths of the transformation in the direction of the eigenvectors or the factors by which the compressions happens.
10. Can you compare the validation set with the test set?
Answer: A validation set is part of the training set used for parameter selection as well as for avoiding overfitting of the machine learning model being developed. On the contrary, a test set is meant for evaluating or testing the performance of a trained machine learning model.
11. What do you understand by linear regression and logistic regression?
Answer: Linear regression is a form of statistical technique in which the score of some variable Y is predicted on the basis of the score of a second variable X, referred to as the predictor variable. The Y variable is known as the criterion variable.
Also known as the logit model, logistic regression is a statistical technique for predicting the binary outcome from a linear combination of predictor variables.
12. Please explain Recommender Systems along with an application.
Answer: Recommender Systems is a subclass of information filtering systems, meant for predicting the preferences or ratings awarded by a user to some product.
An application of a recommender system is the product recommendations section in Amazon. This section contains items based on the user’s search history and past orders.
13. What are outlier values and how do you treat them?
Answer: Outlier values, or simply outliers, are data points in statistics that don’t belong to a certain population. An outlier value is an abnormal observation that is very much different from other values belonging to the set.
Identification of outlier values can be done by using univariate or some other graphical analysis method. Few outlier values can be assessed individually but assessing a large set of outlier values require the substitution of the same with either the 99th or the 1st percentile values.
There are two popular ways of treating outlier values:
1.	To change the value so that it can be brought within a range
2.	To simply remove the value
Note: - Not all extreme values are outlier values.
14. Please enumerate the various steps involved in an analytics project.
Answer: Following are the numerous steps involved in an analytics project:
•	Understanding the business problem
•	Exploring the data and familiarizing with the same
•	Preparing the data for modeling by means of detecting outlier values, transforming variables, treating missing values, et cetera
•	Running the model and analyzing the result for making appropriate changes or modifications to the model (an iterative step that repeats until the best possible outcome is gained)
•	Validating the model using a new dataset
•	Implementing the model and tracking the result for analyzing the performance of the same
15. Could you explain how to define the number of clusters in a clustering algorithm?
Answer: The primary objective of clustering is to group together similar identities in such a way that while entities within a group are similar to each other, the groups remain different from one another.
Generally, Within Sum of Squares is used for explaining the homogeneity within a cluster. For defining the number of clusters in a clustering algorithm, WSS is plotted for a range pertaining to a number of clusters. The resultant graph is known as the Elbow Curve.
The Elbow Curve graph contains a point that represents the point post in which there aren’t any decrements in the WSS. This is known as the bending point and represents K in K–Means.
Although the aforementioned is the widely-used approach, another important approach is the Hierarchical clustering. In this approach, dendrograms are created first and then distinct groups are identified from there.
16. What do you understand by Deep Learning?
Answer: Deep Learning is a paradigm of machine learning that displays a great degree of analogy with the functioning of the human brain. It is a neural network method based on convolutional neural networks (CNN).
Deep learning has a wide array of uses, ranging from social network filtering to medical image analysis and speech recognition. Although Deep Learning has been present for a long time, it’s only recently that it has gained worldwide acclaim. This is mainly due to:
•	An increase in the amount of data generation via various sources
•	The growth in hardware resources required for running Deep Learning models
17. Please explain Gradient Descent.
Answer: The degree of change in the output of a function relating to the changes made to the inputs is known as a gradient. It measures the change in all weights with respect to the change in error. A gradient can also be comprehended as the slope of a function.
Gradient Descent refers to escalating down to the bottom of a valley. Simply, consider this something as opposed to climbing up a hill. It is a minimization algorithm meant for minimizing a given activation function.
18. How does Backpropagation work? Also, it state its various variants.
Answer: Backpropagation refers to a training algorithm used for multilayer neural networks. Following the backpropagation algorithm, the error is moved from an end of the network to all weights inside the network. Doing so allows for efficient computation of the gradient.
Backpropagation works in the following way:
•	Forward propagation of training data
•	Output and target is used for computing derivatives
•	Backpropagate for computing the derivative of the error with respect to the output activation
•	Using previously calculated derivatives for output generation
•	Updating the weights
Following are the various variants of Backpropagation:
•	Batch Gradient Descent – The gradient is calculated for the complete dataset and update is performed on each iteration
•	Mini-batch Gradient Descent – Mini-batch samples are used for calculating gradient and updating parameters (a variant of the Stochastic Gradient Descent approach)
•	Stochastic Gradient Descent – Only a single training example is used to calculate gradient and updating parameters
19. What do you know about Autoencoders?
Answer: Autoencoders are simplistic learning networks used for transforming inputs into outputs with minimum possible error. It means that the outputs resulted are very close to the inputs.
A couple of layers are added between the input and the output with the size of each layer smaller than the size pertaining to the input layer. An autoencoder receives unlabeled input that is encoded for reconstructing the output.
20. Please explain the concept of a Boltzmann Machine.
Answer: A Boltzmann Machine features a simple learning algorithm that enables the same to discover fascinating features representing complex regularities present in the training data. It is basically used for optimizing the quantity and weight for some given problem.
The simple learning algorithm involved in a Boltzmann Machine is very slow in networks that have many layers of feature detectors.
21. What are the skills required as a Data Scientist that could help in using Python for data analysis purposes?
Answer: The skills required as a Data Scientist that could help in using Python for data analysis purposes are stated under:
1.	Expertize in Pandas Dataframes, Scikit-learn, and N-dimensional NumPy Arrays.
2.	Skills to apply element-wise vector and matrix operations on NumPy arrays.
3.	Able to understand built-in data types, including tuples, sets, dictionaries, and various others.
4.	It is equipped with Anaconda distribution and the Conda package manager.
5.	Capability in writing efficient list comprehensions, small, clean functions, and avoid traditional for loops.
6.	Knowledge of Python script and optimizing bottlenecks
22. What is the full form of GAN? Explain GAN?
Answer: The full form of GAN is Generative Adversarial Network. Its task is to take inputs from the noise vector and send it forward to the Generator and then to Discriminator to identify and differentiate the unique and fake inputs.
23. What are the vital components of GAN?
Answer: There are two vital components of GAN. These include the following:
1.	Generator: The Generator act as a Forger, which creates fake copies.
2.	Discriminator: The Discriminator act as a recognizer for fake and unique (real) copies.
24. What is the Computational Graph?
Answer: A computational graph is a graphical presentation that is based on TensorFlow. It has a wide network of different kinds of nodes wherein each node represents a particular mathematical operation. The edges in these nodes are called tensors. This is the reason the computational graph is called a TensorFlow of inputs. The computational graph is characterized by data flows in the form of a graph; therefore, it is also called the DataFlow Graph.
25. What are tensors?
Answer: Tensors are the mathematical objects that represent the collection of higher dimensions of data inputs in the form of alphabets, numerals, and rank fed as inputs to the neural network.
26. Why are Tensorflow considered a high priority in learning Data Science?
Answer: Tensorflow is considered a high priority in learning Data Science because it provides support to using computer languages such as C++ and Python. This way, it makes various processes under data science to achieve faster compilation and completion within the stipulated time frame and faster than the conventional Keras and Torch libraries. Tensorflow supports the computing devices, including the CPU and GPU for faster inputs, editing, and analysis of the data.
27. What is Dropout in Data Science?
Answer: Dropout is a toll in Data Science, which is used for dropping out the hidden and visible units of a network on a random basis. They prevent the overfitting of the data by dropping as much as 20% of the nodes so that the required space can be arranged for iterations needed to converge the network.
28. What is Batch normalization in Data Science?
Answer: Batch Normalization in Data Science is a technique through which attempts could be made to improve the performance and stability of the neural network. This can be done by normalizing the inputs in each layer so that the mean output activation remains 0 with the standard deviation at 1.
29. What is the difference between Batch and Stochastic Gradient Descent?
Answer: The difference between Batch and Stochastic Gradient Descent can be displayed as follows:
Batch Gradient Descent	Stochastic Gradient Descent
It helps in computing the gradient using the complete data set available.	It helps in computing the gradient using only the single sample.
It takes time to converge.	It takes less time to converge.
The volume is huge for analysis purpose	The volume is lesser for analysis purposes.
It updates the weight slowly.	It updates the weight more frequently.
30.What are Auto-Encoders?
Answer: Auto-Encoders are learning networks that are meant to change inputs into output with the lowest chance of getting an error. They intend to keep the output closer to the input. The process of Autoencoders is needed to be done through the development of layers between the input and output. However, efforts are made to keep the size of these layers smaller for faster processing.
31. What are the various Machine Learning Libraries and their benefits?
Answer: The various machine learning libraries and their benefits are as follows.
1.	Numpy: It is used for scientific computation.
2.	Statsmodels: It is used for time-series analysis.
3.	Pandas: It is used for tubular data analysis.
4.	Scikit learns: It is used for data modeling and pre-processing.
5.	Tensorflow: It is used for the deep learning process.
6.	Regular Expressions: It is used for text processing.
7.	Pytorch: It is used for the deep learning process.
8.	NLTK: It is used for text processing.
32. What is an Activation function?
Answer: An Activation function helps in introducing the non-linearity in the neural network. This is done to help the learning process for complex functions. Without the activation function, the neural network will be unable to perform only the linear function and apply linear combinations. Activation function, therefore, offers complex functions and combinations by applying artificial neurons, which helps in delivering output based on the inputs.
33. What are the different types of Deep Learning Frameworks?
Answer: The different types of Deep Learning Framework includes the following:
1.	Caffe
2.	Keras
3.	TensorFlow
4.	Pytorch
5.	Chainer
6.	Microsoft Cognitive Toolkit
34. What are vanishing gradients?
Answer: The vanishing gradients is a condition when the slope is too small during the training process of RNN. The result of vanishing gradients is poor performance outcomes, low accuracy, and long term training processes.
35. What are exploding gradients?
Answer: The exploding gradients are a condition when the errors grow at an exponential rate or high rate during the training of RNN. This error gradient accumulates and results in applying large updates to the neural network, causes an overflow, and results in NaN values.
36. What is the full form of LSTM? What is its function?
Answer: LSTM stands for Long Short Term Memory. It is a recurrent neural network that is capable of learning long term dependencies and recalling information for the longer period as part of its default behavior.
37. What are the different steps in LSTM?
Answer: The different steps in LSTM include the following.
•	Step 1: The network helps in deciding the things that need to be remembered while others that need to be forgotten.
•	Step 2: The selection is made for cell state values that can be updated.
•	Step 3: The network decides as to what can be made as part of the current output.
38. What is Pooling on CNN?
Answer: Polling is a method that is used with the purpose to reduce the spatial dimensions of a CNN. It helps in performing downsampling operations for reducing dimensionality and creating pooled feature maps. Pooling in CNN helps in sliding the filter matrix over the input matrix.
49. What is RNN?
Answer: The RNN stands for Recurrent Neural Networks. They are an artificial neural network that is a sequence of data, including stock markets, sequence of data including stock markets, time series, and various others. The main idea behind the RNN application is to understand the basics of the feedforward nets.
40. What are the different layers on CNN?
Answer: There are four different layers on CNN. These include the following.
1.	Convolutional Layer: In this layer, several small picture windows are created to go over the data.
2.	ReLU Layer: This layer helps in bringing non-linearity to the network and converts the negative pixels to zero so that the output becomes a rectified feature map.
3.	Pooling Layer: This layer reduces the dimensionality of the feature map.
4.	Fully Connected Layer: This layer recognizes and classifies the objects in the image.
41. What is an Epoch in Data Science?
Answer: Epoch in Data Science represents one of the iterations over the entire dataset. It includes everything that is applied to the learning model.
42. What is a Batch in Data Science?
Answer: Batch is referred to as a different dataset that is divided into the form of different batches to help to pass the information into the system. It is developed in the situation when the developer cannot pass the entire dataset into the neural network at once.
43. What is the iteration in Data Science? Give an example?
Answer: Iteration in Data Science is applied by Epoch for analysis of data. The iteration is, therefore, classification of the data into different groups. For example, when there are 50,000 images, and the batch size is 100, then in such a case, the Epoch will run about 500 iterations.
44. What is the cost function?
Answer: Cost functions are a tool to evaluate how good the model performance has been made. It takes into consideration the errors and losses that are made in the output layer during the backpropagation process. In such a case, the errors are moved backward in the neural network, and various other training functions are applied.
45. What are hyperparameters?
Answer: Hyperparameter is a kind of parameter whose value is set before the learning process so that the network training requirements can be identified and the structure of the network can be improved. This process includes recognizing the hidden units, learning rate, epochs, and various others associated.
46. Which skills are important to become a certified Data Scientist?
Answer: The important skills to become a certified Data Scientist include the following:
1.	Knowledge of built-in data types including lists, tuples, sets and related.
2.	Expertize in N-dimensional NumPy Arrays.
3.	Ability to apply Pandas Dataframes.
4.	Strong hold over performance in element wise vectors.
5.	Knowledge of matrix operations on NumPy arrays.
47. What is an Artificial Neural Network in Data Science?
Answer: Artificial Neural Network in Data Science is the specific set of algorithms that are inspired by the biological neural network meant to adapt the changes in the input so that the best output can be achieved. It helps in generating the best possible results without the need to redesign the output methods.
48. What is Deep Learning in Data Science?
Answer: Deep Learning in Data Science is a name given to machine learning, which requires a great level of analogy with the functioning of the human brain. This way, it is a paradigm of machine learning.
49. Are there differences between Deep Learning and Machine Learning?
Answer: Yes, there are differences between Deep Learning and Machine learning. These are stated as under:
Deep Learning	Machine Learning
It gives computers the ability to learn without being explicitly programmed.	It gives computers a limited to unlimited ability wherein nothing major can be done without getting programmed, and many things can be done without the prior programming. It includes supervised, unsupervised, and reinforcement machine learning processes.
It is a subcomponent of machine learning that is concerned with algorithms that are inspired by the structure and functions of the human brains called the Artificial Neural Networks.	It includes Deep Learning as one of its components.
50.What is Ensemble learning?
Answer: Ensemble learning is a process of combining the diverse set of learners that is the individual models with each other. It helps in improving the stability and predictive power of the model.
Question: What are the different kinds of Ensemble learning?
Answer: The different kinds of Ensemble learning includes the following.
1.	Bagging: It implements simple learners on one small population and takes mean for estimation purposes.
2.	Boosting: It adjusts the weight of the observation and thereby classifies the population in different sets before the outcome prediction is made.

51. What is Naive Bayes?
Naive Bayes Machine Learning algorithm is a powerful algorithm for predictive modeling. It is a set of algorithms with a common principle based on Bayes Theorem. The fundamental Naive Bayes assumption is that each feature makes an independent and equal contribution to the outcome.
52. What is perceptron in Machine Learning?
Perceptron is an algorithm that is able to simulate the ability of the human brain to understand and discard; it is used for the supervised classification of the input into one of the several possible non-binary outputs.
53. List the extraction techniques used for dimensionality reduction.
•	Independent component analysis
•	Principal component analysis
•	Kernel-based principal component analysis
54. Is KNN different from K-means Clustering?
KNN	K-means Clustering
Supervised	Unsupervised
Classification algorithms	Clustering algorithms
Minimal training model	Exhaustive training model
Used in the classification and regression of the known data	Used in population demographics, market segmentation, social media trends, anomaly detection, etc.
55.What is ensemble learning?
Ensemble learning is a computational technique in which classifiers or experts are strategically formed and combined. It is used to improve classification, prediction, function approximation, etc. of a model.
56. List the steps involved in Machine Learning.
•	Data collection
•	Data preparation
•	Choosing an appropriate model
•	Training the dataset
•	Evaluation
•	Parameter tuning
•	Predictions
57. What is a hash table?
A hash table is a data structure that is used to produce an associative array which is mostly used for database indexing.
58. What is regularization in Machine Learning?
Regularization comes into the picture when a model is either overfit or underfit. It is basically used to minimize the error in a dataset. A new piece of information is fit into the dataset to avoid fitting issues.
59.What are the components of relational evaluation techniques?
•	Data acquisition
•	Ground truth acquisition
•	Cross validation technique
•	Query type
•	Scoring metric
•	Significance test
60.What is model accuracy and model performance?
Model accuracy, a subset of model performance, is based on the model performance of an algorithm. Whereas, model performance is based on the datasets we feed as inputs to the algorithm.
61. Define F1 score.
F1 score is the weighted average of precision and recall. It considers both false positive and false negative values into account. It is used to measure a model’s performance.
62. List the applications of Machine Learning.
•	Image, speech, and face detection
•	Bioinformatics
•	Market segmentation
•	Manufacturing and inventory management
•	Fraud detection, and so on
63. Can you name three feature selection techniques in Machine Learning?
1.	Univariate Selection
2.	Feature Importance
3.	Correlation Matrix with Heatmap
64.What is a recommendation system?
A recommendation system is an information filtering system that is used to predict user preference based on choice patterns followed by the user while browsing/using the system.
65.What methods are used for reducing dimensionality?
Dimensionality reduction is the process of reducing the number of random variables. We can reduce dimensionality using techniques such as missing values ratio, low variance filter, high correlation filter, random forest, principal component analysis, etc.
66. List different methods for sequential supervised learning.
•	Sliding window methods
•	Recurrent sliding windows methods
•	Hidden Markov models
•	Maximum entropy Markov models
•	Conditional random fields
•	Graph transformer networks
67. What are the advantages of neural networks?
•	Require less formal statistical training
•	Have the ability to detect nonlinear relationships between variables
•	Detect all possible interactions between predictor variables
•	Availability of multiple training algorithms
68. What is Bias–Variance tradeoff?
Bias error is used to measure how much on an average the predicted values vary from the actual values. In case a high-bias error occurs, we have an under-performing model.
Variance is used to measure how the predictions made on the same observation differ from each other. A high-variance model will overfit the dataset and perform badly on any observation.

69. What is TensorFlow?
TensorFlow is an open-source Machine Learning library. It is a fast, flexible, and low-level toolkit for doing complex algorithms and offers users customizability to build experimental learning architectures and to work on them to produce desired outputs.
70.How to install TensorFlow?
TensorFlow Installation Guide:
CPU : pip install tensorflow-cpu
GPU : pip install tensorflow-gpu
71.What are the TensorFlow objects?
1.	Constants
2.	Variables
3.	Placeholder
4.	Graph
5.	Session
72. What is a cost function?
A cost function is a scalar function that quantifies the error factor of the neural network. Lower the cost function better the neural network. For example, while classifying the image in the MNIST dataset, the input image is digit 2, but the neural network wrongly predicts it to be 3.
73. List different activation neurons or functions.
1.	Linear neuron
2.	Binary threshold neuron
3.	Stochastic binary neuron
4.	Sigmoid neuron
5.	Tanh function
6.	Rectified linear unit (ReLU)
74. What are the hyper parameters of ANN?
•	Learning rate: The learning rate is how fast the network learns its parameters.
•	Momentum: It is a parameter that helps to come out of the local minima and smoothen the jumps while gradient descent.
•	Number of epochs: The number of times the entire training data is fed to the network while training is referred to as the number of epochs. We increase the number of epochs until the validation accuracy starts decreasing, even if the training accuracy is increasing (overfitting).
75. What is vanishing gradient?
As we add more and more hidden layers, backpropagation becomes less useful in passing information to the lower layers. In effect, as information is passed back, the gradients begin to vanish and become small relative to the weights of the network.
76. What are dropouts?
Dropout is a simple way to prevent a neural network from overfitting. It is the dropping out of some of the units in a neural network. It is similar to the natural reproduction process, where the nature produces offsprings by combining distinct genes (dropping out others) rather than strengthening the co-adapting of them.
77. Define LSTM.
Long short-term memory (LSTM) is explicitly designed to address the long-term dependency problem, by maintaining a state of what to remember and what to forget.
78. List the key components of LSTM.
•	Gates (forget, Memory, update, and Read)
•	Tanh(x) (values between -1 and 1)
•	Sigmoid(x) (values between 0 and 1)
79. List the variants of RNN.
•	LSTM: Long Short-term Memory
•	GRU: Gated Recurrent Unit
•	End-to-end Network
•	Memory Network
80. What is an autoencoder? Name a few applications.
An autoencoder is basically used to learn a compressed form of the given data. A few applications of an autoencoder are given below:
1.	Data denoising
2.	Dimensionality reduction
3.	Image reconstruction
4.	Image colorization
81. What are the components of the generative adversarial network (GAN)? How do you deploy it?
Components of GAN:
•	Generator
•	Discriminator
Deployment Steps:
•	Train the model
•	Validate and finalize the model
•	Save the model
•	Load the saved model for the next prediction
82. What are the steps involved in the gradient descent algorithm?
Gradient descent is an optimization algorithm that is used to find the coefficients of parameters that are used to reduce the cost function to a minimum.
Step 1: Allocate weights (x,y) with random values and calculate the error (SSE)
Step 2: Calculate the gradient, i.e., the variation in SSE when the weights (x,y) are changed by a very small value. This helps us move the values of x and y in the direction in which SSE is minimized
Step 3: Adjust the weights with the gradients to move toward the optimal values where SSE is minimized
Step 4: Use new weights for prediction and calculating the new SSE
Step 5: Repeat Steps 2 and 3 until further adjustments to the weights do not significantly reduce the error
83. What do you understand by session in TensorFlow?
Syntax: Class Session
It is a class for running TensorFlow operations. The environment is encapsulated in the session object wherein the operation objects are executed and Tensor objects are evaluated.
# Build a graph
x = tf.constant(2.0)
y = tf.constant(5.0)
z = x * y
# Launch the graph in a session
sess = tf.Session()
# Evaluate the tensor `z`
print(sess.run(z))
84. What do you mean by TensorFlow cluster?
TensorFlow cluster is a set of ‘tasks’ that participate in the distributed execution of a TensorFlow graph. Each task is associated with a TensorFlow server, which contains a ‘master’ that can be used to create sessions and a ‘worker’ that executes operations in the graph. A cluster can also be divided into one or more ‘jobs’, where each job contains one or more tasks.

85. How to run TensorFlow on Hadoop?
To use HDFS with TensorFlow, we need to change the file path for reading and writing data to an HDFS path. For example:
filename_queue = tf.train.string_input_producer([
“hdfs://namenode:8020/path/to/file1.csv”,
“hdfs://namenode:8020/path/to/file2.csv”,
])
86. What are intermediate tensors? Do sessions have lifetime?
The intermediate tensors are tensors that are neither inputs nor outputs of the Session.run() call, but are in the path leading from the inputs to the outputs; they will be freed at or before the end of the call.
Sessions can own resources, few classes like tf.Variable, tf.QueueBase, and tf.ReaderBase, and they use a significant amount of memory. These resources (and the associated memory) are released when the session is closed, by calling tf.Session.close.

87. What is the lifetime of a variable?
When we first run the tf.Variable.initializer operation for a variable in a session, it is started. It is destroyed when we run the tf.Session.close operation.




Data Science

Basic Level-Q&A:
1. Can you enumerate the various differences between Supervised and Unsupervised Learning?
Answer: Supervised learning is a type of machine learning where a function is inferred from labeled training data. The training data contains a set of training examples.
Unsupervised learning, on the other hand, is a type of machine learning where inferences are drawn from datasets containing input data without labeled responses. Following are the various other differences between the two types of machine learning:
•	Algorithms Used – Supervised learning makes use of Decision Trees, K-nearest Neighbor algorithm, Neural Networks, Regression, and Support Vector Machines. Unsupervised learning uses Anomaly Detection, Clustering, Latent Variable Models, and Neural Networks.
•	Enables – Supervised learning enables classification and regression, whereas unsupervised learning enables classification, dimension reduction, and density estimation
•	Use – While supervised learning is used for prediction, unsupervised learning finds use in analysis
2. What do you understand by the Selection Bias? What are its various types?
Answer: Selection bias is typically associated with research that doesn’t have a random selection of participants. It is a type of error that occurs when a researcher decides who is going to be studied. On some occasions, selection bias is also referred to as the selection effect.
In other words, selection bias is a distortion of statistical analysis that results from the sample collecting method. When selection bias is not taken into account, some conclusions made by a research study might not be accurate. Following are the various types of selection bias:
•	Sampling Bias – A systematic error resulting due to a non-random sample of a populace causing certain members of the same to be less likely included than others that results in a biased sample.
•	Time Interval – A trial might be ended at an extreme value, usually due to ethical reasons, but the extreme value is most likely to be reached by the variable with the most variance, even though all variables have a similar mean.
•	Data – Results when specific data subsets are selected for supporting a conclusion or rejection of bad data arbitrarily.
•	Attrition – Caused due to attrition, i.e. loss of participants, discounting trial subjects or tests that didn’t run to completion.
3. Please explain the goal of A/B Testing.
Answer: A/B Testing is a statistical hypothesis testing meant for a randomized experiment with two variables, A and B. The goal of A/B Testing is to maximize the likelihood of an outcome of some interest by identifying any changes to a webpage.
A highly reliable method for finding out the best online marketing and promotional strategies for a business, A/B Testing can be employed for testing everything, ranging from sales emails to search ads and website copy.
4. How will you calculate the Sensitivity of machine learning models?
Answer: In machine learning, Sensitivity is used for validating the accuracy of a classifier, such as Logistic, Random Forest, and SVM. It is also known as REC (recall) or TPR (true positive rate).
Sensitivity can be defined as the ratio of predicted true events and total events i.e.:
Sensitivity = True Positives / Positives in Actual Dependent Variable
Here, true events are those events that were true as predicted by a machine learning model. The best sensitivity is 1.0 and the worst sensitivity is 0.0.
5. Could you draw a comparison between overfitting and underfitting?
Answer: In order to make reliable predictions on general untrained data in machine learning and statistics, it is required to fit a (machine learning) model to a set of training data. Overfitting and underfitting are two of the most common modeling errors that occur while doing so.
Following are the various differences between overfitting and underfitting:
•	Definition - A statistical model suffering from overfitting describes some random error or noise in place of the underlying relationship. When underfitting occurs, a statistical model or machine learning algorithm fails in capturing the underlying trend of the data.
•	Occurrence – When a statistical model or machine learning algorithm is excessively complex, it can result in overfitting. Example of a complex model is one having too many parameters when compared to the total number of observations. Underfitting occurs when trying to fit a linear model to non-linear data.
•	Poor Predictive Performance – Although both overfitting and underfitting yield poor predictive performance, the way in which each one of them does so is different. While the overfitted model overreacts to minor fluctuations in the training data, the underfit model under-reacts to even bigger fluctuations.
6. Between Python and R, which one would you pick for text analytics and why?
Answer: For text analytics, Python will gain an upper hand over R due to these reasons:
•	The Pandas library in Python offers easy-to-use data structures as well as high-performance data analysis tools
•	Python has a faster performance for all types of text analytics
•	R is a best-fit for machine learning than mere text analysis
7. Please explain the role of data cleaning in data analysis.
Answer: Data cleaning can be a daunting task due to the fact that with the increase in the number of data sources, the time required for cleaning the data increases at an exponential rate.
This is due to the vast volume of data generated by additional sources. Also, data cleaning can solely take up to 80% of the total time required for carrying out a data analysis task.
Nevertheless, there are several reasons for using data cleaning in data analysis. Two of the most important ones are:
•	Cleaning data from different sources helps in transforming the data into a format that is easy to work with
•	Data cleaning increases the accuracy of a machine learning model
8. What do you mean by cluster sampling and systematic sampling?
Answer: When studying the target population spread throughout a wide area becomes difficult and applying simple random sampling becomes ineffective, the technique of cluster sampling is used. A cluster sample is a probability sample, in which each of the sampling units is a collection or cluster of elements.
Following the technique of systematic sampling, elements are chosen from an ordered sampling frame. The list is advanced in a circular fashion. This is done in such a way so that once the end of the list is reached, the same is progressed from the start, or top, again.
9. Please explain Eigenvectors and Eigenvalues.
Answer: Eigenvectors help in understanding linear transformations. They are calculated typically for a correlation or covariance matrix in data analysis.
In other words, eigenvectors are those directions along which some particular linear transformation acts by compressing, flipping, or stretching.
Eigenvalues can be understood either as the strengths of the transformation in the direction of the eigenvectors or the factors by which the compressions happens.
10. Can you compare the validation set with the test set?
Answer: A validation set is part of the training set used for parameter selection as well as for avoiding overfitting of the machine learning model being developed. On the contrary, a test set is meant for evaluating or testing the performance of a trained machine learning model.
11. What do you understand by linear regression and logistic regression?
Answer: Linear regression is a form of statistical technique in which the score of some variable Y is predicted on the basis of the score of a second variable X, referred to as the predictor variable. The Y variable is known as the criterion variable.
Also known as the logit model, logistic regression is a statistical technique for predicting the binary outcome from a linear combination of predictor variables.
12. Please explain Recommender Systems along with an application.
Answer: Recommender Systems is a subclass of information filtering systems, meant for predicting the preferences or ratings awarded by a user to some product.
An application of a recommender system is the product recommendations section in Amazon. This section contains items based on the user’s search history and past orders.
13. What are outlier values and how do you treat them?
Answer: Outlier values, or simply outliers, are data points in statistics that don’t belong to a certain population. An outlier value is an abnormal observation that is very much different from other values belonging to the set.
Identification of outlier values can be done by using univariate or some other graphical analysis method. Few outlier values can be assessed individually but assessing a large set of outlier values require the substitution of the same with either the 99th or the 1st percentile values.
There are two popular ways of treating outlier values:
1.	To change the value so that it can be brought within a range
2.	To simply remove the value
Note: - Not all extreme values are outlier values.
14. Please enumerate the various steps involved in an analytics project.
Answer: Following are the numerous steps involved in an analytics project:
•	Understanding the business problem
•	Exploring the data and familiarizing with the same
•	Preparing the data for modeling by means of detecting outlier values, transforming variables, treating missing values, et cetera
•	Running the model and analyzing the result for making appropriate changes or modifications to the model (an iterative step that repeats until the best possible outcome is gained)
•	Validating the model using a new dataset
•	Implementing the model and tracking the result for analyzing the performance of the same
15. Could you explain how to define the number of clusters in a clustering algorithm?
Answer: The primary objective of clustering is to group together similar identities in such a way that while entities within a group are similar to each other, the groups remain different from one another.
Generally, Within Sum of Squares is used for explaining the homogeneity within a cluster. For defining the number of clusters in a clustering algorithm, WSS is plotted for a range pertaining to a number of clusters. The resultant graph is known as the Elbow Curve.
The Elbow Curve graph contains a point that represents the point post in which there aren’t any decrements in the WSS. This is known as the bending point and represents K in K–Means.
Although the aforementioned is the widely-used approach, another important approach is the Hierarchical clustering. In this approach, dendrograms are created first and then distinct groups are identified from there.
16. What do you understand by Deep Learning?
Answer: Deep Learning is a paradigm of machine learning that displays a great degree of analogy with the functioning of the human brain. It is a neural network method based on convolutional neural networks (CNN).
Deep learning has a wide array of uses, ranging from social network filtering to medical image analysis and speech recognition. Although Deep Learning has been present for a long time, it’s only recently that it has gained worldwide acclaim. This is mainly due to:
•	An increase in the amount of data generation via various sources
•	The growth in hardware resources required for running Deep Learning models
17. Please explain Gradient Descent.
Answer: The degree of change in the output of a function relating to the changes made to the inputs is known as a gradient. It measures the change in all weights with respect to the change in error. A gradient can also be comprehended as the slope of a function.
Gradient Descent refers to escalating down to the bottom of a valley. Simply, consider this something as opposed to climbing up a hill. It is a minimization algorithm meant for minimizing a given activation function.
18. How does Backpropagation work? Also, it state its various variants.
Answer: Backpropagation refers to a training algorithm used for multilayer neural networks. Following the backpropagation algorithm, the error is moved from an end of the network to all weights inside the network. Doing so allows for efficient computation of the gradient.
Backpropagation works in the following way:
•	Forward propagation of training data
•	Output and target is used for computing derivatives
•	Backpropagate for computing the derivative of the error with respect to the output activation
•	Using previously calculated derivatives for output generation
•	Updating the weights
Following are the various variants of Backpropagation:
•	Batch Gradient Descent – The gradient is calculated for the complete dataset and update is performed on each iteration
•	Mini-batch Gradient Descent – Mini-batch samples are used for calculating gradient and updating parameters (a variant of the Stochastic Gradient Descent approach)
•	Stochastic Gradient Descent – Only a single training example is used to calculate gradient and updating parameters
19. What do you know about Autoencoders?
Answer: Autoencoders are simplistic learning networks used for transforming inputs into outputs with minimum possible error. It means that the outputs resulted are very close to the inputs.
A couple of layers are added between the input and the output with the size of each layer smaller than the size pertaining to the input layer. An autoencoder receives unlabeled input that is encoded for reconstructing the output.
20. Please explain the concept of a Boltzmann Machine.
Answer: A Boltzmann Machine features a simple learning algorithm that enables the same to discover fascinating features representing complex regularities present in the training data. It is basically used for optimizing the quantity and weight for some given problem.
The simple learning algorithm involved in a Boltzmann Machine is very slow in networks that have many layers of feature detectors.
21. What are the skills required as a Data Scientist that could help in using Python for data analysis purposes?
Answer: The skills required as a Data Scientist that could help in using Python for data analysis purposes are stated under:
1.	Expertize in Pandas Dataframes, Scikit-learn, and N-dimensional NumPy Arrays.
2.	Skills to apply element-wise vector and matrix operations on NumPy arrays.
3.	Able to understand built-in data types, including tuples, sets, dictionaries, and various others.
4.	It is equipped with Anaconda distribution and the Conda package manager.
5.	Capability in writing efficient list comprehensions, small, clean functions, and avoid traditional for loops.
6.	Knowledge of Python script and optimizing bottlenecks
22. What is the full form of GAN? Explain GAN?
Answer: The full form of GAN is Generative Adversarial Network. Its task is to take inputs from the noise vector and send it forward to the Generator and then to Discriminator to identify and differentiate the unique and fake inputs.
23. What are the vital components of GAN?
Answer: There are two vital components of GAN. These include the following:
1.	Generator: The Generator act as a Forger, which creates fake copies.
2.	Discriminator: The Discriminator act as a recognizer for fake and unique (real) copies.
24. What is the Computational Graph?
Answer: A computational graph is a graphical presentation that is based on TensorFlow. It has a wide network of different kinds of nodes wherein each node represents a particular mathematical operation. The edges in these nodes are called tensors. This is the reason the computational graph is called a TensorFlow of inputs. The computational graph is characterized by data flows in the form of a graph; therefore, it is also called the DataFlow Graph.
25. What are tensors?
Answer: Tensors are the mathematical objects that represent the collection of higher dimensions of data inputs in the form of alphabets, numerals, and rank fed as inputs to the neural network.
26. Why are Tensorflow considered a high priority in learning Data Science?
Answer: Tensorflow is considered a high priority in learning Data Science because it provides support to using computer languages such as C++ and Python. This way, it makes various processes under data science to achieve faster compilation and completion within the stipulated time frame and faster than the conventional Keras and Torch libraries. Tensorflow supports the computing devices, including the CPU and GPU for faster inputs, editing, and analysis of the data.
27. What is Dropout in Data Science?
Answer: Dropout is a toll in Data Science, which is used for dropping out the hidden and visible units of a network on a random basis. They prevent the overfitting of the data by dropping as much as 20% of the nodes so that the required space can be arranged for iterations needed to converge the network.
28. What is Batch normalization in Data Science?
Answer: Batch Normalization in Data Science is a technique through which attempts could be made to improve the performance and stability of the neural network. This can be done by normalizing the inputs in each layer so that the mean output activation remains 0 with the standard deviation at 1.
29. What is the difference between Batch and Stochastic Gradient Descent?
Answer: The difference between Batch and Stochastic Gradient Descent can be displayed as follows:
Batch Gradient Descent	Stochastic Gradient Descent
It helps in computing the gradient using the complete data set available.	It helps in computing the gradient using only the single sample.
It takes time to converge.	It takes less time to converge.
The volume is huge for analysis purpose	The volume is lesser for analysis purposes.
It updates the weight slowly.	It updates the weight more frequently.
30.What are Auto-Encoders?
Answer: Auto-Encoders are learning networks that are meant to change inputs into output with the lowest chance of getting an error. They intend to keep the output closer to the input. The process of Autoencoders is needed to be done through the development of layers between the input and output. However, efforts are made to keep the size of these layers smaller for faster processing.
31. What are the various Machine Learning Libraries and their benefits?
Answer: The various machine learning libraries and their benefits are as follows.
1.	Numpy: It is used for scientific computation.
2.	Statsmodels: It is used for time-series analysis.
3.	Pandas: It is used for tubular data analysis.
4.	Scikit learns: It is used for data modeling and pre-processing.
5.	Tensorflow: It is used for the deep learning process.
6.	Regular Expressions: It is used for text processing.
7.	Pytorch: It is used for the deep learning process.
8.	NLTK: It is used for text processing.
32. What is an Activation function?
Answer: An Activation function helps in introducing the non-linearity in the neural network. This is done to help the learning process for complex functions. Without the activation function, the neural network will be unable to perform only the linear function and apply linear combinations. Activation function, therefore, offers complex functions and combinations by applying artificial neurons, which helps in delivering output based on the inputs.
33. What are the different types of Deep Learning Frameworks?
Answer: The different types of Deep Learning Framework includes the following:
1.	Caffe
2.	Keras
3.	TensorFlow
4.	Pytorch
5.	Chainer
6.	Microsoft Cognitive Toolkit
34. What are vanishing gradients?
Answer: The vanishing gradients is a condition when the slope is too small during the training process of RNN. The result of vanishing gradients is poor performance outcomes, low accuracy, and long term training processes.
35. What are exploding gradients?
Answer: The exploding gradients are a condition when the errors grow at an exponential rate or high rate during the training of RNN. This error gradient accumulates and results in applying large updates to the neural network, causes an overflow, and results in NaN values.
36. What is the full form of LSTM? What is its function?
Answer: LSTM stands for Long Short Term Memory. It is a recurrent neural network that is capable of learning long term dependencies and recalling information for the longer period as part of its default behavior.
37. What are the different steps in LSTM?
Answer: The different steps in LSTM include the following.
•	Step 1: The network helps in deciding the things that need to be remembered while others that need to be forgotten.
•	Step 2: The selection is made for cell state values that can be updated.
•	Step 3: The network decides as to what can be made as part of the current output.
38. What is Pooling on CNN?
Answer: Polling is a method that is used with the purpose to reduce the spatial dimensions of a CNN. It helps in performing downsampling operations for reducing dimensionality and creating pooled feature maps. Pooling in CNN helps in sliding the filter matrix over the input matrix.
49. What is RNN?
Answer: The RNN stands for Recurrent Neural Networks. They are an artificial neural network that is a sequence of data, including stock markets, sequence of data including stock markets, time series, and various others. The main idea behind the RNN application is to understand the basics of the feedforward nets.
40. What are the different layers on CNN?
Answer: There are four different layers on CNN. These include the following.
1.	Convolutional Layer: In this layer, several small picture windows are created to go over the data.
2.	ReLU Layer: This layer helps in bringing non-linearity to the network and converts the negative pixels to zero so that the output becomes a rectified feature map.
3.	Pooling Layer: This layer reduces the dimensionality of the feature map.
4.	Fully Connected Layer: This layer recognizes and classifies the objects in the image.
41. What is an Epoch in Data Science?
Answer: Epoch in Data Science represents one of the iterations over the entire dataset. It includes everything that is applied to the learning model.
42. What is a Batch in Data Science?
Answer: Batch is referred to as a different dataset that is divided into the form of different batches to help to pass the information into the system. It is developed in the situation when the developer cannot pass the entire dataset into the neural network at once.
43. What is the iteration in Data Science? Give an example?
Answer: Iteration in Data Science is applied by Epoch for analysis of data. The iteration is, therefore, classification of the data into different groups. For example, when there are 50,000 images, and the batch size is 100, then in such a case, the Epoch will run about 500 iterations.
44. What is the cost function?
Answer: Cost functions are a tool to evaluate how good the model performance has been made. It takes into consideration the errors and losses that are made in the output layer during the backpropagation process. In such a case, the errors are moved backward in the neural network, and various other training functions are applied.
45. What are hyperparameters?
Answer: Hyperparameter is a kind of parameter whose value is set before the learning process so that the network training requirements can be identified and the structure of the network can be improved. This process includes recognizing the hidden units, learning rate, epochs, and various others associated.
46. Which skills are important to become a certified Data Scientist?
Answer: The important skills to become a certified Data Scientist include the following:
1.	Knowledge of built-in data types including lists, tuples, sets and related.
2.	Expertize in N-dimensional NumPy Arrays.
3.	Ability to apply Pandas Dataframes.
4.	Strong hold over performance in element wise vectors.
5.	Knowledge of matrix operations on NumPy arrays.
47. What is an Artificial Neural Network in Data Science?
Answer: Artificial Neural Network in Data Science is the specific set of algorithms that are inspired by the biological neural network meant to adapt the changes in the input so that the best output can be achieved. It helps in generating the best possible results without the need to redesign the output methods.
48. What is Deep Learning in Data Science?
Answer: Deep Learning in Data Science is a name given to machine learning, which requires a great level of analogy with the functioning of the human brain. This way, it is a paradigm of machine learning.
49. Are there differences between Deep Learning and Machine Learning?
Answer: Yes, there are differences between Deep Learning and Machine learning. These are stated as under:
Deep Learning	Machine Learning
It gives computers the ability to learn without being explicitly programmed.	It gives computers a limited to unlimited ability wherein nothing major can be done without getting programmed, and many things can be done without the prior programming. It includes supervised, unsupervised, and reinforcement machine learning processes.
It is a subcomponent of machine learning that is concerned with algorithms that are inspired by the structure and functions of the human brains called the Artificial Neural Networks.	It includes Deep Learning as one of its components.
50.What is Ensemble learning?
Answer: Ensemble learning is a process of combining the diverse set of learners that is the individual models with each other. It helps in improving the stability and predictive power of the model.
Question: What are the different kinds of Ensemble learning?
Answer: The different kinds of Ensemble learning includes the following.
1.	Bagging: It implements simple learners on one small population and takes mean for estimation purposes.
2.	Boosting: It adjusts the weight of the observation and thereby classifies the population in different sets before the outcome prediction is made.

51. What is Naive Bayes?
Naive Bayes Machine Learning algorithm is a powerful algorithm for predictive modeling. It is a set of algorithms with a common principle based on Bayes Theorem. The fundamental Naive Bayes assumption is that each feature makes an independent and equal contribution to the outcome.
52. What is perceptron in Machine Learning?
Perceptron is an algorithm that is able to simulate the ability of the human brain to understand and discard; it is used for the supervised classification of the input into one of the several possible non-binary outputs.
53. List the extraction techniques used for dimensionality reduction.
•	Independent component analysis
•	Principal component analysis
•	Kernel-based principal component analysis
54. Is KNN different from K-means Clustering?
KNN	K-means Clustering
Supervised	Unsupervised
Classification algorithms	Clustering algorithms
Minimal training model	Exhaustive training model
Used in the classification and regression of the known data	Used in population demographics, market segmentation, social media trends, anomaly detection, etc.
55.What is ensemble learning?
Ensemble learning is a computational technique in which classifiers or experts are strategically formed and combined. It is used to improve classification, prediction, function approximation, etc. of a model.
56. List the steps involved in Machine Learning.
•	Data collection
•	Data preparation
•	Choosing an appropriate model
•	Training the dataset
•	Evaluation
•	Parameter tuning
•	Predictions
57. What is a hash table?
A hash table is a data structure that is used to produce an associative array which is mostly used for database indexing.
58. What is regularization in Machine Learning?
Regularization comes into the picture when a model is either overfit or underfit. It is basically used to minimize the error in a dataset. A new piece of information is fit into the dataset to avoid fitting issues.
59.What are the components of relational evaluation techniques?
•	Data acquisition
•	Ground truth acquisition
•	Cross validation technique
•	Query type
•	Scoring metric
•	Significance test
60.What is model accuracy and model performance?
Model accuracy, a subset of model performance, is based on the model performance of an algorithm. Whereas, model performance is based on the datasets we feed as inputs to the algorithm.
61. Define F1 score.
F1 score is the weighted average of precision and recall. It considers both false positive and false negative values into account. It is used to measure a model’s performance.
62. List the applications of Machine Learning.
•	Image, speech, and face detection
•	Bioinformatics
•	Market segmentation
•	Manufacturing and inventory management
•	Fraud detection, and so on
63. Can you name three feature selection techniques in Machine Learning?
1.	Univariate Selection
2.	Feature Importance
3.	Correlation Matrix with Heatmap
64.What is a recommendation system?
A recommendation system is an information filtering system that is used to predict user preference based on choice patterns followed by the user while browsing/using the system.
65.What methods are used for reducing dimensionality?
Dimensionality reduction is the process of reducing the number of random variables. We can reduce dimensionality using techniques such as missing values ratio, low variance filter, high correlation filter, random forest, principal component analysis, etc.
66. List different methods for sequential supervised learning.
•	Sliding window methods
•	Recurrent sliding windows methods
•	Hidden Markov models
•	Maximum entropy Markov models
•	Conditional random fields
•	Graph transformer networks
67. What are the advantages of neural networks?
•	Require less formal statistical training
•	Have the ability to detect nonlinear relationships between variables
•	Detect all possible interactions between predictor variables
•	Availability of multiple training algorithms
68. What is Bias–Variance tradeoff?
Bias error is used to measure how much on an average the predicted values vary from the actual values. In case a high-bias error occurs, we have an under-performing model.
Variance is used to measure how the predictions made on the same observation differ from each other. A high-variance model will overfit the dataset and perform badly on any observation.

69. What is TensorFlow?
TensorFlow is an open-source Machine Learning library. It is a fast, flexible, and low-level toolkit for doing complex algorithms and offers users customizability to build experimental learning architectures and to work on them to produce desired outputs.
70.How to install TensorFlow?
TensorFlow Installation Guide:
CPU : pip install tensorflow-cpu
GPU : pip install tensorflow-gpu
71.What are the TensorFlow objects?
1.	Constants
2.	Variables
3.	Placeholder
4.	Graph
5.	Session
72. What is a cost function?
A cost function is a scalar function that quantifies the error factor of the neural network. Lower the cost function better the neural network. For example, while classifying the image in the MNIST dataset, the input image is digit 2, but the neural network wrongly predicts it to be 3.
73. List different activation neurons or functions.
1.	Linear neuron
2.	Binary threshold neuron
3.	Stochastic binary neuron
4.	Sigmoid neuron
5.	Tanh function
6.	Rectified linear unit (ReLU)
74. What are the hyper parameters of ANN?
•	Learning rate: The learning rate is how fast the network learns its parameters.
•	Momentum: It is a parameter that helps to come out of the local minima and smoothen the jumps while gradient descent.
•	Number of epochs: The number of times the entire training data is fed to the network while training is referred to as the number of epochs. We increase the number of epochs until the validation accuracy starts decreasing, even if the training accuracy is increasing (overfitting).
75. What is vanishing gradient?
As we add more and more hidden layers, backpropagation becomes less useful in passing information to the lower layers. In effect, as information is passed back, the gradients begin to vanish and become small relative to the weights of the network.
76. What are dropouts?
Dropout is a simple way to prevent a neural network from overfitting. It is the dropping out of some of the units in a neural network. It is similar to the natural reproduction process, where the nature produces offsprings by combining distinct genes (dropping out others) rather than strengthening the co-adapting of them.
77. Define LSTM.
Long short-term memory (LSTM) is explicitly designed to address the long-term dependency problem, by maintaining a state of what to remember and what to forget.
78. List the key components of LSTM.
•	Gates (forget, Memory, update, and Read)
•	Tanh(x) (values between -1 and 1)
•	Sigmoid(x) (values between 0 and 1)
79. List the variants of RNN.
•	LSTM: Long Short-term Memory
•	GRU: Gated Recurrent Unit
•	End-to-end Network
•	Memory Network
80. What is an autoencoder? Name a few applications.
An autoencoder is basically used to learn a compressed form of the given data. A few applications of an autoencoder are given below:
1.	Data denoising
2.	Dimensionality reduction
3.	Image reconstruction
4.	Image colorization
81. What are the components of the generative adversarial network (GAN)? How do you deploy it?
Components of GAN:
•	Generator
•	Discriminator
Deployment Steps:
•	Train the model
•	Validate and finalize the model
•	Save the model
•	Load the saved model for the next prediction
82. What are the steps involved in the gradient descent algorithm?
Gradient descent is an optimization algorithm that is used to find the coefficients of parameters that are used to reduce the cost function to a minimum.
Step 1: Allocate weights (x,y) with random values and calculate the error (SSE)
Step 2: Calculate the gradient, i.e., the variation in SSE when the weights (x,y) are changed by a very small value. This helps us move the values of x and y in the direction in which SSE is minimized
Step 3: Adjust the weights with the gradients to move toward the optimal values where SSE is minimized
Step 4: Use new weights for prediction and calculating the new SSE
Step 5: Repeat Steps 2 and 3 until further adjustments to the weights do not significantly reduce the error
83. What do you understand by session in TensorFlow?
Syntax: Class Session
It is a class for running TensorFlow operations. The environment is encapsulated in the session object wherein the operation objects are executed and Tensor objects are evaluated.
# Build a graph
x = tf.constant(2.0)
y = tf.constant(5.0)
z = x * y
# Launch the graph in a session
sess = tf.Session()
# Evaluate the tensor `z`
print(sess.run(z))
84. What do you mean by TensorFlow cluster?
TensorFlow cluster is a set of ‘tasks’ that participate in the distributed execution of a TensorFlow graph. Each task is associated with a TensorFlow server, which contains a ‘master’ that can be used to create sessions and a ‘worker’ that executes operations in the graph. A cluster can also be divided into one or more ‘jobs’, where each job contains one or more tasks.

85. How to run TensorFlow on Hadoop?
To use HDFS with TensorFlow, we need to change the file path for reading and writing data to an HDFS path. For example:
filename_queue = tf.train.string_input_producer([
“hdfs://namenode:8020/path/to/file1.csv”,
“hdfs://namenode:8020/path/to/file2.csv”,
])
86. What are intermediate tensors? Do sessions have lifetime?
The intermediate tensors are tensors that are neither inputs nor outputs of the Session.run() call, but are in the path leading from the inputs to the outputs; they will be freed at or before the end of the call.
Sessions can own resources, few classes like tf.Variable, tf.QueueBase, and tf.ReaderBase, and they use a significant amount of memory. These resources (and the associated memory) are released when the session is closed, by calling tf.Session.close.

87. What is the lifetime of a variable?
When we first run the tf.Variable.initializer operation for a variable in a session, it is started. It is destroyed when we run the tf.Session.close operation.




Data Science

Basic Level-Q&A:
1. Can you enumerate the various differences between Supervised and Unsupervised Learning?
Answer: Supervised learning is a type of machine learning where a function is inferred from labeled training data. The training data contains a set of training examples.
Unsupervised learning, on the other hand, is a type of machine learning where inferences are drawn from datasets containing input data without labeled responses. Following are the various other differences between the two types of machine learning:
•	Algorithms Used – Supervised learning makes use of Decision Trees, K-nearest Neighbor algorithm, Neural Networks, Regression, and Support Vector Machines. Unsupervised learning uses Anomaly Detection, Clustering, Latent Variable Models, and Neural Networks.
•	Enables – Supervised learning enables classification and regression, whereas unsupervised learning enables classification, dimension reduction, and density estimation
•	Use – While supervised learning is used for prediction, unsupervised learning finds use in analysis
2. What do you understand by the Selection Bias? What are its various types?
Answer: Selection bias is typically associated with research that doesn’t have a random selection of participants. It is a type of error that occurs when a researcher decides who is going to be studied. On some occasions, selection bias is also referred to as the selection effect.
In other words, selection bias is a distortion of statistical analysis that results from the sample collecting method. When selection bias is not taken into account, some conclusions made by a research study might not be accurate. Following are the various types of selection bias:
•	Sampling Bias – A systematic error resulting due to a non-random sample of a populace causing certain members of the same to be less likely included than others that results in a biased sample.
•	Time Interval – A trial might be ended at an extreme value, usually due to ethical reasons, but the extreme value is most likely to be reached by the variable with the most variance, even though all variables have a similar mean.
•	Data – Results when specific data subsets are selected for supporting a conclusion or rejection of bad data arbitrarily.
•	Attrition – Caused due to attrition, i.e. loss of participants, discounting trial subjects or tests that didn’t run to completion.
3. Please explain the goal of A/B Testing.
Answer: A/B Testing is a statistical hypothesis testing meant for a randomized experiment with two variables, A and B. The goal of A/B Testing is to maximize the likelihood of an outcome of some interest by identifying any changes to a webpage.
A highly reliable method for finding out the best online marketing and promotional strategies for a business, A/B Testing can be employed for testing everything, ranging from sales emails to search ads and website copy.
4. How will you calculate the Sensitivity of machine learning models?
Answer: In machine learning, Sensitivity is used for validating the accuracy of a classifier, such as Logistic, Random Forest, and SVM. It is also known as REC (recall) or TPR (true positive rate).
Sensitivity can be defined as the ratio of predicted true events and total events i.e.:
Sensitivity = True Positives / Positives in Actual Dependent Variable
Here, true events are those events that were true as predicted by a machine learning model. The best sensitivity is 1.0 and the worst sensitivity is 0.0.
5. Could you draw a comparison between overfitting and underfitting?
Answer: In order to make reliable predictions on general untrained data in machine learning and statistics, it is required to fit a (machine learning) model to a set of training data. Overfitting and underfitting are two of the most common modeling errors that occur while doing so.
Following are the various differences between overfitting and underfitting:
•	Definition - A statistical model suffering from overfitting describes some random error or noise in place of the underlying relationship. When underfitting occurs, a statistical model or machine learning algorithm fails in capturing the underlying trend of the data.
•	Occurrence – When a statistical model or machine learning algorithm is excessively complex, it can result in overfitting. Example of a complex model is one having too many parameters when compared to the total number of observations. Underfitting occurs when trying to fit a linear model to non-linear data.
•	Poor Predictive Performance – Although both overfitting and underfitting yield poor predictive performance, the way in which each one of them does so is different. While the overfitted model overreacts to minor fluctuations in the training data, the underfit model under-reacts to even bigger fluctuations.
6. Between Python and R, which one would you pick for text analytics and why?
Answer: For text analytics, Python will gain an upper hand over R due to these reasons:
•	The Pandas library in Python offers easy-to-use data structures as well as high-performance data analysis tools
•	Python has a faster performance for all types of text analytics
•	R is a best-fit for machine learning than mere text analysis
7. Please explain the role of data cleaning in data analysis.
Answer: Data cleaning can be a daunting task due to the fact that with the increase in the number of data sources, the time required for cleaning the data increases at an exponential rate.
This is due to the vast volume of data generated by additional sources. Also, data cleaning can solely take up to 80% of the total time required for carrying out a data analysis task.
Nevertheless, there are several reasons for using data cleaning in data analysis. Two of the most important ones are:
•	Cleaning data from different sources helps in transforming the data into a format that is easy to work with
•	Data cleaning increases the accuracy of a machine learning model
8. What do you mean by cluster sampling and systematic sampling?
Answer: When studying the target population spread throughout a wide area becomes difficult and applying simple random sampling becomes ineffective, the technique of cluster sampling is used. A cluster sample is a probability sample, in which each of the sampling units is a collection or cluster of elements.
Following the technique of systematic sampling, elements are chosen from an ordered sampling frame. The list is advanced in a circular fashion. This is done in such a way so that once the end of the list is reached, the same is progressed from the start, or top, again.
9. Please explain Eigenvectors and Eigenvalues.
Answer: Eigenvectors help in understanding linear transformations. They are calculated typically for a correlation or covariance matrix in data analysis.
In other words, eigenvectors are those directions along which some particular linear transformation acts by compressing, flipping, or stretching.
Eigenvalues can be understood either as the strengths of the transformation in the direction of the eigenvectors or the factors by which the compressions happens.
10. Can you compare the validation set with the test set?
Answer: A validation set is part of the training set used for parameter selection as well as for avoiding overfitting of the machine learning model being developed. On the contrary, a test set is meant for evaluating or testing the performance of a trained machine learning model.
11. What do you understand by linear regression and logistic regression?
Answer: Linear regression is a form of statistical technique in which the score of some variable Y is predicted on the basis of the score of a second variable X, referred to as the predictor variable. The Y variable is known as the criterion variable.
Also known as the logit model, logistic regression is a statistical technique for predicting the binary outcome from a linear combination of predictor variables.
12. Please explain Recommender Systems along with an application.
Answer: Recommender Systems is a subclass of information filtering systems, meant for predicting the preferences or ratings awarded by a user to some product.
An application of a recommender system is the product recommendations section in Amazon. This section contains items based on the user’s search history and past orders.
13. What are outlier values and how do you treat them?
Answer: Outlier values, or simply outliers, are data points in statistics that don’t belong to a certain population. An outlier value is an abnormal observation that is very much different from other values belonging to the set.
Identification of outlier values can be done by using univariate or some other graphical analysis method. Few outlier values can be assessed individually but assessing a large set of outlier values require the substitution of the same with either the 99th or the 1st percentile values.
There are two popular ways of treating outlier values:
1.	To change the value so that it can be brought within a range
2.	To simply remove the value
Note: - Not all extreme values are outlier values.
14. Please enumerate the various steps involved in an analytics project.
Answer: Following are the numerous steps involved in an analytics project:
•	Understanding the business problem
•	Exploring the data and familiarizing with the same
•	Preparing the data for modeling by means of detecting outlier values, transforming variables, treating missing values, et cetera
•	Running the model and analyzing the result for making appropriate changes or modifications to the model (an iterative step that repeats until the best possible outcome is gained)
•	Validating the model using a new dataset
•	Implementing the model and tracking the result for analyzing the performance of the same
15. Could you explain how to define the number of clusters in a clustering algorithm?
Answer: The primary objective of clustering is to group together similar identities in such a way that while entities within a group are similar to each other, the groups remain different from one another.
Generally, Within Sum of Squares is used for explaining the homogeneity within a cluster. For defining the number of clusters in a clustering algorithm, WSS is plotted for a range pertaining to a number of clusters. The resultant graph is known as the Elbow Curve.
The Elbow Curve graph contains a point that represents the point post in which there aren’t any decrements in the WSS. This is known as the bending point and represents K in K–Means.
Although the aforementioned is the widely-used approach, another important approach is the Hierarchical clustering. In this approach, dendrograms are created first and then distinct groups are identified from there.
16. What do you understand by Deep Learning?
Answer: Deep Learning is a paradigm of machine learning that displays a great degree of analogy with the functioning of the human brain. It is a neural network method based on convolutional neural networks (CNN).
Deep learning has a wide array of uses, ranging from social network filtering to medical image analysis and speech recognition. Although Deep Learning has been present for a long time, it’s only recently that it has gained worldwide acclaim. This is mainly due to:
•	An increase in the amount of data generation via various sources
•	The growth in hardware resources required for running Deep Learning models
17. Please explain Gradient Descent.
Answer: The degree of change in the output of a function relating to the changes made to the inputs is known as a gradient. It measures the change in all weights with respect to the change in error. A gradient can also be comprehended as the slope of a function.
Gradient Descent refers to escalating down to the bottom of a valley. Simply, consider this something as opposed to climbing up a hill. It is a minimization algorithm meant for minimizing a given activation function.
18. How does Backpropagation work? Also, it state its various variants.
Answer: Backpropagation refers to a training algorithm used for multilayer neural networks. Following the backpropagation algorithm, the error is moved from an end of the network to all weights inside the network. Doing so allows for efficient computation of the gradient.
Backpropagation works in the following way:
•	Forward propagation of training data
•	Output and target is used for computing derivatives
•	Backpropagate for computing the derivative of the error with respect to the output activation
•	Using previously calculated derivatives for output generation
•	Updating the weights
Following are the various variants of Backpropagation:
•	Batch Gradient Descent – The gradient is calculated for the complete dataset and update is performed on each iteration
•	Mini-batch Gradient Descent – Mini-batch samples are used for calculating gradient and updating parameters (a variant of the Stochastic Gradient Descent approach)
•	Stochastic Gradient Descent – Only a single training example is used to calculate gradient and updating parameters
19. What do you know about Autoencoders?
Answer: Autoencoders are simplistic learning networks used for transforming inputs into outputs with minimum possible error. It means that the outputs resulted are very close to the inputs.
A couple of layers are added between the input and the output with the size of each layer smaller than the size pertaining to the input layer. An autoencoder receives unlabeled input that is encoded for reconstructing the output.
20. Please explain the concept of a Boltzmann Machine.
Answer: A Boltzmann Machine features a simple learning algorithm that enables the same to discover fascinating features representing complex regularities present in the training data. It is basically used for optimizing the quantity and weight for some given problem.
The simple learning algorithm involved in a Boltzmann Machine is very slow in networks that have many layers of feature detectors.
21. What are the skills required as a Data Scientist that could help in using Python for data analysis purposes?
Answer: The skills required as a Data Scientist that could help in using Python for data analysis purposes are stated under:
1.	Expertize in Pandas Dataframes, Scikit-learn, and N-dimensional NumPy Arrays.
2.	Skills to apply element-wise vector and matrix operations on NumPy arrays.
3.	Able to understand built-in data types, including tuples, sets, dictionaries, and various others.
4.	It is equipped with Anaconda distribution and the Conda package manager.
5.	Capability in writing efficient list comprehensions, small, clean functions, and avoid traditional for loops.
6.	Knowledge of Python script and optimizing bottlenecks
22. What is the full form of GAN? Explain GAN?
Answer: The full form of GAN is Generative Adversarial Network. Its task is to take inputs from the noise vector and send it forward to the Generator and then to Discriminator to identify and differentiate the unique and fake inputs.
23. What are the vital components of GAN?
Answer: There are two vital components of GAN. These include the following:
1.	Generator: The Generator act as a Forger, which creates fake copies.
2.	Discriminator: The Discriminator act as a recognizer for fake and unique (real) copies.
24. What is the Computational Graph?
Answer: A computational graph is a graphical presentation that is based on TensorFlow. It has a wide network of different kinds of nodes wherein each node represents a particular mathematical operation. The edges in these nodes are called tensors. This is the reason the computational graph is called a TensorFlow of inputs. The computational graph is characterized by data flows in the form of a graph; therefore, it is also called the DataFlow Graph.
25. What are tensors?
Answer: Tensors are the mathematical objects that represent the collection of higher dimensions of data inputs in the form of alphabets, numerals, and rank fed as inputs to the neural network.
26. Why are Tensorflow considered a high priority in learning Data Science?
Answer: Tensorflow is considered a high priority in learning Data Science because it provides support to using computer languages such as C++ and Python. This way, it makes various processes under data science to achieve faster compilation and completion within the stipulated time frame and faster than the conventional Keras and Torch libraries. Tensorflow supports the computing devices, including the CPU and GPU for faster inputs, editing, and analysis of the data.
27. What is Dropout in Data Science?
Answer: Dropout is a toll in Data Science, which is used for dropping out the hidden and visible units of a network on a random basis. They prevent the overfitting of the data by dropping as much as 20% of the nodes so that the required space can be arranged for iterations needed to converge the network.
28. What is Batch normalization in Data Science?
Answer: Batch Normalization in Data Science is a technique through which attempts could be made to improve the performance and stability of the neural network. This can be done by normalizing the inputs in each layer so that the mean output activation remains 0 with the standard deviation at 1.
29. What is the difference between Batch and Stochastic Gradient Descent?
Answer: The difference between Batch and Stochastic Gradient Descent can be displayed as follows:
Batch Gradient Descent	Stochastic Gradient Descent
It helps in computing the gradient using the complete data set available.	It helps in computing the gradient using only the single sample.
It takes time to converge.	It takes less time to converge.
The volume is huge for analysis purpose	The volume is lesser for analysis purposes.
It updates the weight slowly.	It updates the weight more frequently.
30.What are Auto-Encoders?
Answer: Auto-Encoders are learning networks that are meant to change inputs into output with the lowest chance of getting an error. They intend to keep the output closer to the input. The process of Autoencoders is needed to be done through the development of layers between the input and output. However, efforts are made to keep the size of these layers smaller for faster processing.
31. What are the various Machine Learning Libraries and their benefits?
Answer: The various machine learning libraries and their benefits are as follows.
1.	Numpy: It is used for scientific computation.
2.	Statsmodels: It is used for time-series analysis.
3.	Pandas: It is used for tubular data analysis.
4.	Scikit learns: It is used for data modeling and pre-processing.
5.	Tensorflow: It is used for the deep learning process.
6.	Regular Expressions: It is used for text processing.
7.	Pytorch: It is used for the deep learning process.
8.	NLTK: It is used for text processing.
32. What is an Activation function?
Answer: An Activation function helps in introducing the non-linearity in the neural network. This is done to help the learning process for complex functions. Without the activation function, the neural network will be unable to perform only the linear function and apply linear combinations. Activation function, therefore, offers complex functions and combinations by applying artificial neurons, which helps in delivering output based on the inputs.
33. What are the different types of Deep Learning Frameworks?
Answer: The different types of Deep Learning Framework includes the following:
1.	Caffe
2.	Keras
3.	TensorFlow
4.	Pytorch
5.	Chainer
6.	Microsoft Cognitive Toolkit
34. What are vanishing gradients?
Answer: The vanishing gradients is a condition when the slope is too small during the training process of RNN. The result of vanishing gradients is poor performance outcomes, low accuracy, and long term training processes.
35. What are exploding gradients?
Answer: The exploding gradients are a condition when the errors grow at an exponential rate or high rate during the training of RNN. This error gradient accumulates and results in applying large updates to the neural network, causes an overflow, and results in NaN values.
36. What is the full form of LSTM? What is its function?
Answer: LSTM stands for Long Short Term Memory. It is a recurrent neural network that is capable of learning long term dependencies and recalling information for the longer period as part of its default behavior.
37. What are the different steps in LSTM?
Answer: The different steps in LSTM include the following.
•	Step 1: The network helps in deciding the things that need to be remembered while others that need to be forgotten.
•	Step 2: The selection is made for cell state values that can be updated.
•	Step 3: The network decides as to what can be made as part of the current output.
38. What is Pooling on CNN?
Answer: Polling is a method that is used with the purpose to reduce the spatial dimensions of a CNN. It helps in performing downsampling operations for reducing dimensionality and creating pooled feature maps. Pooling in CNN helps in sliding the filter matrix over the input matrix.
49. What is RNN?
Answer: The RNN stands for Recurrent Neural Networks. They are an artificial neural network that is a sequence of data, including stock markets, sequence of data including stock markets, time series, and various others. The main idea behind the RNN application is to understand the basics of the feedforward nets.
40. What are the different layers on CNN?
Answer: There are four different layers on CNN. These include the following.
1.	Convolutional Layer: In this layer, several small picture windows are created to go over the data.
2.	ReLU Layer: This layer helps in bringing non-linearity to the network and converts the negative pixels to zero so that the output becomes a rectified feature map.
3.	Pooling Layer: This layer reduces the dimensionality of the feature map.
4.	Fully Connected Layer: This layer recognizes and classifies the objects in the image.
41. What is an Epoch in Data Science?
Answer: Epoch in Data Science represents one of the iterations over the entire dataset. It includes everything that is applied to the learning model.
42. What is a Batch in Data Science?
Answer: Batch is referred to as a different dataset that is divided into the form of different batches to help to pass the information into the system. It is developed in the situation when the developer cannot pass the entire dataset into the neural network at once.
43. What is the iteration in Data Science? Give an example?
Answer: Iteration in Data Science is applied by Epoch for analysis of data. The iteration is, therefore, classification of the data into different groups. For example, when there are 50,000 images, and the batch size is 100, then in such a case, the Epoch will run about 500 iterations.
44. What is the cost function?
Answer: Cost functions are a tool to evaluate how good the model performance has been made. It takes into consideration the errors and losses that are made in the output layer during the backpropagation process. In such a case, the errors are moved backward in the neural network, and various other training functions are applied.
45. What are hyperparameters?
Answer: Hyperparameter is a kind of parameter whose value is set before the learning process so that the network training requirements can be identified and the structure of the network can be improved. This process includes recognizing the hidden units, learning rate, epochs, and various others associated.
46. Which skills are important to become a certified Data Scientist?
Answer: The important skills to become a certified Data Scientist include the following:
1.	Knowledge of built-in data types including lists, tuples, sets and related.
2.	Expertize in N-dimensional NumPy Arrays.
3.	Ability to apply Pandas Dataframes.
4.	Strong hold over performance in element wise vectors.
5.	Knowledge of matrix operations on NumPy arrays.
47. What is an Artificial Neural Network in Data Science?
Answer: Artificial Neural Network in Data Science is the specific set of algorithms that are inspired by the biological neural network meant to adapt the changes in the input so that the best output can be achieved. It helps in generating the best possible results without the need to redesign the output methods.
48. What is Deep Learning in Data Science?
Answer: Deep Learning in Data Science is a name given to machine learning, which requires a great level of analogy with the functioning of the human brain. This way, it is a paradigm of machine learning.
49. Are there differences between Deep Learning and Machine Learning?
Answer: Yes, there are differences between Deep Learning and Machine learning. These are stated as under:
Deep Learning	Machine Learning
It gives computers the ability to learn without being explicitly programmed.	It gives computers a limited to unlimited ability wherein nothing major can be done without getting programmed, and many things can be done without the prior programming. It includes supervised, unsupervised, and reinforcement machine learning processes.
It is a subcomponent of machine learning that is concerned with algorithms that are inspired by the structure and functions of the human brains called the Artificial Neural Networks.	It includes Deep Learning as one of its components.
50.What is Ensemble learning?
Answer: Ensemble learning is a process of combining the diverse set of learners that is the individual models with each other. It helps in improving the stability and predictive power of the model.
Question: What are the different kinds of Ensemble learning?
Answer: The different kinds of Ensemble learning includes the following.
1.	Bagging: It implements simple learners on one small population and takes mean for estimation purposes.
2.	Boosting: It adjusts the weight of the observation and thereby classifies the population in different sets before the outcome prediction is made.

51. What is Naive Bayes?
Naive Bayes Machine Learning algorithm is a powerful algorithm for predictive modeling. It is a set of algorithms with a common principle based on Bayes Theorem. The fundamental Naive Bayes assumption is that each feature makes an independent and equal contribution to the outcome.
52. What is perceptron in Machine Learning?
Perceptron is an algorithm that is able to simulate the ability of the human brain to understand and discard; it is used for the supervised classification of the input into one of the several possible non-binary outputs.
53. List the extraction techniques used for dimensionality reduction.
•	Independent component analysis
•	Principal component analysis
•	Kernel-based principal component analysis
54. Is KNN different from K-means Clustering?
KNN	K-means Clustering
Supervised	Unsupervised
Classification algorithms	Clustering algorithms
Minimal training model	Exhaustive training model
Used in the classification and regression of the known data	Used in population demographics, market segmentation, social media trends, anomaly detection, etc.
55.What is ensemble learning?
Ensemble learning is a computational technique in which classifiers or experts are strategically formed and combined. It is used to improve classification, prediction, function approximation, etc. of a model.
56. List the steps involved in Machine Learning.
•	Data collection
•	Data preparation
•	Choosing an appropriate model
•	Training the dataset
•	Evaluation
•	Parameter tuning
•	Predictions
57. What is a hash table?
A hash table is a data structure that is used to produce an associative array which is mostly used for database indexing.
58. What is regularization in Machine Learning?
Regularization comes into the picture when a model is either overfit or underfit. It is basically used to minimize the error in a dataset. A new piece of information is fit into the dataset to avoid fitting issues.
59.What are the components of relational evaluation techniques?
•	Data acquisition
•	Ground truth acquisition
•	Cross validation technique
•	Query type
•	Scoring metric
•	Significance test
60.What is model accuracy and model performance?
Model accuracy, a subset of model performance, is based on the model performance of an algorithm. Whereas, model performance is based on the datasets we feed as inputs to the algorithm.
61. Define F1 score.
F1 score is the weighted average of precision and recall. It considers both false positive and false negative values into account. It is used to measure a model’s performance.
62. List the applications of Machine Learning.
•	Image, speech, and face detection
•	Bioinformatics
•	Market segmentation
•	Manufacturing and inventory management
•	Fraud detection, and so on
63. Can you name three feature selection techniques in Machine Learning?
1.	Univariate Selection
2.	Feature Importance
3.	Correlation Matrix with Heatmap
64.What is a recommendation system?
A recommendation system is an information filtering system that is used to predict user preference based on choice patterns followed by the user while browsing/using the system.
65.What methods are used for reducing dimensionality?
Dimensionality reduction is the process of reducing the number of random variables. We can reduce dimensionality using techniques such as missing values ratio, low variance filter, high correlation filter, random forest, principal component analysis, etc.
66. List different methods for sequential supervised learning.
•	Sliding window methods
•	Recurrent sliding windows methods
•	Hidden Markov models
•	Maximum entropy Markov models
•	Conditional random fields
•	Graph transformer networks
67. What are the advantages of neural networks?
•	Require less formal statistical training
•	Have the ability to detect nonlinear relationships between variables
•	Detect all possible interactions between predictor variables
•	Availability of multiple training algorithms
68. What is Bias–Variance tradeoff?
Bias error is used to measure how much on an average the predicted values vary from the actual values. In case a high-bias error occurs, we have an under-performing model.
Variance is used to measure how the predictions made on the same observation differ from each other. A high-variance model will overfit the dataset and perform badly on any observation.

69. What is TensorFlow?
TensorFlow is an open-source Machine Learning library. It is a fast, flexible, and low-level toolkit for doing complex algorithms and offers users customizability to build experimental learning architectures and to work on them to produce desired outputs.
70.How to install TensorFlow?
TensorFlow Installation Guide:
CPU : pip install tensorflow-cpu
GPU : pip install tensorflow-gpu
71.What are the TensorFlow objects?
1.	Constants
2.	Variables
3.	Placeholder
4.	Graph
5.	Session
72. What is a cost function?
A cost function is a scalar function that quantifies the error factor of the neural network. Lower the cost function better the neural network. For example, while classifying the image in the MNIST dataset, the input image is digit 2, but the neural network wrongly predicts it to be 3.
73. List different activation neurons or functions.
1.	Linear neuron
2.	Binary threshold neuron
3.	Stochastic binary neuron
4.	Sigmoid neuron
5.	Tanh function
6.	Rectified linear unit (ReLU)
74. What are the hyper parameters of ANN?
•	Learning rate: The learning rate is how fast the network learns its parameters.
•	Momentum: It is a parameter that helps to come out of the local minima and smoothen the jumps while gradient descent.
•	Number of epochs: The number of times the entire training data is fed to the network while training is referred to as the number of epochs. We increase the number of epochs until the validation accuracy starts decreasing, even if the training accuracy is increasing (overfitting).
75. What is vanishing gradient?
As we add more and more hidden layers, backpropagation becomes less useful in passing information to the lower layers. In effect, as information is passed back, the gradients begin to vanish and become small relative to the weights of the network.
76. What are dropouts?
Dropout is a simple way to prevent a neural network from overfitting. It is the dropping out of some of the units in a neural network. It is similar to the natural reproduction process, where the nature produces offsprings by combining distinct genes (dropping out others) rather than strengthening the co-adapting of them.
77. Define LSTM.
Long short-term memory (LSTM) is explicitly designed to address the long-term dependency problem, by maintaining a state of what to remember and what to forget.
78. List the key components of LSTM.
•	Gates (forget, Memory, update, and Read)
•	Tanh(x) (values between -1 and 1)
•	Sigmoid(x) (values between 0 and 1)
79. List the variants of RNN.
•	LSTM: Long Short-term Memory
•	GRU: Gated Recurrent Unit
•	End-to-end Network
•	Memory Network
80. What is an autoencoder? Name a few applications.
An autoencoder is basically used to learn a compressed form of the given data. A few applications of an autoencoder are given below:
1.	Data denoising
2.	Dimensionality reduction
3.	Image reconstruction
4.	Image colorization
81. What are the components of the generative adversarial network (GAN)? How do you deploy it?
Components of GAN:
•	Generator
•	Discriminator
Deployment Steps:
•	Train the model
•	Validate and finalize the model
•	Save the model
•	Load the saved model for the next prediction
82. What are the steps involved in the gradient descent algorithm?
Gradient descent is an optimization algorithm that is used to find the coefficients of parameters that are used to reduce the cost function to a minimum.
Step 1: Allocate weights (x,y) with random values and calculate the error (SSE)
Step 2: Calculate the gradient, i.e., the variation in SSE when the weights (x,y) are changed by a very small value. This helps us move the values of x and y in the direction in which SSE is minimized
Step 3: Adjust the weights with the gradients to move toward the optimal values where SSE is minimized
Step 4: Use new weights for prediction and calculating the new SSE
Step 5: Repeat Steps 2 and 3 until further adjustments to the weights do not significantly reduce the error
83. What do you understand by session in TensorFlow?
Syntax: Class Session
It is a class for running TensorFlow operations. The environment is encapsulated in the session object wherein the operation objects are executed and Tensor objects are evaluated.
# Build a graph
x = tf.constant(2.0)
y = tf.constant(5.0)
z = x * y
# Launch the graph in a session
sess = tf.Session()
# Evaluate the tensor `z`
print(sess.run(z))
84. What do you mean by TensorFlow cluster?
TensorFlow cluster is a set of ‘tasks’ that participate in the distributed execution of a TensorFlow graph. Each task is associated with a TensorFlow server, which contains a ‘master’ that can be used to create sessions and a ‘worker’ that executes operations in the graph. A cluster can also be divided into one or more ‘jobs’, where each job contains one or more tasks.

85. How to run TensorFlow on Hadoop?
To use HDFS with TensorFlow, we need to change the file path for reading and writing data to an HDFS path. For example:
filename_queue = tf.train.string_input_producer([
“hdfs://namenode:8020/path/to/file1.csv”,
“hdfs://namenode:8020/path/to/file2.csv”,
])
86. What are intermediate tensors? Do sessions have lifetime?
The intermediate tensors are tensors that are neither inputs nor outputs of the Session.run() call, but are in the path leading from the inputs to the outputs; they will be freed at or before the end of the call.
Sessions can own resources, few classes like tf.Variable, tf.QueueBase, and tf.ReaderBase, and they use a significant amount of memory. These resources (and the associated memory) are released when the session is closed, by calling tf.Session.close.

87. What is the lifetime of a variable?
When we first run the tf.Variable.initializer operation for a variable in a session, it is started. It is destroyed when we run the tf.Session.close operation.




Data Science

Basic Level-Q&A:
1. Can you enumerate the various differences between Supervised and Unsupervised Learning?
Answer: Supervised learning is a type of machine learning where a function is inferred from labeled training data. The training data contains a set of training examples.
Unsupervised learning, on the other hand, is a type of machine learning where inferences are drawn from datasets containing input data without labeled responses. Following are the various other differences between the two types of machine learning:
•	Algorithms Used – Supervised learning makes use of Decision Trees, K-nearest Neighbor algorithm, Neural Networks, Regression, and Support Vector Machines. Unsupervised learning uses Anomaly Detection, Clustering, Latent Variable Models, and Neural Networks.
•	Enables – Supervised learning enables classification and regression, whereas unsupervised learning enables classification, dimension reduction, and density estimation
•	Use – While supervised learning is used for prediction, unsupervised learning finds use in analysis
2. What do you understand by the Selection Bias? What are its various types?
Answer: Selection bias is typically associated with research that doesn’t have a random selection of participants. It is a type of error that occurs when a researcher decides who is going to be studied. On some occasions, selection bias is also referred to as the selection effect.
In other words, selection bias is a distortion of statistical analysis that results from the sample collecting method. When selection bias is not taken into account, some conclusions made by a research study might not be accurate. Following are the various types of selection bias:
•	Sampling Bias – A systematic error resulting due to a non-random sample of a populace causing certain members of the same to be less likely included than others that results in a biased sample.
•	Time Interval – A trial might be ended at an extreme value, usually due to ethical reasons, but the extreme value is most likely to be reached by the variable with the most variance, even though all variables have a similar mean.
•	Data – Results when specific data subsets are selected for supporting a conclusion or rejection of bad data arbitrarily.
•	Attrition – Caused due to attrition, i.e. loss of participants, discounting trial subjects or tests that didn’t run to completion.
3. Please explain the goal of A/B Testing.
Answer: A/B Testing is a statistical hypothesis testing meant for a randomized experiment with two variables, A and B. The goal of A/B Testing is to maximize the likelihood of an outcome of some interest by identifying any changes to a webpage.
A highly reliable method for finding out the best online marketing and promotional strategies for a business, A/B Testing can be employed for testing everything, ranging from sales emails to search ads and website copy.
4. How will you calculate the Sensitivity of machine learning models?
Answer: In machine learning, Sensitivity is used for validating the accuracy of a classifier, such as Logistic, Random Forest, and SVM. It is also known as REC (recall) or TPR (true positive rate).
Sensitivity can be defined as the ratio of predicted true events and total events i.e.:
Sensitivity = True Positives / Positives in Actual Dependent Variable
Here, true events are those events that were true as predicted by a machine learning model. The best sensitivity is 1.0 and the worst sensitivity is 0.0.
5. Could you draw a comparison between overfitting and underfitting?
Answer: In order to make reliable predictions on general untrained data in machine learning and statistics, it is required to fit a (machine learning) model to a set of training data. Overfitting and underfitting are two of the most common modeling errors that occur while doing so.
Following are the various differences between overfitting and underfitting:
•	Definition - A statistical model suffering from overfitting describes some random error or noise in place of the underlying relationship. When underfitting occurs, a statistical model or machine learning algorithm fails in capturing the underlying trend of the data.
•	Occurrence – When a statistical model or machine learning algorithm is excessively complex, it can result in overfitting. Example of a complex model is one having too many parameters when compared to the total number of observations. Underfitting occurs when trying to fit a linear model to non-linear data.
•	Poor Predictive Performance – Although both overfitting and underfitting yield poor predictive performance, the way in which each one of them does so is different. While the overfitted model overreacts to minor fluctuations in the training data, the underfit model under-reacts to even bigger fluctuations.
6. Between Python and R, which one would you pick for text analytics and why?
Answer: For text analytics, Python will gain an upper hand over R due to these reasons:
•	The Pandas library in Python offers easy-to-use data structures as well as high-performance data analysis tools
•	Python has a faster performance for all types of text analytics
•	R is a best-fit for machine learning than mere text analysis
7. Please explain the role of data cleaning in data analysis.
Answer: Data cleaning can be a daunting task due to the fact that with the increase in the number of data sources, the time required for cleaning the data increases at an exponential rate.
This is due to the vast volume of data generated by additional sources. Also, data cleaning can solely take up to 80% of the total time required for carrying out a data analysis task.
Nevertheless, there are several reasons for using data cleaning in data analysis. Two of the most important ones are:
•	Cleaning data from different sources helps in transforming the data into a format that is easy to work with
•	Data cleaning increases the accuracy of a machine learning model
8. What do you mean by cluster sampling and systematic sampling?
Answer: When studying the target population spread throughout a wide area becomes difficult and applying simple random sampling becomes ineffective, the technique of cluster sampling is used. A cluster sample is a probability sample, in which each of the sampling units is a collection or cluster of elements.
Following the technique of systematic sampling, elements are chosen from an ordered sampling frame. The list is advanced in a circular fashion. This is done in such a way so that once the end of the list is reached, the same is progressed from the start, or top, again.
9. Please explain Eigenvectors and Eigenvalues.
Answer: Eigenvectors help in understanding linear transformations. They are calculated typically for a correlation or covariance matrix in data analysis.
In other words, eigenvectors are those directions along which some particular linear transformation acts by compressing, flipping, or stretching.
Eigenvalues can be understood either as the strengths of the transformation in the direction of the eigenvectors or the factors by which the compressions happens.
10. Can you compare the validation set with the test set?
Answer: A validation set is part of the training set used for parameter selection as well as for avoiding overfitting of the machine learning model being developed. On the contrary, a test set is meant for evaluating or testing the performance of a trained machine learning model.
11. What do you understand by linear regression and logistic regression?
Answer: Linear regression is a form of statistical technique in which the score of some variable Y is predicted on the basis of the score of a second variable X, referred to as the predictor variable. The Y variable is known as the criterion variable.
Also known as the logit model, logistic regression is a statistical technique for predicting the binary outcome from a linear combination of predictor variables.
12. Please explain Recommender Systems along with an application.
Answer: Recommender Systems is a subclass of information filtering systems, meant for predicting the preferences or ratings awarded by a user to some product.
An application of a recommender system is the product recommendations section in Amazon. This section contains items based on the user’s search history and past orders.
13. What are outlier values and how do you treat them?
Answer: Outlier values, or simply outliers, are data points in statistics that don’t belong to a certain population. An outlier value is an abnormal observation that is very much different from other values belonging to the set.
Identification of outlier values can be done by using univariate or some other graphical analysis method. Few outlier values can be assessed individually but assessing a large set of outlier values require the substitution of the same with either the 99th or the 1st percentile values.
There are two popular ways of treating outlier values:
1.	To change the value so that it can be brought within a range
2.	To simply remove the value
Note: - Not all extreme values are outlier values.
14. Please enumerate the various steps involved in an analytics project.
Answer: Following are the numerous steps involved in an analytics project:
•	Understanding the business problem
•	Exploring the data and familiarizing with the same
•	Preparing the data for modeling by means of detecting outlier values, transforming variables, treating missing values, et cetera
•	Running the model and analyzing the result for making appropriate changes or modifications to the model (an iterative step that repeats until the best possible outcome is gained)
•	Validating the model using a new dataset
•	Implementing the model and tracking the result for analyzing the performance of the same
15. Could you explain how to define the number of clusters in a clustering algorithm?
Answer: The primary objective of clustering is to group together similar identities in such a way that while entities within a group are similar to each other, the groups remain different from one another.
Generally, Within Sum of Squares is used for explaining the homogeneity within a cluster. For defining the number of clusters in a clustering algorithm, WSS is plotted for a range pertaining to a number of clusters. The resultant graph is known as the Elbow Curve.
The Elbow Curve graph contains a point that represents the point post in which there aren’t any decrements in the WSS. This is known as the bending point and represents K in K–Means.
Although the aforementioned is the widely-used approach, another important approach is the Hierarchical clustering. In this approach, dendrograms are created first and then distinct groups are identified from there.
16. What do you understand by Deep Learning?
Answer: Deep Learning is a paradigm of machine learning that displays a great degree of analogy with the functioning of the human brain. It is a neural network method based on convolutional neural networks (CNN).
Deep learning has a wide array of uses, ranging from social network filtering to medical image analysis and speech recognition. Although Deep Learning has been present for a long time, it’s only recently that it has gained worldwide acclaim. This is mainly due to:
•	An increase in the amount of data generation via various sources
•	The growth in hardware resources required for running Deep Learning models
17. Please explain Gradient Descent.
Answer: The degree of change in the output of a function relating to the changes made to the inputs is known as a gradient. It measures the change in all weights with respect to the change in error. A gradient can also be comprehended as the slope of a function.
Gradient Descent refers to escalating down to the bottom of a valley. Simply, consider this something as opposed to climbing up a hill. It is a minimization algorithm meant for minimizing a given activation function.
18. How does Backpropagation work? Also, it state its various variants.
Answer: Backpropagation refers to a training algorithm used for multilayer neural networks. Following the backpropagation algorithm, the error is moved from an end of the network to all weights inside the network. Doing so allows for efficient computation of the gradient.
Backpropagation works in the following way:
•	Forward propagation of training data
•	Output and target is used for computing derivatives
•	Backpropagate for computing the derivative of the error with respect to the output activation
•	Using previously calculated derivatives for output generation
•	Updating the weights
Following are the various variants of Backpropagation:
•	Batch Gradient Descent – The gradient is calculated for the complete dataset and update is performed on each iteration
•	Mini-batch Gradient Descent – Mini-batch samples are used for calculating gradient and updating parameters (a variant of the Stochastic Gradient Descent approach)
•	Stochastic Gradient Descent – Only a single training example is used to calculate gradient and updating parameters
19. What do you know about Autoencoders?
Answer: Autoencoders are simplistic learning networks used for transforming inputs into outputs with minimum possible error. It means that the outputs resulted are very close to the inputs.
A couple of layers are added between the input and the output with the size of each layer smaller than the size pertaining to the input layer. An autoencoder receives unlabeled input that is encoded for reconstructing the output.
20. Please explain the concept of a Boltzmann Machine.
Answer: A Boltzmann Machine features a simple learning algorithm that enables the same to discover fascinating features representing complex regularities present in the training data. It is basically used for optimizing the quantity and weight for some given problem.
The simple learning algorithm involved in a Boltzmann Machine is very slow in networks that have many layers of feature detectors.
21. What are the skills required as a Data Scientist that could help in using Python for data analysis purposes?
Answer: The skills required as a Data Scientist that could help in using Python for data analysis purposes are stated under:
1.	Expertize in Pandas Dataframes, Scikit-learn, and N-dimensional NumPy Arrays.
2.	Skills to apply element-wise vector and matrix operations on NumPy arrays.
3.	Able to understand built-in data types, including tuples, sets, dictionaries, and various others.
4.	It is equipped with Anaconda distribution and the Conda package manager.
5.	Capability in writing efficient list comprehensions, small, clean functions, and avoid traditional for loops.
6.	Knowledge of Python script and optimizing bottlenecks
22. What is the full form of GAN? Explain GAN?
Answer: The full form of GAN is Generative Adversarial Network. Its task is to take inputs from the noise vector and send it forward to the Generator and then to Discriminator to identify and differentiate the unique and fake inputs.
23. What are the vital components of GAN?
Answer: There are two vital components of GAN. These include the following:
1.	Generator: The Generator act as a Forger, which creates fake copies.
2.	Discriminator: The Discriminator act as a recognizer for fake and unique (real) copies.
24. What is the Computational Graph?
Answer: A computational graph is a graphical presentation that is based on TensorFlow. It has a wide network of different kinds of nodes wherein each node represents a particular mathematical operation. The edges in these nodes are called tensors. This is the reason the computational graph is called a TensorFlow of inputs. The computational graph is characterized by data flows in the form of a graph; therefore, it is also called the DataFlow Graph.
25. What are tensors?
Answer: Tensors are the mathematical objects that represent the collection of higher dimensions of data inputs in the form of alphabets, numerals, and rank fed as inputs to the neural network.
26. Why are Tensorflow considered a high priority in learning Data Science?
Answer: Tensorflow is considered a high priority in learning Data Science because it provides support to using computer languages such as C++ and Python. This way, it makes various processes under data science to achieve faster compilation and completion within the stipulated time frame and faster than the conventional Keras and Torch libraries. Tensorflow supports the computing devices, including the CPU and GPU for faster inputs, editing, and analysis of the data.
27. What is Dropout in Data Science?
Answer: Dropout is a toll in Data Science, which is used for dropping out the hidden and visible units of a network on a random basis. They prevent the overfitting of the data by dropping as much as 20% of the nodes so that the required space can be arranged for iterations needed to converge the network.
28. What is Batch normalization in Data Science?
Answer: Batch Normalization in Data Science is a technique through which attempts could be made to improve the performance and stability of the neural network. This can be done by normalizing the inputs in each layer so that the mean output activation remains 0 with the standard deviation at 1.
29. What is the difference between Batch and Stochastic Gradient Descent?
Answer: The difference between Batch and Stochastic Gradient Descent can be displayed as follows:
Batch Gradient Descent	Stochastic Gradient Descent
It helps in computing the gradient using the complete data set available.	It helps in computing the gradient using only the single sample.
It takes time to converge.	It takes less time to converge.
The volume is huge for analysis purpose	The volume is lesser for analysis purposes.
It updates the weight slowly.	It updates the weight more frequently.
30.What are Auto-Encoders?
Answer: Auto-Encoders are learning networks that are meant to change inputs into output with the lowest chance of getting an error. They intend to keep the output closer to the input. The process of Autoencoders is needed to be done through the development of layers between the input and output. However, efforts are made to keep the size of these layers smaller for faster processing.
31. What are the various Machine Learning Libraries and their benefits?
Answer: The various machine learning libraries and their benefits are as follows.
1.	Numpy: It is used for scientific computation.
2.	Statsmodels: It is used for time-series analysis.
3.	Pandas: It is used for tubular data analysis.
4.	Scikit learns: It is used for data modeling and pre-processing.
5.	Tensorflow: It is used for the deep learning process.
6.	Regular Expressions: It is used for text processing.
7.	Pytorch: It is used for the deep learning process.
8.	NLTK: It is used for text processing.
32. What is an Activation function?
Answer: An Activation function helps in introducing the non-linearity in the neural network. This is done to help the learning process for complex functions. Without the activation function, the neural network will be unable to perform only the linear function and apply linear combinations. Activation function, therefore, offers complex functions and combinations by applying artificial neurons, which helps in delivering output based on the inputs.
33. What are the different types of Deep Learning Frameworks?
Answer: The different types of Deep Learning Framework includes the following:
1.	Caffe
2.	Keras
3.	TensorFlow
4.	Pytorch
5.	Chainer
6.	Microsoft Cognitive Toolkit
34. What are vanishing gradients?
Answer: The vanishing gradients is a condition when the slope is too small during the training process of RNN. The result of vanishing gradients is poor performance outcomes, low accuracy, and long term training processes.
35. What are exploding gradients?
Answer: The exploding gradients are a condition when the errors grow at an exponential rate or high rate during the training of RNN. This error gradient accumulates and results in applying large updates to the neural network, causes an overflow, and results in NaN values.
36. What is the full form of LSTM? What is its function?
Answer: LSTM stands for Long Short Term Memory. It is a recurrent neural network that is capable of learning long term dependencies and recalling information for the longer period as part of its default behavior.
37. What are the different steps in LSTM?
Answer: The different steps in LSTM include the following.
•	Step 1: The network helps in deciding the things that need to be remembered while others that need to be forgotten.
•	Step 2: The selection is made for cell state values that can be updated.
•	Step 3: The network decides as to what can be made as part of the current output.
38. What is Pooling on CNN?
Answer: Polling is a method that is used with the purpose to reduce the spatial dimensions of a CNN. It helps in performing downsampling operations for reducing dimensionality and creating pooled feature maps. Pooling in CNN helps in sliding the filter matrix over the input matrix.
49. What is RNN?
Answer: The RNN stands for Recurrent Neural Networks. They are an artificial neural network that is a sequence of data, including stock markets, sequence of data including stock markets, time series, and various others. The main idea behind the RNN application is to understand the basics of the feedforward nets.
40. What are the different layers on CNN?
Answer: There are four different layers on CNN. These include the following.
1.	Convolutional Layer: In this layer, several small picture windows are created to go over the data.
2.	ReLU Layer: This layer helps in bringing non-linearity to the network and converts the negative pixels to zero so that the output becomes a rectified feature map.
3.	Pooling Layer: This layer reduces the dimensionality of the feature map.
4.	Fully Connected Layer: This layer recognizes and classifies the objects in the image.
41. What is an Epoch in Data Science?
Answer: Epoch in Data Science represents one of the iterations over the entire dataset. It includes everything that is applied to the learning model.
42. What is a Batch in Data Science?
Answer: Batch is referred to as a different dataset that is divided into the form of different batches to help to pass the information into the system. It is developed in the situation when the developer cannot pass the entire dataset into the neural network at once.
43. What is the iteration in Data Science? Give an example?
Answer: Iteration in Data Science is applied by Epoch for analysis of data. The iteration is, therefore, classification of the data into different groups. For example, when there are 50,000 images, and the batch size is 100, then in such a case, the Epoch will run about 500 iterations.
44. What is the cost function?
Answer: Cost functions are a tool to evaluate how good the model performance has been made. It takes into consideration the errors and losses that are made in the output layer during the backpropagation process. In such a case, the errors are moved backward in the neural network, and various other training functions are applied.
45. What are hyperparameters?
Answer: Hyperparameter is a kind of parameter whose value is set before the learning process so that the network training requirements can be identified and the structure of the network can be improved. This process includes recognizing the hidden units, learning rate, epochs, and various others associated.
46. Which skills are important to become a certified Data Scientist?
Answer: The important skills to become a certified Data Scientist include the following:
1.	Knowledge of built-in data types including lists, tuples, sets and related.
2.	Expertize in N-dimensional NumPy Arrays.
3.	Ability to apply Pandas Dataframes.
4.	Strong hold over performance in element wise vectors.
5.	Knowledge of matrix operations on NumPy arrays.
47. What is an Artificial Neural Network in Data Science?
Answer: Artificial Neural Network in Data Science is the specific set of algorithms that are inspired by the biological neural network meant to adapt the changes in the input so that the best output can be achieved. It helps in generating the best possible results without the need to redesign the output methods.
48. What is Deep Learning in Data Science?
Answer: Deep Learning in Data Science is a name given to machine learning, which requires a great level of analogy with the functioning of the human brain. This way, it is a paradigm of machine learning.
49. Are there differences between Deep Learning and Machine Learning?
Answer: Yes, there are differences between Deep Learning and Machine learning. These are stated as under:
Deep Learning	Machine Learning
It gives computers the ability to learn without being explicitly programmed.	It gives computers a limited to unlimited ability wherein nothing major can be done without getting programmed, and many things can be done without the prior programming. It includes supervised, unsupervised, and reinforcement machine learning processes.
It is a subcomponent of machine learning that is concerned with algorithms that are inspired by the structure and functions of the human brains called the Artificial Neural Networks.	It includes Deep Learning as one of its components.
50.What is Ensemble learning?
Answer: Ensemble learning is a process of combining the diverse set of learners that is the individual models with each other. It helps in improving the stability and predictive power of the model.
Question: What are the different kinds of Ensemble learning?
Answer: The different kinds of Ensemble learning includes the following.
1.	Bagging: It implements simple learners on one small population and takes mean for estimation purposes.
2.	Boosting: It adjusts the weight of the observation and thereby classifies the population in different sets before the outcome prediction is made.

51. What is Naive Bayes?
Naive Bayes Machine Learning algorithm is a powerful algorithm for predictive modeling. It is a set of algorithms with a common principle based on Bayes Theorem. The fundamental Naive Bayes assumption is that each feature makes an independent and equal contribution to the outcome.
52. What is perceptron in Machine Learning?
Perceptron is an algorithm that is able to simulate the ability of the human brain to understand and discard; it is used for the supervised classification of the input into one of the several possible non-binary outputs.
53. List the extraction techniques used for dimensionality reduction.
•	Independent component analysis
•	Principal component analysis
•	Kernel-based principal component analysis
54. Is KNN different from K-means Clustering?
KNN	K-means Clustering
Supervised	Unsupervised
Classification algorithms	Clustering algorithms
Minimal training model	Exhaustive training model
Used in the classification and regression of the known data	Used in population demographics, market segmentation, social media trends, anomaly detection, etc.
55.What is ensemble learning?
Ensemble learning is a computational technique in which classifiers or experts are strategically formed and combined. It is used to improve classification, prediction, function approximation, etc. of a model.
56. List the steps involved in Machine Learning.
•	Data collection
•	Data preparation
•	Choosing an appropriate model
•	Training the dataset
•	Evaluation
•	Parameter tuning
•	Predictions
57. What is a hash table?
A hash table is a data structure that is used to produce an associative array which is mostly used for database indexing.
58. What is regularization in Machine Learning?
Regularization comes into the picture when a model is either overfit or underfit. It is basically used to minimize the error in a dataset. A new piece of information is fit into the dataset to avoid fitting issues.
59.What are the components of relational evaluation techniques?
•	Data acquisition
•	Ground truth acquisition
•	Cross validation technique
•	Query type
•	Scoring metric
•	Significance test
60.What is model accuracy and model performance?
Model accuracy, a subset of model performance, is based on the model performance of an algorithm. Whereas, model performance is based on the datasets we feed as inputs to the algorithm.
61. Define F1 score.
F1 score is the weighted average of precision and recall. It considers both false positive and false negative values into account. It is used to measure a model’s performance.
62. List the applications of Machine Learning.
•	Image, speech, and face detection
•	Bioinformatics
•	Market segmentation
•	Manufacturing and inventory management
•	Fraud detection, and so on
63. Can you name three feature selection techniques in Machine Learning?
1.	Univariate Selection
2.	Feature Importance
3.	Correlation Matrix with Heatmap
64.What is a recommendation system?
A recommendation system is an information filtering system that is used to predict user preference based on choice patterns followed by the user while browsing/using the system.
65.What methods are used for reducing dimensionality?
Dimensionality reduction is the process of reducing the number of random variables. We can reduce dimensionality using techniques such as missing values ratio, low variance filter, high correlation filter, random forest, principal component analysis, etc.
66. List different methods for sequential supervised learning.
•	Sliding window methods
•	Recurrent sliding windows methods
•	Hidden Markov models
•	Maximum entropy Markov models
•	Conditional random fields
•	Graph transformer networks
67. What are the advantages of neural networks?
•	Require less formal statistical training
•	Have the ability to detect nonlinear relationships between variables
•	Detect all possible interactions between predictor variables
•	Availability of multiple training algorithms
68. What is Bias–Variance tradeoff?
Bias error is used to measure how much on an average the predicted values vary from the actual values. In case a high-bias error occurs, we have an under-performing model.
Variance is used to measure how the predictions made on the same observation differ from each other. A high-variance model will overfit the dataset and perform badly on any observation.

69. What is TensorFlow?
TensorFlow is an open-source Machine Learning library. It is a fast, flexible, and low-level toolkit for doing complex algorithms and offers users customizability to build experimental learning architectures and to work on them to produce desired outputs.
70.How to install TensorFlow?
TensorFlow Installation Guide:
CPU : pip install tensorflow-cpu
GPU : pip install tensorflow-gpu
71.What are the TensorFlow objects?
1.	Constants
2.	Variables
3.	Placeholder
4.	Graph
5.	Session
72. What is a cost function?
A cost function is a scalar function that quantifies the error factor of the neural network. Lower the cost function better the neural network. For example, while classifying the image in the MNIST dataset, the input image is digit 2, but the neural network wrongly predicts it to be 3.
73. List different activation neurons or functions.
1.	Linear neuron
2.	Binary threshold neuron
3.	Stochastic binary neuron
4.	Sigmoid neuron
5.	Tanh function
6.	Rectified linear unit (ReLU)
74. What are the hyper parameters of ANN?
•	Learning rate: The learning rate is how fast the network learns its parameters.
•	Momentum: It is a parameter that helps to come out of the local minima and smoothen the jumps while gradient descent.
•	Number of epochs: The number of times the entire training data is fed to the network while training is referred to as the number of epochs. We increase the number of epochs until the validation accuracy starts decreasing, even if the training accuracy is increasing (overfitting).
75. What is vanishing gradient?
As we add more and more hidden layers, backpropagation becomes less useful in passing information to the lower layers. In effect, as information is passed back, the gradients begin to vanish and become small relative to the weights of the network.
76. What are dropouts?
Dropout is a simple way to prevent a neural network from overfitting. It is the dropping out of some of the units in a neural network. It is similar to the natural reproduction process, where the nature produces offsprings by combining distinct genes (dropping out others) rather than strengthening the co-adapting of them.
77. Define LSTM.
Long short-term memory (LSTM) is explicitly designed to address the long-term dependency problem, by maintaining a state of what to remember and what to forget.
78. List the key components of LSTM.
•	Gates (forget, Memory, update, and Read)
•	Tanh(x) (values between -1 and 1)
•	Sigmoid(x) (values between 0 and 1)
79. List the variants of RNN.
•	LSTM: Long Short-term Memory
•	GRU: Gated Recurrent Unit
•	End-to-end Network
•	Memory Network
80. What is an autoencoder? Name a few applications.
An autoencoder is basically used to learn a compressed form of the given data. A few applications of an autoencoder are given below:
1.	Data denoising
2.	Dimensionality reduction
3.	Image reconstruction
4.	Image colorization
81. What are the components of the generative adversarial network (GAN)? How do you deploy it?
Components of GAN:
•	Generator
•	Discriminator
Deployment Steps:
•	Train the model
•	Validate and finalize the model
•	Save the model
•	Load the saved model for the next prediction
82. What are the steps involved in the gradient descent algorithm?
Gradient descent is an optimization algorithm that is used to find the coefficients of parameters that are used to reduce the cost function to a minimum.
Step 1: Allocate weights (x,y) with random values and calculate the error (SSE)
Step 2: Calculate the gradient, i.e., the variation in SSE when the weights (x,y) are changed by a very small value. This helps us move the values of x and y in the direction in which SSE is minimized
Step 3: Adjust the weights with the gradients to move toward the optimal values where SSE is minimized
Step 4: Use new weights for prediction and calculating the new SSE
Step 5: Repeat Steps 2 and 3 until further adjustments to the weights do not significantly reduce the error
83. What do you understand by session in TensorFlow?
Syntax: Class Session
It is a class for running TensorFlow operations. The environment is encapsulated in the session object wherein the operation objects are executed and Tensor objects are evaluated.
# Build a graph
x = tf.constant(2.0)
y = tf.constant(5.0)
z = x * y
# Launch the graph in a session
sess = tf.Session()
# Evaluate the tensor `z`
print(sess.run(z))
84. What do you mean by TensorFlow cluster?
TensorFlow cluster is a set of ‘tasks’ that participate in the distributed execution of a TensorFlow graph. Each task is associated with a TensorFlow server, which contains a ‘master’ that can be used to create sessions and a ‘worker’ that executes operations in the graph. A cluster can also be divided into one or more ‘jobs’, where each job contains one or more tasks.

85. How to run TensorFlow on Hadoop?
To use HDFS with TensorFlow, we need to change the file path for reading and writing data to an HDFS path. For example:
filename_queue = tf.train.string_input_producer([
“hdfs://namenode:8020/path/to/file1.csv”,
“hdfs://namenode:8020/path/to/file2.csv”,
])
86. What are intermediate tensors? Do sessions have lifetime?
The intermediate tensors are tensors that are neither inputs nor outputs of the Session.run() call, but are in the path leading from the inputs to the outputs; they will be freed at or before the end of the call.
Sessions can own resources, few classes like tf.Variable, tf.QueueBase, and tf.ReaderBase, and they use a significant amount of memory. These resources (and the associated memory) are released when the session is closed, by calling tf.Session.close.

87. What is the lifetime of a variable?
When we first run the tf.Variable.initializer operation for a variable in a session, it is started. It is destroyed when we run the tf.Session.close operation.




Data Science

Basic Level-Q&A:
1. Can you enumerate the various differences between Supervised and Unsupervised Learning?
Answer: Supervised learning is a type of machine learning where a function is inferred from labeled training data. The training data contains a set of training examples.
Unsupervised learning, on the other hand, is a type of machine learning where inferences are drawn from datasets containing input data without labeled responses. Following are the various other differences between the two types of machine learning:
•	Algorithms Used – Supervised learning makes use of Decision Trees, K-nearest Neighbor algorithm, Neural Networks, Regression, and Support Vector Machines. Unsupervised learning uses Anomaly Detection, Clustering, Latent Variable Models, and Neural Networks.
•	Enables – Supervised learning enables classification and regression, whereas unsupervised learning enables classification, dimension reduction, and density estimation
•	Use – While supervised learning is used for prediction, unsupervised learning finds use in analysis
2. What do you understand by the Selection Bias? What are its various types?
Answer: Selection bias is typically associated with research that doesn’t have a random selection of participants. It is a type of error that occurs when a researcher decides who is going to be studied. On some occasions, selection bias is also referred to as the selection effect.
In other words, selection bias is a distortion of statistical analysis that results from the sample collecting method. When selection bias is not taken into account, some conclusions made by a research study might not be accurate. Following are the various types of selection bias:
•	Sampling Bias – A systematic error resulting due to a non-random sample of a populace causing certain members of the same to be less likely included than others that results in a biased sample.
•	Time Interval – A trial might be ended at an extreme value, usually due to ethical reasons, but the extreme value is most likely to be reached by the variable with the most variance, even though all variables have a similar mean.
•	Data – Results when specific data subsets are selected for supporting a conclusion or rejection of bad data arbitrarily.
•	Attrition – Caused due to attrition, i.e. loss of participants, discounting trial subjects or tests that didn’t run to completion.
3. Please explain the goal of A/B Testing.
Answer: A/B Testing is a statistical hypothesis testing meant for a randomized experiment with two variables, A and B. The goal of A/B Testing is to maximize the likelihood of an outcome of some interest by identifying any changes to a webpage.
A highly reliable method for finding out the best online marketing and promotional strategies for a business, A/B Testing can be employed for testing everything, ranging from sales emails to search ads and website copy.
4. How will you calculate the Sensitivity of machine learning models?
Answer: In machine learning, Sensitivity is used for validating the accuracy of a classifier, such as Logistic, Random Forest, and SVM. It is also known as REC (recall) or TPR (true positive rate).
Sensitivity can be defined as the ratio of predicted true events and total events i.e.:
Sensitivity = True Positives / Positives in Actual Dependent Variable
Here, true events are those events that were true as predicted by a machine learning model. The best sensitivity is 1.0 and the worst sensitivity is 0.0.
5. Could you draw a comparison between overfitting and underfitting?
Answer: In order to make reliable predictions on general untrained data in machine learning and statistics, it is required to fit a (machine learning) model to a set of training data. Overfitting and underfitting are two of the most common modeling errors that occur while doing so.
Following are the various differences between overfitting and underfitting:
•	Definition - A statistical model suffering from overfitting describes some random error or noise in place of the underlying relationship. When underfitting occurs, a statistical model or machine learning algorithm fails in capturing the underlying trend of the data.
•	Occurrence – When a statistical model or machine learning algorithm is excessively complex, it can result in overfitting. Example of a complex model is one having too many parameters when compared to the total number of observations. Underfitting occurs when trying to fit a linear model to non-linear data.
•	Poor Predictive Performance – Although both overfitting and underfitting yield poor predictive performance, the way in which each one of them does so is different. While the overfitted model overreacts to minor fluctuations in the training data, the underfit model under-reacts to even bigger fluctuations.
6. Between Python and R, which one would you pick for text analytics and why?
Answer: For text analytics, Python will gain an upper hand over R due to these reasons:
•	The Pandas library in Python offers easy-to-use data structures as well as high-performance data analysis tools
•	Python has a faster performance for all types of text analytics
•	R is a best-fit for machine learning than mere text analysis
7. Please explain the role of data cleaning in data analysis.
Answer: Data cleaning can be a daunting task due to the fact that with the increase in the number of data sources, the time required for cleaning the data increases at an exponential rate.
This is due to the vast volume of data generated by additional sources. Also, data cleaning can solely take up to 80% of the total time required for carrying out a data analysis task.
Nevertheless, there are several reasons for using data cleaning in data analysis. Two of the most important ones are:
•	Cleaning data from different sources helps in transforming the data into a format that is easy to work with
•	Data cleaning increases the accuracy of a machine learning model
8. What do you mean by cluster sampling and systematic sampling?
Answer: When studying the target population spread throughout a wide area becomes difficult and applying simple random sampling becomes ineffective, the technique of cluster sampling is used. A cluster sample is a probability sample, in which each of the sampling units is a collection or cluster of elements.
Following the technique of systematic sampling, elements are chosen from an ordered sampling frame. The list is advanced in a circular fashion. This is done in such a way so that once the end of the list is reached, the same is progressed from the start, or top, again.
9. Please explain Eigenvectors and Eigenvalues.
Answer: Eigenvectors help in understanding linear transformations. They are calculated typically for a correlation or covariance matrix in data analysis.
In other words, eigenvectors are those directions along which some particular linear transformation acts by compressing, flipping, or stretching.
Eigenvalues can be understood either as the strengths of the transformation in the direction of the eigenvectors or the factors by which the compressions happens.
10. Can you compare the validation set with the test set?
Answer: A validation set is part of the training set used for parameter selection as well as for avoiding overfitting of the machine learning model being developed. On the contrary, a test set is meant for evaluating or testing the performance of a trained machine learning model.
11. What do you understand by linear regression and logistic regression?
Answer: Linear regression is a form of statistical technique in which the score of some variable Y is predicted on the basis of the score of a second variable X, referred to as the predictor variable. The Y variable is known as the criterion variable.
Also known as the logit model, logistic regression is a statistical technique for predicting the binary outcome from a linear combination of predictor variables.
12. Please explain Recommender Systems along with an application.
Answer: Recommender Systems is a subclass of information filtering systems, meant for predicting the preferences or ratings awarded by a user to some product.
An application of a recommender system is the product recommendations section in Amazon. This section contains items based on the user’s search history and past orders.
13. What are outlier values and how do you treat them?
Answer: Outlier values, or simply outliers, are data points in statistics that don’t belong to a certain population. An outlier value is an abnormal observation that is very much different from other values belonging to the set.
Identification of outlier values can be done by using univariate or some other graphical analysis method. Few outlier values can be assessed individually but assessing a large set of outlier values require the substitution of the same with either the 99th or the 1st percentile values.
There are two popular ways of treating outlier values:
1.	To change the value so that it can be brought within a range
2.	To simply remove the value
Note: - Not all extreme values are outlier values.
14. Please enumerate the various steps involved in an analytics project.
Answer: Following are the numerous steps involved in an analytics project:
•	Understanding the business problem
•	Exploring the data and familiarizing with the same
•	Preparing the data for modeling by means of detecting outlier values, transforming variables, treating missing values, et cetera
•	Running the model and analyzing the result for making appropriate changes or modifications to the model (an iterative step that repeats until the best possible outcome is gained)
•	Validating the model using a new dataset
•	Implementing the model and tracking the result for analyzing the performance of the same
15. Could you explain how to define the number of clusters in a clustering algorithm?
Answer: The primary objective of clustering is to group together similar identities in such a way that while entities within a group are similar to each other, the groups remain different from one another.
Generally, Within Sum of Squares is used for explaining the homogeneity within a cluster. For defining the number of clusters in a clustering algorithm, WSS is plotted for a range pertaining to a number of clusters. The resultant graph is known as the Elbow Curve.
The Elbow Curve graph contains a point that represents the point post in which there aren’t any decrements in the WSS. This is known as the bending point and represents K in K–Means.
Although the aforementioned is the widely-used approach, another important approach is the Hierarchical clustering. In this approach, dendrograms are created first and then distinct groups are identified from there.
16. What do you understand by Deep Learning?
Answer: Deep Learning is a paradigm of machine learning that displays a great degree of analogy with the functioning of the human brain. It is a neural network method based on convolutional neural networks (CNN).
Deep learning has a wide array of uses, ranging from social network filtering to medical image analysis and speech recognition. Although Deep Learning has been present for a long time, it’s only recently that it has gained worldwide acclaim. This is mainly due to:
•	An increase in the amount of data generation via various sources
•	The growth in hardware resources required for running Deep Learning models
17. Please explain Gradient Descent.
Answer: The degree of change in the output of a function relating to the changes made to the inputs is known as a gradient. It measures the change in all weights with respect to the change in error. A gradient can also be comprehended as the slope of a function.
Gradient Descent refers to escalating down to the bottom of a valley. Simply, consider this something as opposed to climbing up a hill. It is a minimization algorithm meant for minimizing a given activation function.
18. How does Backpropagation work? Also, it state its various variants.
Answer: Backpropagation refers to a training algorithm used for multilayer neural networks. Following the backpropagation algorithm, the error is moved from an end of the network to all weights inside the network. Doing so allows for efficient computation of the gradient.
Backpropagation works in the following way:
•	Forward propagation of training data
•	Output and target is used for computing derivatives
•	Backpropagate for computing the derivative of the error with respect to the output activation
•	Using previously calculated derivatives for output generation
•	Updating the weights
Following are the various variants of Backpropagation:
•	Batch Gradient Descent – The gradient is calculated for the complete dataset and update is performed on each iteration
•	Mini-batch Gradient Descent – Mini-batch samples are used for calculating gradient and updating parameters (a variant of the Stochastic Gradient Descent approach)
•	Stochastic Gradient Descent – Only a single training example is used to calculate gradient and updating parameters
19. What do you know about Autoencoders?
Answer: Autoencoders are simplistic learning networks used for transforming inputs into outputs with minimum possible error. It means that the outputs resulted are very close to the inputs.
A couple of layers are added between the input and the output with the size of each layer smaller than the size pertaining to the input layer. An autoencoder receives unlabeled input that is encoded for reconstructing the output.
20. Please explain the concept of a Boltzmann Machine.
Answer: A Boltzmann Machine features a simple learning algorithm that enables the same to discover fascinating features representing complex regularities present in the training data. It is basically used for optimizing the quantity and weight for some given problem.
The simple learning algorithm involved in a Boltzmann Machine is very slow in networks that have many layers of feature detectors.
21. What are the skills required as a Data Scientist that could help in using Python for data analysis purposes?
Answer: The skills required as a Data Scientist that could help in using Python for data analysis purposes are stated under:
1.	Expertize in Pandas Dataframes, Scikit-learn, and N-dimensional NumPy Arrays.
2.	Skills to apply element-wise vector and matrix operations on NumPy arrays.
3.	Able to understand built-in data types, including tuples, sets, dictionaries, and various others.
4.	It is equipped with Anaconda distribution and the Conda package manager.
5.	Capability in writing efficient list comprehensions, small, clean functions, and avoid traditional for loops.
6.	Knowledge of Python script and optimizing bottlenecks
22. What is the full form of GAN? Explain GAN?
Answer: The full form of GAN is Generative Adversarial Network. Its task is to take inputs from the noise vector and send it forward to the Generator and then to Discriminator to identify and differentiate the unique and fake inputs.
23. What are the vital components of GAN?
Answer: There are two vital components of GAN. These include the following:
1.	Generator: The Generator act as a Forger, which creates fake copies.
2.	Discriminator: The Discriminator act as a recognizer for fake and unique (real) copies.
24. What is the Computational Graph?
Answer: A computational graph is a graphical presentation that is based on TensorFlow. It has a wide network of different kinds of nodes wherein each node represents a particular mathematical operation. The edges in these nodes are called tensors. This is the reason the computational graph is called a TensorFlow of inputs. The computational graph is characterized by data flows in the form of a graph; therefore, it is also called the DataFlow Graph.
25. What are tensors?
Answer: Tensors are the mathematical objects that represent the collection of higher dimensions of data inputs in the form of alphabets, numerals, and rank fed as inputs to the neural network.
26. Why are Tensorflow considered a high priority in learning Data Science?
Answer: Tensorflow is considered a high priority in learning Data Science because it provides support to using computer languages such as C++ and Python. This way, it makes various processes under data science to achieve faster compilation and completion within the stipulated time frame and faster than the conventional Keras and Torch libraries. Tensorflow supports the computing devices, including the CPU and GPU for faster inputs, editing, and analysis of the data.
27. What is Dropout in Data Science?
Answer: Dropout is a toll in Data Science, which is used for dropping out the hidden and visible units of a network on a random basis. They prevent the overfitting of the data by dropping as much as 20% of the nodes so that the required space can be arranged for iterations needed to converge the network.
28. What is Batch normalization in Data Science?
Answer: Batch Normalization in Data Science is a technique through which attempts could be made to improve the performance and stability of the neural network. This can be done by normalizing the inputs in each layer so that the mean output activation remains 0 with the standard deviation at 1.
29. What is the difference between Batch and Stochastic Gradient Descent?
Answer: The difference between Batch and Stochastic Gradient Descent can be displayed as follows:
Batch Gradient Descent	Stochastic Gradient Descent
It helps in computing the gradient using the complete data set available.	It helps in computing the gradient using only the single sample.
It takes time to converge.	It takes less time to converge.
The volume is huge for analysis purpose	The volume is lesser for analysis purposes.
It updates the weight slowly.	It updates the weight more frequently.
30.What are Auto-Encoders?
Answer: Auto-Encoders are learning networks that are meant to change inputs into output with the lowest chance of getting an error. They intend to keep the output closer to the input. The process of Autoencoders is needed to be done through the development of layers between the input and output. However, efforts are made to keep the size of these layers smaller for faster processing.
31. What are the various Machine Learning Libraries and their benefits?
Answer: The various machine learning libraries and their benefits are as follows.
1.	Numpy: It is used for scientific computation.
2.	Statsmodels: It is used for time-series analysis.
3.	Pandas: It is used for tubular data analysis.
4.	Scikit learns: It is used for data modeling and pre-processing.
5.	Tensorflow: It is used for the deep learning process.
6.	Regular Expressions: It is used for text processing.
7.	Pytorch: It is used for the deep learning process.
8.	NLTK: It is used for text processing.
32. What is an Activation function?
Answer: An Activation function helps in introducing the non-linearity in the neural network. This is done to help the learning process for complex functions. Without the activation function, the neural network will be unable to perform only the linear function and apply linear combinations. Activation function, therefore, offers complex functions and combinations by applying artificial neurons, which helps in delivering output based on the inputs.
33. What are the different types of Deep Learning Frameworks?
Answer: The different types of Deep Learning Framework includes the following:
1.	Caffe
2.	Keras
3.	TensorFlow
4.	Pytorch
5.	Chainer
6.	Microsoft Cognitive Toolkit
34. What are vanishing gradients?
Answer: The vanishing gradients is a condition when the slope is too small during the training process of RNN. The result of vanishing gradients is poor performance outcomes, low accuracy, and long term training processes.
35. What are exploding gradients?
Answer: The exploding gradients are a condition when the errors grow at an exponential rate or high rate during the training of RNN. This error gradient accumulates and results in applying large updates to the neural network, causes an overflow, and results in NaN values.
36. What is the full form of LSTM? What is its function?
Answer: LSTM stands for Long Short Term Memory. It is a recurrent neural network that is capable of learning long term dependencies and recalling information for the longer period as part of its default behavior.
37. What are the different steps in LSTM?
Answer: The different steps in LSTM include the following.
•	Step 1: The network helps in deciding the things that need to be remembered while others that need to be forgotten.
•	Step 2: The selection is made for cell state values that can be updated.
•	Step 3: The network decides as to what can be made as part of the current output.
38. What is Pooling on CNN?
Answer: Polling is a method that is used with the purpose to reduce the spatial dimensions of a CNN. It helps in performing downsampling operations for reducing dimensionality and creating pooled feature maps. Pooling in CNN helps in sliding the filter matrix over the input matrix.
49. What is RNN?
Answer: The RNN stands for Recurrent Neural Networks. They are an artificial neural network that is a sequence of data, including stock markets, sequence of data including stock markets, time series, and various others. The main idea behind the RNN application is to understand the basics of the feedforward nets.
40. What are the different layers on CNN?
Answer: There are four different layers on CNN. These include the following.
1.	Convolutional Layer: In this layer, several small picture windows are created to go over the data.
2.	ReLU Layer: This layer helps in bringing non-linearity to the network and converts the negative pixels to zero so that the output becomes a rectified feature map.
3.	Pooling Layer: This layer reduces the dimensionality of the feature map.
4.	Fully Connected Layer: This layer recognizes and classifies the objects in the image.
41. What is an Epoch in Data Science?
Answer: Epoch in Data Science represents one of the iterations over the entire dataset. It includes everything that is applied to the learning model.
42. What is a Batch in Data Science?
Answer: Batch is referred to as a different dataset that is divided into the form of different batches to help to pass the information into the system. It is developed in the situation when the developer cannot pass the entire dataset into the neural network at once.
43. What is the iteration in Data Science? Give an example?
Answer: Iteration in Data Science is applied by Epoch for analysis of data. The iteration is, therefore, classification of the data into different groups. For example, when there are 50,000 images, and the batch size is 100, then in such a case, the Epoch will run about 500 iterations.
44. What is the cost function?
Answer: Cost functions are a tool to evaluate how good the model performance has been made. It takes into consideration the errors and losses that are made in the output layer during the backpropagation process. In such a case, the errors are moved backward in the neural network, and various other training functions are applied.
45. What are hyperparameters?
Answer: Hyperparameter is a kind of parameter whose value is set before the learning process so that the network training requirements can be identified and the structure of the network can be improved. This process includes recognizing the hidden units, learning rate, epochs, and various others associated.
46. Which skills are important to become a certified Data Scientist?
Answer: The important skills to become a certified Data Scientist include the following:
1.	Knowledge of built-in data types including lists, tuples, sets and related.
2.	Expertize in N-dimensional NumPy Arrays.
3.	Ability to apply Pandas Dataframes.
4.	Strong hold over performance in element wise vectors.
5.	Knowledge of matrix operations on NumPy arrays.
47. What is an Artificial Neural Network in Data Science?
Answer: Artificial Neural Network in Data Science is the specific set of algorithms that are inspired by the biological neural network meant to adapt the changes in the input so that the best output can be achieved. It helps in generating the best possible results without the need to redesign the output methods.
48. What is Deep Learning in Data Science?
Answer: Deep Learning in Data Science is a name given to machine learning, which requires a great level of analogy with the functioning of the human brain. This way, it is a paradigm of machine learning.
49. Are there differences between Deep Learning and Machine Learning?
Answer: Yes, there are differences between Deep Learning and Machine learning. These are stated as under:
Deep Learning	Machine Learning
It gives computers the ability to learn without being explicitly programmed.	It gives computers a limited to unlimited ability wherein nothing major can be done without getting programmed, and many things can be done without the prior programming. It includes supervised, unsupervised, and reinforcement machine learning processes.
It is a subcomponent of machine learning that is concerned with algorithms that are inspired by the structure and functions of the human brains called the Artificial Neural Networks.	It includes Deep Learning as one of its components.
50.What is Ensemble learning?
Answer: Ensemble learning is a process of combining the diverse set of learners that is the individual models with each other. It helps in improving the stability and predictive power of the model.
Question: What are the different kinds of Ensemble learning?
Answer: The different kinds of Ensemble learning includes the following.
1.	Bagging: It implements simple learners on one small population and takes mean for estimation purposes.
2.	Boosting: It adjusts the weight of the observation and thereby classifies the population in different sets before the outcome prediction is made.

51. What is Naive Bayes?
Naive Bayes Machine Learning algorithm is a powerful algorithm for predictive modeling. It is a set of algorithms with a common principle based on Bayes Theorem. The fundamental Naive Bayes assumption is that each feature makes an independent and equal contribution to the outcome.
52. What is perceptron in Machine Learning?
Perceptron is an algorithm that is able to simulate the ability of the human brain to understand and discard; it is used for the supervised classification of the input into one of the several possible non-binary outputs.
53. List the extraction techniques used for dimensionality reduction.
•	Independent component analysis
•	Principal component analysis
•	Kernel-based principal component analysis
54. Is KNN different from K-means Clustering?
KNN	K-means Clustering
Supervised	Unsupervised
Classification algorithms	Clustering algorithms
Minimal training model	Exhaustive training model
Used in the classification and regression of the known data	Used in population demographics, market segmentation, social media trends, anomaly detection, etc.
55.What is ensemble learning?
Ensemble learning is a computational technique in which classifiers or experts are strategically formed and combined. It is used to improve classification, prediction, function approximation, etc. of a model.
56. List the steps involved in Machine Learning.
•	Data collection
•	Data preparation
•	Choosing an appropriate model
•	Training the dataset
•	Evaluation
•	Parameter tuning
•	Predictions
57. What is a hash table?
A hash table is a data structure that is used to produce an associative array which is mostly used for database indexing.
58. What is regularization in Machine Learning?
Regularization comes into the picture when a model is either overfit or underfit. It is basically used to minimize the error in a dataset. A new piece of information is fit into the dataset to avoid fitting issues.
59.What are the components of relational evaluation techniques?
•	Data acquisition
•	Ground truth acquisition
•	Cross validation technique
•	Query type
•	Scoring metric
•	Significance test
60.What is model accuracy and model performance?
Model accuracy, a subset of model performance, is based on the model performance of an algorithm. Whereas, model performance is based on the datasets we feed as inputs to the algorithm.
61. Define F1 score.
F1 score is the weighted average of precision and recall. It considers both false positive and false negative values into account. It is used to measure a model’s performance.
62. List the applications of Machine Learning.
•	Image, speech, and face detection
•	Bioinformatics
•	Market segmentation
•	Manufacturing and inventory management
•	Fraud detection, and so on
63. Can you name three feature selection techniques in Machine Learning?
1.	Univariate Selection
2.	Feature Importance
3.	Correlation Matrix with Heatmap
64.What is a recommendation system?
A recommendation system is an information filtering system that is used to predict user preference based on choice patterns followed by the user while browsing/using the system.
65.What methods are used for reducing dimensionality?
Dimensionality reduction is the process of reducing the number of random variables. We can reduce dimensionality using techniques such as missing values ratio, low variance filter, high correlation filter, random forest, principal component analysis, etc.
66. List different methods for sequential supervised learning.
•	Sliding window methods
•	Recurrent sliding windows methods
•	Hidden Markov models
•	Maximum entropy Markov models
•	Conditional random fields
•	Graph transformer networks
67. What are the advantages of neural networks?
•	Require less formal statistical training
•	Have the ability to detect nonlinear relationships between variables
•	Detect all possible interactions between predictor variables
•	Availability of multiple training algorithms
68. What is Bias–Variance tradeoff?
Bias error is used to measure how much on an average the predicted values vary from the actual values. In case a high-bias error occurs, we have an under-performing model.
Variance is used to measure how the predictions made on the same observation differ from each other. A high-variance model will overfit the dataset and perform badly on any observation.

69. What is TensorFlow?
TensorFlow is an open-source Machine Learning library. It is a fast, flexible, and low-level toolkit for doing complex algorithms and offers users customizability to build experimental learning architectures and to work on them to produce desired outputs.
70.How to install TensorFlow?
TensorFlow Installation Guide:
CPU : pip install tensorflow-cpu
GPU : pip install tensorflow-gpu
71.What are the TensorFlow objects?
1.	Constants
2.	Variables
3.	Placeholder
4.	Graph
5.	Session
72. What is a cost function?
A cost function is a scalar function that quantifies the error factor of the neural network. Lower the cost function better the neural network. For example, while classifying the image in the MNIST dataset, the input image is digit 2, but the neural network wrongly predicts it to be 3.
73. List different activation neurons or functions.
1.	Linear neuron
2.	Binary threshold neuron
3.	Stochastic binary neuron
4.	Sigmoid neuron
5.	Tanh function
6.	Rectified linear unit (ReLU)
74. What are the hyper parameters of ANN?
•	Learning rate: The learning rate is how fast the network learns its parameters.
•	Momentum: It is a parameter that helps to come out of the local minima and smoothen the jumps while gradient descent.
•	Number of epochs: The number of times the entire training data is fed to the network while training is referred to as the number of epochs. We increase the number of epochs until the validation accuracy starts decreasing, even if the training accuracy is increasing (overfitting).
75. What is vanishing gradient?
As we add more and more hidden layers, backpropagation becomes less useful in passing information to the lower layers. In effect, as information is passed back, the gradients begin to vanish and become small relative to the weights of the network.
76. What are dropouts?
Dropout is a simple way to prevent a neural network from overfitting. It is the dropping out of some of the units in a neural network. It is similar to the natural reproduction process, where the nature produces offsprings by combining distinct genes (dropping out others) rather than strengthening the co-adapting of them.
77. Define LSTM.
Long short-term memory (LSTM) is explicitly designed to address the long-term dependency problem, by maintaining a state of what to remember and what to forget.
78. List the key components of LSTM.
•	Gates (forget, Memory, update, and Read)
•	Tanh(x) (values between -1 and 1)
•	Sigmoid(x) (values between 0 and 1)
79. List the variants of RNN.
•	LSTM: Long Short-term Memory
•	GRU: Gated Recurrent Unit
•	End-to-end Network
•	Memory Network
80. What is an autoencoder? Name a few applications.
An autoencoder is basically used to learn a compressed form of the given data. A few applications of an autoencoder are given below:
1.	Data denoising
2.	Dimensionality reduction
3.	Image reconstruction
4.	Image colorization
81. What are the components of the generative adversarial network (GAN)? How do you deploy it?
Components of GAN:
•	Generator
•	Discriminator
Deployment Steps:
•	Train the model
•	Validate and finalize the model
•	Save the model
•	Load the saved model for the next prediction
82. What are the steps involved in the gradient descent algorithm?
Gradient descent is an optimization algorithm that is used to find the coefficients of parameters that are used to reduce the cost function to a minimum.
Step 1: Allocate weights (x,y) with random values and calculate the error (SSE)
Step 2: Calculate the gradient, i.e., the variation in SSE when the weights (x,y) are changed by a very small value. This helps us move the values of x and y in the direction in which SSE is minimized
Step 3: Adjust the weights with the gradients to move toward the optimal values where SSE is minimized
Step 4: Use new weights for prediction and calculating the new SSE
Step 5: Repeat Steps 2 and 3 until further adjustments to the weights do not significantly reduce the error
83. What do you understand by session in TensorFlow?
Syntax: Class Session
It is a class for running TensorFlow operations. The environment is encapsulated in the session object wherein the operation objects are executed and Tensor objects are evaluated.
# Build a graph
x = tf.constant(2.0)
y = tf.constant(5.0)
z = x * y
# Launch the graph in a session
sess = tf.Session()
# Evaluate the tensor `z`
print(sess.run(z))
84. What do you mean by TensorFlow cluster?
TensorFlow cluster is a set of ‘tasks’ that participate in the distributed execution of a TensorFlow graph. Each task is associated with a TensorFlow server, which contains a ‘master’ that can be used to create sessions and a ‘worker’ that executes operations in the graph. A cluster can also be divided into one or more ‘jobs’, where each job contains one or more tasks.

85. How to run TensorFlow on Hadoop?
To use HDFS with TensorFlow, we need to change the file path for reading and writing data to an HDFS path. For example:
filename_queue = tf.train.string_input_producer([
“hdfs://namenode:8020/path/to/file1.csv”,
“hdfs://namenode:8020/path/to/file2.csv”,
])
86. What are intermediate tensors? Do sessions have lifetime?
The intermediate tensors are tensors that are neither inputs nor outputs of the Session.run() call, but are in the path leading from the inputs to the outputs; they will be freed at or before the end of the call.
Sessions can own resources, few classes like tf.Variable, tf.QueueBase, and tf.ReaderBase, and they use a significant amount of memory. These resources (and the associated memory) are released when the session is closed, by calling tf.Session.close.

87. What is the lifetime of a variable?
When we first run the tf.Variable.initializer operation for a variable in a session, it is started. It is destroyed when we run the tf.Session.close operation.




Data Science

Basic Level-Q&A:
1. Can you enumerate the various differences between Supervised and Unsupervised Learning?
Answer: Supervised learning is a type of machine learning where a function is inferred from labeled training data. The training data contains a set of training examples.
Unsupervised learning, on the other hand, is a type of machine learning where inferences are drawn from datasets containing input data without labeled responses. Following are the various other differences between the two types of machine learning:
•	Algorithms Used – Supervised learning makes use of Decision Trees, K-nearest Neighbor algorithm, Neural Networks, Regression, and Support Vector Machines. Unsupervised learning uses Anomaly Detection, Clustering, Latent Variable Models, and Neural Networks.
•	Enables – Supervised learning enables classification and regression, whereas unsupervised learning enables classification, dimension reduction, and density estimation
•	Use – While supervised learning is used for prediction, unsupervised learning finds use in analysis
2. What do you understand by the Selection Bias? What are its various types?
Answer: Selection bias is typically associated with research that doesn’t have a random selection of participants. It is a type of error that occurs when a researcher decides who is going to be studied. On some occasions, selection bias is also referred to as the selection effect.
In other words, selection bias is a distortion of statistical analysis that results from the sample collecting method. When selection bias is not taken into account, some conclusions made by a research study might not be accurate. Following are the various types of selection bias:
•	Sampling Bias – A systematic error resulting due to a non-random sample of a populace causing certain members of the same to be less likely included than others that results in a biased sample.
•	Time Interval – A trial might be ended at an extreme value, usually due to ethical reasons, but the extreme value is most likely to be reached by the variable with the most variance, even though all variables have a similar mean.
•	Data – Results when specific data subsets are selected for supporting a conclusion or rejection of bad data arbitrarily.
•	Attrition – Caused due to attrition, i.e. loss of participants, discounting trial subjects or tests that didn’t run to completion.
3. Please explain the goal of A/B Testing.
Answer: A/B Testing is a statistical hypothesis testing meant for a randomized experiment with two variables, A and B. The goal of A/B Testing is to maximize the likelihood of an outcome of some interest by identifying any changes to a webpage.
A highly reliable method for finding out the best online marketing and promotional strategies for a business, A/B Testing can be employed for testing everything, ranging from sales emails to search ads and website copy.
4. How will you calculate the Sensitivity of machine learning models?
Answer: In machine learning, Sensitivity is used for validating the accuracy of a classifier, such as Logistic, Random Forest, and SVM. It is also known as REC (recall) or TPR (true positive rate).
Sensitivity can be defined as the ratio of predicted true events and total events i.e.:
Sensitivity = True Positives / Positives in Actual Dependent Variable
Here, true events are those events that were true as predicted by a machine learning model. The best sensitivity is 1.0 and the worst sensitivity is 0.0.
5. Could you draw a comparison between overfitting and underfitting?
Answer: In order to make reliable predictions on general untrained data in machine learning and statistics, it is required to fit a (machine learning) model to a set of training data. Overfitting and underfitting are two of the most common modeling errors that occur while doing so.
Following are the various differences between overfitting and underfitting:
•	Definition - A statistical model suffering from overfitting describes some random error or noise in place of the underlying relationship. When underfitting occurs, a statistical model or machine learning algorithm fails in capturing the underlying trend of the data.
•	Occurrence – When a statistical model or machine learning algorithm is excessively complex, it can result in overfitting. Example of a complex model is one having too many parameters when compared to the total number of observations. Underfitting occurs when trying to fit a linear model to non-linear data.
•	Poor Predictive Performance – Although both overfitting and underfitting yield poor predictive performance, the way in which each one of them does so is different. While the overfitted model overreacts to minor fluctuations in the training data, the underfit model under-reacts to even bigger fluctuations.
6. Between Python and R, which one would you pick for text analytics and why?
Answer: For text analytics, Python will gain an upper hand over R due to these reasons:
•	The Pandas library in Python offers easy-to-use data structures as well as high-performance data analysis tools
•	Python has a faster performance for all types of text analytics
•	R is a best-fit for machine learning than mere text analysis
7. Please explain the role of data cleaning in data analysis.
Answer: Data cleaning can be a daunting task due to the fact that with the increase in the number of data sources, the time required for cleaning the data increases at an exponential rate.
This is due to the vast volume of data generated by additional sources. Also, data cleaning can solely take up to 80% of the total time required for carrying out a data analysis task.
Nevertheless, there are several reasons for using data cleaning in data analysis. Two of the most important ones are:
•	Cleaning data from different sources helps in transforming the data into a format that is easy to work with
•	Data cleaning increases the accuracy of a machine learning model
8. What do you mean by cluster sampling and systematic sampling?
Answer: When studying the target population spread throughout a wide area becomes difficult and applying simple random sampling becomes ineffective, the technique of cluster sampling is used. A cluster sample is a probability sample, in which each of the sampling units is a collection or cluster of elements.
Following the technique of systematic sampling, elements are chosen from an ordered sampling frame. The list is advanced in a circular fashion. This is done in such a way so that once the end of the list is reached, the same is progressed from the start, or top, again.
9. Please explain Eigenvectors and Eigenvalues.
Answer: Eigenvectors help in understanding linear transformations. They are calculated typically for a correlation or covariance matrix in data analysis.
In other words, eigenvectors are those directions along which some particular linear transformation acts by compressing, flipping, or stretching.
Eigenvalues can be understood either as the strengths of the transformation in the direction of the eigenvectors or the factors by which the compressions happens.
10. Can you compare the validation set with the test set?
Answer: A validation set is part of the training set used for parameter selection as well as for avoiding overfitting of the machine learning model being developed. On the contrary, a test set is meant for evaluating or testing the performance of a trained machine learning model.
11. What do you understand by linear regression and logistic regression?
Answer: Linear regression is a form of statistical technique in which the score of some variable Y is predicted on the basis of the score of a second variable X, referred to as the predictor variable. The Y variable is known as the criterion variable.
Also known as the logit model, logistic regression is a statistical technique for predicting the binary outcome from a linear combination of predictor variables.
12. Please explain Recommender Systems along with an application.
Answer: Recommender Systems is a subclass of information filtering systems, meant for predicting the preferences or ratings awarded by a user to some product.
An application of a recommender system is the product recommendations section in Amazon. This section contains items based on the user’s search history and past orders.
13. What are outlier values and how do you treat them?
Answer: Outlier values, or simply outliers, are data points in statistics that don’t belong to a certain population. An outlier value is an abnormal observation that is very much different from other values belonging to the set.
Identification of outlier values can be done by using univariate or some other graphical analysis method. Few outlier values can be assessed individually but assessing a large set of outlier values require the substitution of the same with either the 99th or the 1st percentile values.
There are two popular ways of treating outlier values:
1.	To change the value so that it can be brought within a range
2.	To simply remove the value
Note: - Not all extreme values are outlier values.
14. Please enumerate the various steps involved in an analytics project.
Answer: Following are the numerous steps involved in an analytics project:
•	Understanding the business problem
•	Exploring the data and familiarizing with the same
•	Preparing the data for modeling by means of detecting outlier values, transforming variables, treating missing values, et cetera
•	Running the model and analyzing the result for making appropriate changes or modifications to the model (an iterative step that repeats until the best possible outcome is gained)
•	Validating the model using a new dataset
•	Implementing the model and tracking the result for analyzing the performance of the same
15. Could you explain how to define the number of clusters in a clustering algorithm?
Answer: The primary objective of clustering is to group together similar identities in such a way that while entities within a group are similar to each other, the groups remain different from one another.
Generally, Within Sum of Squares is used for explaining the homogeneity within a cluster. For defining the number of clusters in a clustering algorithm, WSS is plotted for a range pertaining to a number of clusters. The resultant graph is known as the Elbow Curve.
The Elbow Curve graph contains a point that represents the point post in which there aren’t any decrements in the WSS. This is known as the bending point and represents K in K–Means.
Although the aforementioned is the widely-used approach, another important approach is the Hierarchical clustering. In this approach, dendrograms are created first and then distinct groups are identified from there.
16. What do you understand by Deep Learning?
Answer: Deep Learning is a paradigm of machine learning that displays a great degree of analogy with the functioning of the human brain. It is a neural network method based on convolutional neural networks (CNN).
Deep learning has a wide array of uses, ranging from social network filtering to medical image analysis and speech recognition. Although Deep Learning has been present for a long time, it’s only recently that it has gained worldwide acclaim. This is mainly due to:
•	An increase in the amount of data generation via various sources
•	The growth in hardware resources required for running Deep Learning models
17. Please explain Gradient Descent.
Answer: The degree of change in the output of a function relating to the changes made to the inputs is known as a gradient. It measures the change in all weights with respect to the change in error. A gradient can also be comprehended as the slope of a function.
Gradient Descent refers to escalating down to the bottom of a valley. Simply, consider this something as opposed to climbing up a hill. It is a minimization algorithm meant for minimizing a given activation function.
18. How does Backpropagation work? Also, it state its various variants.
Answer: Backpropagation refers to a training algorithm used for multilayer neural networks. Following the backpropagation algorithm, the error is moved from an end of the network to all weights inside the network. Doing so allows for efficient computation of the gradient.
Backpropagation works in the following way:
•	Forward propagation of training data
•	Output and target is used for computing derivatives
•	Backpropagate for computing the derivative of the error with respect to the output activation
•	Using previously calculated derivatives for output generation
•	Updating the weights
Following are the various variants of Backpropagation:
•	Batch Gradient Descent – The gradient is calculated for the complete dataset and update is performed on each iteration
•	Mini-batch Gradient Descent – Mini-batch samples are used for calculating gradient and updating parameters (a variant of the Stochastic Gradient Descent approach)
•	Stochastic Gradient Descent – Only a single training example is used to calculate gradient and updating parameters
19. What do you know about Autoencoders?
Answer: Autoencoders are simplistic learning networks used for transforming inputs into outputs with minimum possible error. It means that the outputs resulted are very close to the inputs.
A couple of layers are added between the input and the output with the size of each layer smaller than the size pertaining to the input layer. An autoencoder receives unlabeled input that is encoded for reconstructing the output.
20. Please explain the concept of a Boltzmann Machine.
Answer: A Boltzmann Machine features a simple learning algorithm that enables the same to discover fascinating features representing complex regularities present in the training data. It is basically used for optimizing the quantity and weight for some given problem.
The simple learning algorithm involved in a Boltzmann Machine is very slow in networks that have many layers of feature detectors.
21. What are the skills required as a Data Scientist that could help in using Python for data analysis purposes?
Answer: The skills required as a Data Scientist that could help in using Python for data analysis purposes are stated under:
1.	Expertize in Pandas Dataframes, Scikit-learn, and N-dimensional NumPy Arrays.
2.	Skills to apply element-wise vector and matrix operations on NumPy arrays.
3.	Able to understand built-in data types, including tuples, sets, dictionaries, and various others.
4.	It is equipped with Anaconda distribution and the Conda package manager.
5.	Capability in writing efficient list comprehensions, small, clean functions, and avoid traditional for loops.
6.	Knowledge of Python script and optimizing bottlenecks
22. What is the full form of GAN? Explain GAN?
Answer: The full form of GAN is Generative Adversarial Network. Its task is to take inputs from the noise vector and send it forward to the Generator and then to Discriminator to identify and differentiate the unique and fake inputs.
23. What are the vital components of GAN?
Answer: There are two vital components of GAN. These include the following:
1.	Generator: The Generator act as a Forger, which creates fake copies.
2.	Discriminator: The Discriminator act as a recognizer for fake and unique (real) copies.
24. What is the Computational Graph?
Answer: A computational graph is a graphical presentation that is based on TensorFlow. It has a wide network of different kinds of nodes wherein each node represents a particular mathematical operation. The edges in these nodes are called tensors. This is the reason the computational graph is called a TensorFlow of inputs. The computational graph is characterized by data flows in the form of a graph; therefore, it is also called the DataFlow Graph.
25. What are tensors?
Answer: Tensors are the mathematical objects that represent the collection of higher dimensions of data inputs in the form of alphabets, numerals, and rank fed as inputs to the neural network.
26. Why are Tensorflow considered a high priority in learning Data Science?
Answer: Tensorflow is considered a high priority in learning Data Science because it provides support to using computer languages such as C++ and Python. This way, it makes various processes under data science to achieve faster compilation and completion within the stipulated time frame and faster than the conventional Keras and Torch libraries. Tensorflow supports the computing devices, including the CPU and GPU for faster inputs, editing, and analysis of the data.
27. What is Dropout in Data Science?
Answer: Dropout is a toll in Data Science, which is used for dropping out the hidden and visible units of a network on a random basis. They prevent the overfitting of the data by dropping as much as 20% of the nodes so that the required space can be arranged for iterations needed to converge the network.
28. What is Batch normalization in Data Science?
Answer: Batch Normalization in Data Science is a technique through which attempts could be made to improve the performance and stability of the neural network. This can be done by normalizing the inputs in each layer so that the mean output activation remains 0 with the standard deviation at 1.
29. What is the difference between Batch and Stochastic Gradient Descent?
Answer: The difference between Batch and Stochastic Gradient Descent can be displayed as follows:
Batch Gradient Descent	Stochastic Gradient Descent
It helps in computing the gradient using the complete data set available.	It helps in computing the gradient using only the single sample.
It takes time to converge.	It takes less time to converge.
The volume is huge for analysis purpose	The volume is lesser for analysis purposes.
It updates the weight slowly.	It updates the weight more frequently.
30.What are Auto-Encoders?
Answer: Auto-Encoders are learning networks that are meant to change inputs into output with the lowest chance of getting an error. They intend to keep the output closer to the input. The process of Autoencoders is needed to be done through the development of layers between the input and output. However, efforts are made to keep the size of these layers smaller for faster processing.
31. What are the various Machine Learning Libraries and their benefits?
Answer: The various machine learning libraries and their benefits are as follows.
1.	Numpy: It is used for scientific computation.
2.	Statsmodels: It is used for time-series analysis.
3.	Pandas: It is used for tubular data analysis.
4.	Scikit learns: It is used for data modeling and pre-processing.
5.	Tensorflow: It is used for the deep learning process.
6.	Regular Expressions: It is used for text processing.
7.	Pytorch: It is used for the deep learning process.
8.	NLTK: It is used for text processing.
32. What is an Activation function?
Answer: An Activation function helps in introducing the non-linearity in the neural network. This is done to help the learning process for complex functions. Without the activation function, the neural network will be unable to perform only the linear function and apply linear combinations. Activation function, therefore, offers complex functions and combinations by applying artificial neurons, which helps in delivering output based on the inputs.
33. What are the different types of Deep Learning Frameworks?
Answer: The different types of Deep Learning Framework includes the following:
1.	Caffe
2.	Keras
3.	TensorFlow
4.	Pytorch
5.	Chainer
6.	Microsoft Cognitive Toolkit
34. What are vanishing gradients?
Answer: The vanishing gradients is a condition when the slope is too small during the training process of RNN. The result of vanishing gradients is poor performance outcomes, low accuracy, and long term training processes.
35. What are exploding gradients?
Answer: The exploding gradients are a condition when the errors grow at an exponential rate or high rate during the training of RNN. This error gradient accumulates and results in applying large updates to the neural network, causes an overflow, and results in NaN values.
36. What is the full form of LSTM? What is its function?
Answer: LSTM stands for Long Short Term Memory. It is a recurrent neural network that is capable of learning long term dependencies and recalling information for the longer period as part of its default behavior.
37. What are the different steps in LSTM?
Answer: The different steps in LSTM include the following.
•	Step 1: The network helps in deciding the things that need to be remembered while others that need to be forgotten.
•	Step 2: The selection is made for cell state values that can be updated.
•	Step 3: The network decides as to what can be made as part of the current output.
38. What is Pooling on CNN?
Answer: Polling is a method that is used with the purpose to reduce the spatial dimensions of a CNN. It helps in performing downsampling operations for reducing dimensionality and creating pooled feature maps. Pooling in CNN helps in sliding the filter matrix over the input matrix.
49. What is RNN?
Answer: The RNN stands for Recurrent Neural Networks. They are an artificial neural network that is a sequence of data, including stock markets, sequence of data including stock markets, time series, and various others. The main idea behind the RNN application is to understand the basics of the feedforward nets.
40. What are the different layers on CNN?
Answer: There are four different layers on CNN. These include the following.
1.	Convolutional Layer: In this layer, several small picture windows are created to go over the data.
2.	ReLU Layer: This layer helps in bringing non-linearity to the network and converts the negative pixels to zero so that the output becomes a rectified feature map.
3.	Pooling Layer: This layer reduces the dimensionality of the feature map.
4.	Fully Connected Layer: This layer recognizes and classifies the objects in the image.
41. What is an Epoch in Data Science?
Answer: Epoch in Data Science represents one of the iterations over the entire dataset. It includes everything that is applied to the learning model.
42. What is a Batch in Data Science?
Answer: Batch is referred to as a different dataset that is divided into the form of different batches to help to pass the information into the system. It is developed in the situation when the developer cannot pass the entire dataset into the neural network at once.
43. What is the iteration in Data Science? Give an example?
Answer: Iteration in Data Science is applied by Epoch for analysis of data. The iteration is, therefore, classification of the data into different groups. For example, when there are 50,000 images, and the batch size is 100, then in such a case, the Epoch will run about 500 iterations.
44. What is the cost function?
Answer: Cost functions are a tool to evaluate how good the model performance has been made. It takes into consideration the errors and losses that are made in the output layer during the backpropagation process. In such a case, the errors are moved backward in the neural network, and various other training functions are applied.
45. What are hyperparameters?
Answer: Hyperparameter is a kind of parameter whose value is set before the learning process so that the network training requirements can be identified and the structure of the network can be improved. This process includes recognizing the hidden units, learning rate, epochs, and various others associated.
46. Which skills are important to become a certified Data Scientist?
Answer: The important skills to become a certified Data Scientist include the following:
1.	Knowledge of built-in data types including lists, tuples, sets and related.
2.	Expertize in N-dimensional NumPy Arrays.
3.	Ability to apply Pandas Dataframes.
4.	Strong hold over performance in element wise vectors.
5.	Knowledge of matrix operations on NumPy arrays.
47. What is an Artificial Neural Network in Data Science?
Answer: Artificial Neural Network in Data Science is the specific set of algorithms that are inspired by the biological neural network meant to adapt the changes in the input so that the best output can be achieved. It helps in generating the best possible results without the need to redesign the output methods.
48. What is Deep Learning in Data Science?
Answer: Deep Learning in Data Science is a name given to machine learning, which requires a great level of analogy with the functioning of the human brain. This way, it is a paradigm of machine learning.
49. Are there differences between Deep Learning and Machine Learning?
Answer: Yes, there are differences between Deep Learning and Machine learning. These are stated as under:
Deep Learning	Machine Learning
It gives computers the ability to learn without being explicitly programmed.	It gives computers a limited to unlimited ability wherein nothing major can be done without getting programmed, and many things can be done without the prior programming. It includes supervised, unsupervised, and reinforcement machine learning processes.
It is a subcomponent of machine learning that is concerned with algorithms that are inspired by the structure and functions of the human brains called the Artificial Neural Networks.	It includes Deep Learning as one of its components.
50.What is Ensemble learning?
Answer: Ensemble learning is a process of combining the diverse set of learners that is the individual models with each other. It helps in improving the stability and predictive power of the model.
Question: What are the different kinds of Ensemble learning?
Answer: The different kinds of Ensemble learning includes the following.
1.	Bagging: It implements simple learners on one small population and takes mean for estimation purposes.
2.	Boosting: It adjusts the weight of the observation and thereby classifies the population in different sets before the outcome prediction is made.

51. What is Naive Bayes?
Naive Bayes Machine Learning algorithm is a powerful algorithm for predictive modeling. It is a set of algorithms with a common principle based on Bayes Theorem. The fundamental Naive Bayes assumption is that each feature makes an independent and equal contribution to the outcome.
52. What is perceptron in Machine Learning?
Perceptron is an algorithm that is able to simulate the ability of the human brain to understand and discard; it is used for the supervised classification of the input into one of the several possible non-binary outputs.
53. List the extraction techniques used for dimensionality reduction.
•	Independent component analysis
•	Principal component analysis
•	Kernel-based principal component analysis
54. Is KNN different from K-means Clustering?
KNN	K-means Clustering
Supervised	Unsupervised
Classification algorithms	Clustering algorithms
Minimal training model	Exhaustive training model
Used in the classification and regression of the known data	Used in population demographics, market segmentation, social media trends, anomaly detection, etc.
55.What is ensemble learning?
Ensemble learning is a computational technique in which classifiers or experts are strategically formed and combined. It is used to improve classification, prediction, function approximation, etc. of a model.
56. List the steps involved in Machine Learning.
•	Data collection
•	Data preparation
•	Choosing an appropriate model
•	Training the dataset
•	Evaluation
•	Parameter tuning
•	Predictions
57. What is a hash table?
A hash table is a data structure that is used to produce an associative array which is mostly used for database indexing.
58. What is regularization in Machine Learning?
Regularization comes into the picture when a model is either overfit or underfit. It is basically used to minimize the error in a dataset. A new piece of information is fit into the dataset to avoid fitting issues.
59.What are the components of relational evaluation techniques?
•	Data acquisition
•	Ground truth acquisition
•	Cross validation technique
•	Query type
•	Scoring metric
•	Significance test
60.What is model accuracy and model performance?
Model accuracy, a subset of model performance, is based on the model performance of an algorithm. Whereas, model performance is based on the datasets we feed as inputs to the algorithm.
61. Define F1 score.
F1 score is the weighted average of precision and recall. It considers both false positive and false negative values into account. It is used to measure a model’s performance.
62. List the applications of Machine Learning.
•	Image, speech, and face detection
•	Bioinformatics
•	Market segmentation
•	Manufacturing and inventory management
•	Fraud detection, and so on
63. Can you name three feature selection techniques in Machine Learning?
1.	Univariate Selection
2.	Feature Importance
3.	Correlation Matrix with Heatmap
64.What is a recommendation system?
A recommendation system is an information filtering system that is used to predict user preference based on choice patterns followed by the user while browsing/using the system.
65.What methods are used for reducing dimensionality?
Dimensionality reduction is the process of reducing the number of random variables. We can reduce dimensionality using techniques such as missing values ratio, low variance filter, high correlation filter, random forest, principal component analysis, etc.
66. List different methods for sequential supervised learning.
•	Sliding window methods
•	Recurrent sliding windows methods
•	Hidden Markov models
•	Maximum entropy Markov models
•	Conditional random fields
•	Graph transformer networks
67. What are the advantages of neural networks?
•	Require less formal statistical training
•	Have the ability to detect nonlinear relationships between variables
•	Detect all possible interactions between predictor variables
•	Availability of multiple training algorithms
68. What is Bias–Variance tradeoff?
Bias error is used to measure how much on an average the predicted values vary from the actual values. In case a high-bias error occurs, we have an under-performing model.
Variance is used to measure how the predictions made on the same observation differ from each other. A high-variance model will overfit the dataset and perform badly on any observation.

69. What is TensorFlow?
TensorFlow is an open-source Machine Learning library. It is a fast, flexible, and low-level toolkit for doing complex algorithms and offers users customizability to build experimental learning architectures and to work on them to produce desired outputs.
70.How to install TensorFlow?
TensorFlow Installation Guide:
CPU : pip install tensorflow-cpu
GPU : pip install tensorflow-gpu
71.What are the TensorFlow objects?
1.	Constants
2.	Variables
3.	Placeholder
4.	Graph
5.	Session
72. What is a cost function?
A cost function is a scalar function that quantifies the error factor of the neural network. Lower the cost function better the neural network. For example, while classifying the image in the MNIST dataset, the input image is digit 2, but the neural network wrongly predicts it to be 3.
73. List different activation neurons or functions.
1.	Linear neuron
2.	Binary threshold neuron
3.	Stochastic binary neuron
4.	Sigmoid neuron
5.	Tanh function
6.	Rectified linear unit (ReLU)
74. What are the hyper parameters of ANN?
•	Learning rate: The learning rate is how fast the network learns its parameters.
•	Momentum: It is a parameter that helps to come out of the local minima and smoothen the jumps while gradient descent.
•	Number of epochs: The number of times the entire training data is fed to the network while training is referred to as the number of epochs. We increase the number of epochs until the validation accuracy starts decreasing, even if the training accuracy is increasing (overfitting).
75. What is vanishing gradient?
As we add more and more hidden layers, backpropagation becomes less useful in passing information to the lower layers. In effect, as information is passed back, the gradients begin to vanish and become small relative to the weights of the network.
76. What are dropouts?
Dropout is a simple way to prevent a neural network from overfitting. It is the dropping out of some of the units in a neural network. It is similar to the natural reproduction process, where the nature produces offsprings by combining distinct genes (dropping out others) rather than strengthening the co-adapting of them.
77. Define LSTM.
Long short-term memory (LSTM) is explicitly designed to address the long-term dependency problem, by maintaining a state of what to remember and what to forget.
78. List the key components of LSTM.
•	Gates (forget, Memory, update, and Read)
•	Tanh(x) (values between -1 and 1)
•	Sigmoid(x) (values between 0 and 1)
79. List the variants of RNN.
•	LSTM: Long Short-term Memory
•	GRU: Gated Recurrent Unit
•	End-to-end Network
•	Memory Network
80. What is an autoencoder? Name a few applications.
An autoencoder is basically used to learn a compressed form of the given data. A few applications of an autoencoder are given below:
1.	Data denoising
2.	Dimensionality reduction
3.	Image reconstruction
4.	Image colorization
81. What are the components of the generative adversarial network (GAN)? How do you deploy it?
Components of GAN:
•	Generator
•	Discriminator
Deployment Steps:
•	Train the model
•	Validate and finalize the model
•	Save the model
•	Load the saved model for the next prediction
82. What are the steps involved in the gradient descent algorithm?
Gradient descent is an optimization algorithm that is used to find the coefficients of parameters that are used to reduce the cost function to a minimum.
Step 1: Allocate weights (x,y) with random values and calculate the error (SSE)
Step 2: Calculate the gradient, i.e., the variation in SSE when the weights (x,y) are changed by a very small value. This helps us move the values of x and y in the direction in which SSE is minimized
Step 3: Adjust the weights with the gradients to move toward the optimal values where SSE is minimized
Step 4: Use new weights for prediction and calculating the new SSE
Step 5: Repeat Steps 2 and 3 until further adjustments to the weights do not significantly reduce the error
83. What do you understand by session in TensorFlow?
Syntax: Class Session
It is a class for running TensorFlow operations. The environment is encapsulated in the session object wherein the operation objects are executed and Tensor objects are evaluated.
# Build a graph
x = tf.constant(2.0)
y = tf.constant(5.0)
z = x * y
# Launch the graph in a session
sess = tf.Session()
# Evaluate the tensor `z`
print(sess.run(z))
84. What do you mean by TensorFlow cluster?
TensorFlow cluster is a set of ‘tasks’ that participate in the distributed execution of a TensorFlow graph. Each task is associated with a TensorFlow server, which contains a ‘master’ that can be used to create sessions and a ‘worker’ that executes operations in the graph. A cluster can also be divided into one or more ‘jobs’, where each job contains one or more tasks.

85. How to run TensorFlow on Hadoop?
To use HDFS with TensorFlow, we need to change the file path for reading and writing data to an HDFS path. For example:
filename_queue = tf.train.string_input_producer([
“hdfs://namenode:8020/path/to/file1.csv”,
“hdfs://namenode:8020/path/to/file2.csv”,
])
86. What are intermediate tensors? Do sessions have lifetime?
The intermediate tensors are tensors that are neither inputs nor outputs of the Session.run() call, but are in the path leading from the inputs to the outputs; they will be freed at or before the end of the call.
Sessions can own resources, few classes like tf.Variable, tf.QueueBase, and tf.ReaderBase, and they use a significant amount of memory. These resources (and the associated memory) are released when the session is closed, by calling tf.Session.close.

87. What is the lifetime of a variable?
When we first run the tf.Variable.initializer operation for a variable in a session, it is started. It is destroyed when we run the tf.Session.close operation.

LEVEL-2:

Data Science
      Intermediate Level Q&A:
1. What is Data Science? List the differences between supervised and unsupervised learning.
Data Science is a blend of various tools, algorithms, and machine learning principles with the goal to discover hidden patterns from the raw data. How is this different from what statisticians have been doing for years?
The answer lies in the difference between explaining and predicting.
 
The differences between supervised and unsupervised learning are as follows;
Supervised Learning	Unsupervised Learning
Input data is labelled.	Input data is unlabelled.
Uses a training data set.	Uses the input data set.
Used for prediction.	Used for analysis.
Enables classification and regression.	Enables Classification, Density Estimation, & Dimension Reduction

2. What is Selection Bias?
Selection bias is a kind of error that occurs when the researcher decides who is going to be studied. It is usually associated with research where the selection of participants isn’t random. It is sometimes referred to as the selection effect. It is the distortion of statistical analysis, resulting from the method of collecting samples. If the selection bias is not taken into account, then some conclusions of the study may not be accurate.
The types of selection bias include:
Sampling bias: It is a systematic error due to a non-random sample of a population causing some members of the population to be less likely to be included than others resulting in a biased sample.
Time interval: A trial may be terminated early at an extreme value (often for ethical reasons), but the extreme value is likely to be reached by the variable with the largest variance, even if all variables have a similar mean.
Data: When specific subsets of data are chosen to support a conclusion or rejection of bad data on arbitrary grounds, instead of according to previously stated or generally agreed criteria.
Attrition: Attrition bias is a kind of selection bias caused by attrition (loss of participants) discounting trial subjects/tests that did not run to completion.
3. What is bias-variance trade-off?
Bias: Bias is an error introduced in your model due to oversimplification of the machine learning algorithm. It can lead to underfitting. When you train your model at that time model makes simplified assumptions to make the target function easier to understand.
Low bias machine learning algorithms — Decision Trees, k-NN and SVM High bias machine learning algorithms — Linear Regression, Logistic Regression
Variance: Variance is error introduced in your model due to complex machine learning algorithm, your model learns noise also from the training data set and performs badly on test data set. It can lead to high sensitivity and overfitting.
Normally, as you increase the complexity of your model, you will see a reduction in error due to lower bias in the model. However, this only happens until a particular point. As you continue to make your model more complex, you end up over-fitting your model and hence your model will start suffering from high variance.
 
Bias-Variance trade-off: The goal of any supervised machine learning algorithm is to have low bias and low variance to achieve good prediction performance.
The k-nearest neighbour algorithm has low bias and high variance, but the trade-off can be changed by increasing the value of k which increases the number of neighbours that contribute to the prediction and in turn increases the bias of the model.
The support vector machine algorithm has low bias and high variance, but the trade-off can be changed by increasing the C parameter that influences the number of violations of the margin allowed in the training data which increases the bias but decreases the variance.
There is no escaping the relationship between bias and variance in machine learning. Increasing the bias will decrease the variance. Increasing the variance will decrease bias.
4. What is a confusion matrix?
The confusion matrix is a 2X2 table that contains 4 outputs provided by the binary classifier. Various measures, such as error-rate, accuracy, specificity, sensitivity, precision and recall are derived from it. Confusion Matrix
 
A data set used for performance evaluation is called a test data set. It should contain the correct labels and predicted labels.
 
The predicted labels will exactly the same if the performance of a binary classifier is perfect.
 
The predicted labels usually match with part of the observed labels in real-world scenarios.
 
A binary classifier predicts all data instances of a test data set as either positive or negative. This produces four outcomes-
True-positive(TP) — Correct positive prediction
False-positive(FP) — Incorrect positive prediction
True-negative(TN) — Correct negative prediction
False-negative(FN) — Incorrect negative prediction
 
Basic measures derived from the confusion matrix
Error Rate = (FP+FN)/(P+N)
Accuracy = (TP+TN)/(P+N)
Sensitivity(Recall or True positive rate) = TP/P
Specificity(True negative rate) = TN/N
Precision(Positive predicted value) = TP/(TP+FP)
F-Score(Harmonic mean of precision and recall) = (1+b)(PREC.REC)/(b²PREC+REC) where b is commonly 0.5, 1, 2.

5. What is Data Science?
Data science is defined as a multidisciplinary subject used to extract meaningful insights out of different types of data by employing various scientific methods such as scientific processes and algorithms. Data science helps in solving the analytically complex problems in a  simplified way. It acts as a stream where you can utilize raw data to generate business value.
6. What is meant by supervised and unsupervised learning in data?
Supervised Learning: Supervised learning is a process of training machines with the labelled or right kind of data. In supervised learning, the machine uses the labelled data as a base to give the next answer.
Unsupervised learning:  It is another form of training machines using information which is unlabeled or unstructured one. Unlike Supervised learning, there is no special teacher or predefined data for the machine to quickly learn from.
7. What is meant by A/B testing?
It is a statistical hypothesis testing for conducting randomized experiment between two variables A and B.  The A/B testing is used to identify the changes for an outcome of interest or to maximize a web page. A/B testing helps organizations in finding out the right outcome that works best for a particular business.
8. What is meant by selection bias?
Selection bias is a type of error that arises when the researcher decides on whom he is going to conduct the study. It happens when the selection of participants takes place not randomly.  Selection bias is also sometimes referred to as a selection effect.  It works more effectively and sometimes if the selection bias is not taken into account, the conclusions of the study may go wrong.
9. What is the technique that is being used categorical responses?
Classification technique is widely used in mining data sets.
10. What is power analysis?
It is an experimental design technique for expecting the outcome of a given sample size.
11. What is Root cause Analysis?
The Root cause analysis was initially used to make analysis on industrial accidents, but now it has been extended into many areas. It is a technique that is being used to identify the root cause of a particular problem.
12. What is collaborative filtering?
Filtering is a process used by recommender systems to find patterns and information from numerous data sources, several agents, and collaborating perspectives. In other words, the collaborative method is a process of making automatic predictions from human preferences or interests.
13. Do we have different Selection Biases, if yes, what are they?

1.	Sampling Bias: This bias arises when you select only particular people or when non-random selection of samples happened. In general terms, it is nothing but a selection of the majority of the people belong to one group.
2.	Time Interval: sometimes a trial may be terminated earlier than actual time (probably due to some ethical reasons) but the extreme value finally taken into consideration is the most significant value even though all other variables have similar Mean.
3.	Data: We can name it as a Data bias when a separate set of data is taken to support a conclusion or eliminates terrible data based on the orbitory grounds, instead of generally relying on generally stated criteria.
4.	Attrition bias: Attrition bias is defined as an error that occurs due to Unequal loss of participants from a randomized controlled trial (RCT). There are some cases in which the participant's losses due to various reasons is called an Attrition.

14.What is the Central Limit Theorem and why is it important?
1.	“Suppose that we are interested in estimating the average height among all people. Collecting data for every person in the world is impossible. While we can’t obtain a height measurement from everyone in the population, we can still sample some people. The question now becomes, what can we say about the average height of the entire population given a single sample. The Central Limit Theorem addresses this question exactly.”
15.What is sampling? How many sampling methods do you know?
“Data sampling is a statistical analysis technique used to select, manipulate and analyze a representative subset of data points to identify patterns and trends in the larger data set being examined.”
16.What is the difference between type I vs type II error?
“A type I error occurs when the null hypothesis is true, but is rejected. A type II error occurs when the null hypothesis is false, but erroneously fails to be rejected.”
17.What is linear regression? What do the terms p-value, coefficient, and r-squared value mean? What is the significance of each of these components?
A linear regression is a good tool for quick predictive analysis: for example, the price of a house depends on a myriad of factors, such as its size or its location. In order to see the relationship between these variables, we need to build a linear regression, which predicts the line of best fit between them and can help conclude whether or not these two factors have a positive or negative relationship.
18.What are the assumptions required for linear regression?
There are four major assumptions: 1. There is a linear relationship between the dependent variables and the regressors, meaning the model you are creating actually fits the data, 2. The errors or residuals of the data are normally distributed and independent from each other, 3. There is minimal multicollinearity between explanatory variables, and 4. Homoscedasticity. This means the variance around the regression line is the same for all values of the predictor variable.
19.What is a statistical interaction?
”Basically, an interaction is when the effect of one factor (input variable) on the dependent variable (output variable) differs among levels of another factor.”
20.What is selection bias?
“Selection (or ‘sampling’) bias occurs in an ‘active,’ sense when the sample data that is gathered and prepared for modeling has characteristics that are not representative of the true, future population of cases the model will see. That is, active selection bias occurs when a subset of the data are systematically (i.e., non-randomly) excluded from analysis.”
21.What is an example of a data set with a non-Gaussian distribution?
“The Gaussian distribution is part of the Exponential family of distributions, but there are a lot more of them, with the same sort of ease of use, in many cases, and if the person doing the machine learning has a solid grounding in statistics, they can be utilized where appropriate.”
22.What is the Binomial Probability Formula?
“The binomial distribution consists of the probabilities of each of the possible numbers of successes on N trials for independent events that each have a probability of p (the Greek letter pi) of occurring.”
23.How is k-NN different from k-means clustering?
k-NN, or k-nearest neighbors is a classification algorithm, where the k is an integer describing the number of neighboring data points that influence the classification of a given observation. K-means is a clustering algorithm, where the k is an integer describing the number of clusters to be created from the given data.
24.Explain the 80/20 rule, and tell me about its importance in model validation.
“People usually tend to start with a 80-20% split (80% training set – 20% test set) and split the training set once more into a 80-20% ratio to create the validation set.”
25.Explain what precision and recall are. How do they relate to the ROC curve?
Recall describes what percentage of true positives are described as positive by the model. Precision describes what percent of positive predictions were correct. The ROC curve shows the relationship between model recall and specificity–specificity being a measure of the percent of true negatives being described as negative by the model. Recall, precision, and the ROC are measures used to identify how useful a given classification model is.
26.Explain the difference between L1 and L2 regularization methods.
“A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression. The key difference between these two is the penalty term.”
27.What is root cause analysis?
“All of us dread that meeting where the boss asks ‘why is revenue down?’ The only thing worse than that question is not having any answers! There are many changes happening in your business every day, and often you will want to understand exactly what is driving a given change — especially if it is unexpected. Understanding the underlying causes of change is known as root cause analysis.”
28.What are hash table collisions?
“If the range of key values is larger than the size of our hash table, which is usually always the case, then we must account for the possibility that two different records with two different keys can hash to the same table index. There are a few different ways to resolve this issue. In hash table vernacular, this solution implemented is referred to as collision resolution.”
29.What is an exact test?
“In statistics, an exact (significance) test is a test where all assumptions, upon which the derivation of the distribution of the test statistic is based, are met as opposed to an approximate test (in which the approximation may be made as close as desired by making the sample size big enough). This will result in a significance test that will have a false rejection rate always equal to the significance level of the test. For example an exact test at significance level 5% will in the long run reject true null hypotheses exactly 5% of the time.”
30.I had a sequence of logs from driver that were timestamps which their apps keeps sending every 15 seconds e.g. [10,25,40, 100, 115, 130, ...]. If the gap is more than 15 seconds then the driver is considered offline. Find the number of hours he was online.
Take a difference list. a = [15,15,60,15,15,..] Drop ones that are not 15. b = [15,15,15,15] Take the length and multiply by 15. 15*len(b)/3600 to get hours.

31. What is: lift, KPI, robustness, model fitting, design of experiments, 80/20 rule?
Lift:
It’s measure of performance of a targeting model (or a rule) at predicting or classifying cases as having an enhanced response (with respect to the population as a whole), measured against a random choice targeting model. Lift is simply: target response/average response.
Suppose a population has an average response rate of 5% (mailing for instance). A certain model (or rule) has identified a segment with a response rate of 20%, then lift=20/5=4
Typically, the modeler seeks to divide the population into quantiles, and rank the quantiles by lift. He can then consider each quantile, and by weighing the predicted response rate against the cost, he can decide to market that quantile or not.
“if we use the probability scores on customers, we can get 60% of the total responders we’d get mailing randomly by only mailing the top 30% of the scored customers”.
KPI:
- Key performance indicator
- A type of performance measurement
- Examples: 0 defects, 10/10 customer satisfaction
- Relies upon a good understanding of what is important to the organization
More examples:
Marketing & Sales:
- New customers acquisition
- Customer attrition
- Revenue (turnover) generated by segments of the customer population
- Often done with a data management platform
IT operations:
- Mean time between failure
- Mean time to repair
Robustness:
- Statistics with good performance even if the underlying distribution is not normal
- Statistics that are not affected by outliers
- A learning algorithm that can reduce the chance of fitting noise is called robust
- Median is a robust measure of central tendency, while mean is not
- Median absolute deviation is also more robust than the standard deviation
Model fitting:
- How well a statistical model fits a set of observations
- Examples: AIC, R2R2, Kolmogorov-Smirnov test, Chi 2, deviance (glm)
Design of experiments:
The design of any task that aims to describe or explain the variation of information under conditions that are hypothesized to reflect the variation.
In its simplest form, an experiment aims at predicting the outcome by changing the preconditions, the predictors.
- Selection of the suitable predictors and outcomes
- Delivery of the experiment under statistically optimal conditions
- Randomization
- Blocking: an experiment may be conducted with the same equipment to avoid any unwanted variations in the input
- Replication: performing the same combination run more than once, in order to get an estimate for the amount of random error that could be part of the process
- Interaction: when an experiment has 3 or more variables, the situation in which the interaction of two variables on a third is not additive
80/20 rule:
- Pareto principle
- 80% of the effects come from 20% of the causes
- 80% of your sales come from 20% of your clients
- 80% of a company complaints come from 20% of its customers

32. Define: quality assurance, six sigma.
Quality assurance:
- A way of preventing mistakes or defects in manufacturing products or when delivering services to customers
- In a machine learning context: anomaly detection
Six sigma:
- Set of techniques and tools for process improvement
- 99.99966% of products are defect-free products (3.4 per 1 million)
- 6 standard deviation from the process mean

32.. Give examples of data that does not have a Gaussian distribution, nor log-normal.
•	Allocation of wealth among individuals
•	Values of oil reserves among oil fields (many small ones, a small number of large ones)

33. What is root cause analysis? How to identify a cause vs. a correlation? Give examples
Root cause analysis:
- Method of problem solving used for identifying the root causes or faults of a problem
- A factor is considered a root cause if removal of it prevents the final undesirable event from recurring
Identify a cause vs. a correlation:
- Correlation: statistical measure that describes the size and direction of a relationship between two or more variables. A correlation between two variables doesn’t imply that the change in one variable is the cause of the change in the values of the other variable
- Causation: indicates that one event is the result of the occurrence of the other event; there is a causal relationship between the two events
- Differences between the two types of relationships are easy to identify, but establishing a cause and effect is difficult
Example: sleeping with one’s shoes on is strongly correlated with waking up with a headache. Correlation-implies-causation fallacy: therefore, sleeping with one’s shoes causes headache.
More plausible explanation: both are caused by a third factor: going to bed drunk.
Identify a cause Vs a correlation: use of a controlled study
- In medical research, one group may receive a placebo (control) while the other receives a treatment If the two groups have noticeably different outcomes, the different experiences may have caused the different outcomes

34.Give an example where the median is a better measure than the mean
When data is skewed

35.Given two fair dices, what is the probability of getting scores that sum to 4? to 8?
•	Total: 36 combinations
•	Of these, 3 involve a score of 4: (1,3), (3,1), (2,2)
•	So: 336=112336=112
•	Considering a score of 8: (2,6), (3,5), (4,4), (6,2), (5,3)
•	So: 536536

36. What is the Law of Large Numbers?
•	A theorem that describes the result of performing the same experiment a large number of times
•	Forms the basis of frequency-style thinking
•	It says that the sample mean, the sample variance and the sample standard deviation converge to what they are trying to estimate
•	Example: roll a dice, expected value is 3.5. For a large number of experiments, the average converges to 3.5

37. How do you calculate needed sample size?
Estimate a population mean:
- General formula is ME=t×SnvME=t×Sn or ME=z×snvME=z×sn
- MEME is the desired margin of error
- tt is the t score or z score that we need to use to calculate our confidence interval
- ss is the standard deviation
Example: we would like to start a study to estimate the average internet usage of households in one week for our business plan. How many households must we randomly select to be 95% sure that the sample mean is within 1minute from the true mean of the population? A previous survey of household usage has shown a standard deviation of 6.95 minutes.
•	Z score corresponding to a 95% interval: 1.96 (97.5%, a2=0.025a2=0.025)
•	s=6.95s=6.95
•	n=(z×sME)2=(1.96×6.95)2=13.622=186n=(z×sME)2=(1.96×6.95)2=13.622=186
Estimate a proportion:
- Similar: ME=z×p(1-p)n-----vME=z×p(1-p)n
Example: a professor in Harvard wants to determine the proportion of students who support gay marriage. She asks “how large a sample do I need?”
She wants a margin of error of less than 2.5%, she has found a previous survey which indicates a proportion of 30%.
n=0.3×0.70.0252n=0.3×0.70.0252
38. When you sample, what bias are you inflicting?
Selection bias:
- An online survey about computer use is likely to attract people more interested in technology than in typical
Under coverage bias:
- Sample too few observations from a segment of population
Survivorship bias:
- Observations at the end of the study are a non-random set of those present at the beginning of the investigation
- In finance and economics: the tendency for failed companies to be excluded from performance studies because they no longer exist

39. How do you control for biases?
•	Choose a representative sample, preferably by a random method
•	Choose an adequate size of sample
•	Identify all confounding factors if possible
•	Identify sources of bias and include them as additional predictors in statistical analyses
•	Use randomization: by randomly recruiting or assigning subjects in a study, all our experimental groups have an equal chance of being influenced by the same bias
Notes:
- Randomization: in randomized control trials, research participants are assigned by chance, rather than by choice to either the experimental group or the control group.
- Random sampling: obtaining data that is representative of the population of interest

40. What are confounding variables?
•	Extraneous variable in a statistical model that correlates directly or inversely with both the dependent and the independent variable
•	A spurious relationship is a perceived relationship between an independent variable and a dependent variable that has been estimated incorrectly
•	The estimate fails to account for the confounding factor
•	See Question 18 about root cause analysis

41.What is A/B testing?
•	Two-sample hypothesis testing
•	Randomized experiments with two variants: A and B
•	A: control; B: variation
•	User-experience design: identify changes to web pages that increase clicks on a banner
•	Current website: control; NULL hypothesis
•	New version: variation; alternative hypothesis

42. An HIV test has a sensitivity of 99.7% and a specificity of 98.5%. A subject from a population of prevalence 0.1% receives a positive test result. What is the precision of the test (i.e the probability he is HIV positive)?
Bayes rule: P(Actu+|Pred+)=P(Pred+|Actu+)×P(Actu+)P(Pred+|Actu+)×P(Actu+)+P(Pred+|Actu-)P(Actu-)P(Actu+|Pred+)=P(Pred+|Actu+)×P(Actu+)P(Pred+|Actu+)×P(Actu+)+P(Pred+|Actu-)P(Actu-)
We have: sensitivity×prevalencesensitivity×prevalence+(1-specificity)×(1-prevalence)=0.997×0.0010.997×0.001+0.15×0.999=0.62sensitivity×prevalencesensitivity×prevalence+(1-specificity)×(1-prevalence)=0.997×0.0010.997×0.001+0.15×0.999=0.62

43. Infection rates at a hospital above a 1 infection per 100 person days at risk are considered high. An hospital had 10 infections over the last 1787 person days at risk. Give the p-value of the correct one-sided test of whether the hospital is below the standard
One-sided test, assume a Poisson distribution
Ho: lambda=0.01 ; H1:lambda>0.01
R code:
ppois(10,1787*0.01)
## [1] 0.03237153

44.You roll a biased coin (p(head)=0.8) five times. What’s the probability of getting three or more heads?
•	5 trials, p=0.8
P("3ormoreheads")=(35)×0.83×0.8×0.22+(41)×0.84×0.21+(55)*0.85*0.20=0.94P("3ormoreheads")=(35)×0.83×0.8×0.22+(41)×0.84×0.21+(55)*0.85*0.20=0.94

45. A random variable X is normal with mean 1020 and standard deviation 50. Calculate P(X>1200)
X\~N(1020,50)X\~N(1020,50) Our new quantile: z=1200-102050=3.6z=1200-102050=3.6 R code:
pnorm(3.6,lower.tail=F)
## [1] 0.0001591086

46. Consider the number of people that show up at a bus station is Poisson with mean 2.5/h. What is the probability that at most three people show up in a four hour period?
X\~Poisson(?=2.5×t)X\~Poisson(?=2.5×t)
R code:
ppois(3,lambda=2.5*4)
## [1] 0.01033605

47.You are running for office and your pollster polled hundred people. 56 of them claimed they will vote for you. Can you relax?
Quick:
- Intervals take the form p±z×1n×p×(1-p)-------------vp±z×1n×p×(1-p)
- We know that p(1-p)p(1-p) is maximized at 1212 and z=1.96z=1.96 is the relevant quantile for a 95% confidence interval
- So: p±1nvp±1n is a quick estimate for pp
- Here: 1100v=0.11100=0.1 so 95% of the intervals would be [46,66][46,66]
- It’s not enough!

48. Geiger counter records 100 radioactive decays in 5 minutes. Find an approximate 95% interval for the number of decays per hour.
•	Start by finding a 95% interval for radioactive decay in a 5 minutes period
•	The estimated standard deviation is 100---v=10100=10
•	So the interval is ?^±1.96×10=100±19.6?^±1.96×10=100±19.6
•	So, per hour: [964.8,1435.2][964.8,1435.2]

49. The homicide rate in Scotland fell last year to 99 from 115 the year before. Is this reported change really networthy?
•	Consider the homicides as independent; a Poisson distribution can be a reasonable model
•	95% interval for the true homicide rate is 115±2×115---v=115±22=[94,137]115±2×115=115±22=[94,137]
•	It’s not reasonable to conclude that there has been a reduction in the true rate

50.Consider influenza epidemics for two parent heterosexual families. Suppose that the probability is 17% that at least one of the parents has contracted the disease. The probability that the father has contracted influenza is 12% while the probability that both the mother and father have contracted the disease is 6%. What is the probability that the mother has contracted influenza?
•	P("MotherorFather")=P("Mother")+P("Father")-P("MotherandFather")P("MotherorFather")=P("Mother")+P("Father")-P("MotherandFather")
•	Hence: P("Mother")=0.17+0.06-0.12=0.11P("Mother")=0.17+0.06-0.12=0.11

Level-3

DATA SCIENCE

Advance level Q&A:
1. From the below given ‘diamonds’ dataset, extract only those rows where the ‘price’ value is greater than 1000 and the ‘cut’ is ideal.
 
First, we will load the ggplot2 package:
library(ggplot2)
Next, we will use the dplyr package:
library(dplyr)// It is based on the grammar of data manipulation.
To extract those particular records, use the below command:
diamonds %>% filter(price>1000 & cut==”Ideal”)-> diamonds_1000_idea
2. Make a scatter plot between ‘price’ and ‘carat’ using ggplot. ‘Price’ should be on y-axis, ’carat’ should be on x-axis, and the ‘color’ of the points should be determined by ‘cut.’
We will implement the scatter plot using ggplot.
The ggplot is based on the grammar of data visualization, and it helps us stack multiple layers on top of each other.
So, we will start with the data layer, and on top of the data layer we will stack the aesthetic layer. Finally, on top of the aesthetic layer we will stack the geometry layer.
Code:
>ggplot(data=diamonds, aes(x=caret, y=price, col=cut))+geom_point()
3. Introduce 25 percent missing values in this ‘iris’ datset and impute the ‘Sepal.Length’ column with ‘mean’ and the ‘Petal.Length’ column with ‘median.’
 
To introduce missing values, we will be using the missForest package:
library(missForest)
Using the prodNA function, we will be introducing 25 percent of missing values:
Iris.mis<-prodNA(iris,noNA=0.25)
For imputing the ‘Sepal.Length’ column with ‘mean’ and the ‘Petal.Length’ column with ‘median,’ we will be using the Hmisc package and the impute function:
library(Hmisc)
iris.mis$Sepal.Length<-with(iris.mis, impute(Sepal.Length,mean))
iris.mis$Petal.Length<-with(iris.mis, impute(Petal.Length,median))
4. What do you understand by linear regression?
Linear regression helps in understanding the linear relationship between the dependent and the independent variables.
Linear regression is a supervised learning algorithm, which helps in finding the linear relationship between two variables. One is the predictor or the independent variable and the other is the response or the dependent variable. In Linear Regression, we try to understand how the dependent variable changes w.r.t the independent variable.
If there is more than one independent variable, then it is called simple linear regression, and if there is more than one independent variable then it is known as multiple linear regression.
5. Implement simple linear regression in R on this ‘mtcars’ dataset, where the dependent variable is ‘mpg’ and the independent variable is ‘disp.’
 
Here, we need to find how ‘mpg’ varies w.r.t displacement of the column.
We need to divide this data into the training dataset and the testing dataset so that the model does not overfit the data.
So, what happens is when we do not divide the dataset into these two components, it overfits the dataset. Hence, when we add new data, it fails miserably on that new data.
Therefore, to divide this dataset, we would require the caret package. This caret package comprises the createdatapartition() function. This function will give the true or false labels.
Here, we will use the following code:
libraray(caret)

split_tag<-createDataPartition(mtcars$mpg, p=0.65, list=F)

mtcars[split_tag,]->train

mtcars[-split_tag,]->test

lm(mpg-data,data=train)->mod_mtcars

predict(mod_mtcars,newdata=test)->pred_mtcars

>head(pred_mtcars)
Explanation:
Parameters of the createDataPartition function: First is the column which determines the split (it is the mpg column).
Second is the split ratio which is 0.65, i.e., 65 percent of records will have true labels and 35 percent will have false labels. We will store this in split_tag object.
Once we have split_tag object ready, from this entire mtcars dataframe, we will select all those records where the split tag value is true and store those records in the training set.
Similarly, from the mtcars dataframe, we will select all those record where the split_tag value is falseand store those records in the test set.
So, the split tag will have true values in it, and when we put ‘-’ symbol in front of it, ‘-split_tag’ will contain all of the false labels. We will select all those records and store them in the test set.
We will go ahead and build a model on top of the training set, and for the simple linear model we will require the lm function.
lm(mpg-data,data=train)->mod_mtcars
Now, we have built the model on top of the train set. It’s time to predict the values on top of the test set. For that, we will use the predict function that takes in two parameters: first is the model which we have built and second is the dataframe on which we have to predict values.
Thus, we have to predict values for the test set and then store them in pred_mtcars.
predict(mod_mtcars,newdata=test)->pred_mtcars
Output:
These are the predicted values of mpg for all of these cars.
 
So, this is how we can build simple linear model on top of this mtcars dataset.
6. Calculate the RMSE values for the model built.
When we build a regression model, it predicts certain y values associated with the given x values, but there is always an error associated with this prediction. So, to get an estimate of the average error in prediction, RMSE is used.
Code:
cbind(Actual=test$mpg, predicted=pred_mtcars)->final_data

as.data.frame(final_data)->final_data

error<-(final_data$Actual-final_data$Prediction)

cbind(final_data,error)->final_data

sqrt(mean(final_data$error)^2)
Explanation:
We have the actual and the predicted values. We will bind both of them into a single dataframe. For that, we will use the cbind function:
cbind(Actual=test$mpg, predicted=pred_mtcars)->final_data
Our actual values are present in the mpg column from the test set, and our predicted values are stored in the pred_mtcars object which we have created in the previous question.
Hence, we will create this new column and name the column actual. Similarly, we will create another column and name it predicted which will have predicted values and then store the predicted values in the new object which is final_data.
After that, we will convert a matrix into a dataframe. So, we will use the as.data.frame function and convert this object (predicted values) into a dataframe:
as.data.frame(final_data)->final_data
We will pass this object which is final_data and store the result in final_data again.
We will then calculate the error in prediction for each of the records by subtracting the predicted values from the actual values:
error<-(final_data$Actual-final_data$Prediction)
Then, store this result on a new object and name that object as error.
After this, we will bind this error calculated to the same final_data dataframe:
cbind(final_data,error)->final_data //binding error object to this final_data
Here, we bind the error object to this final_data, and store this into final_data again.
Calculating RMSE:
Sqrt(mean(final_data$error)^2)
Output:
[1] 4.334423	

7. Implement simple linear regression in Python on this ‘Boston’ dataset where the dependent variable is ‘medv’ and the independent variable is ‘lstat.’
Simple Linear Regression
import pandas as pd

data=pd.read_csv(‘Boston.csv’)     //loading the Boston dataset

data.head()  //having a glance at the head of this data

data.shape

Let us take out the dependent and the independent variables from the dataset:
data1=data.loc[:,[‘lstat’,’medv’]]

data1.head()
Visualizing Variables
import matplotlib.pyplot as plt

data1.plot(x=’lstat’,y=’medv’,style=’o’)

plt.xlabel(‘lstat’)

plt.ylabel(‘medv’)

plt.show()
Here, ‘medv’ is basically the median values of the price of the houses, and we are trying to find out the median values of the price of the houses w.r.t to the lstat column.
We will separate the dependent and the independent variable from this entire dataframe:
data1=data.loc[:,[‘lstat’,’medv’]]
The only columns we want from all of this record are ‘lstat’ and ‘medv,’ and we need to store these results in data1.
Now, we would also do a visualization w.r.t to these two columns:
import matplotlib.pyplot as plt

data1.plot(x=’lstat’,y=’medv’,style=’o’)

plt.xlabel(‘lstat’)

plt.ylabel(‘medv’)

plt.show()
Preparing the Data
X=pd.Dataframe(data1[‘lstat’])

Y=pd.Dataframe(data1[‘medv’])

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)

from sklearn.linear_model import LinearRegression

regressor=LinearRegression()

regressor.fit(X_train,y_train)
print(regressor.intercept_)
Output :
34.12654201
print(regressor.coef_)//this is the slope
Output :
[[-0.913293]]
By now, we have built the model. Now, we have to predict the values on top of the test set:
y_pred=regressor.predict(X_test)//using the instance and the predict function and pass the X_test object inside the function and store this in y_pred object

Now, let’s have a glance at the rows and columns of the actual values and the predicted values:
Y_pred.shape, y_test.shape
Output :
((102,1),(102,1))
Further, we will go ahead and calculate some metrics so that we can find out the Mean Absolute Error, Mean Squared Error, and RMSE.
from sklearn import metrics import NumPy as np

print(‘Mean Absolute Error: ’, metrics.mean_absolute_error(y_test, y_pred))

print(‘Mean Squared Error: ’, metrics.mean_squared_error(y_test, y_pred))

print(‘Root Mean Squared Error: ’, np.sqrt(metrics.mean_absolute_error(y_test, y_pred))
Output:
Mean Absolute Error: 4.692198

Mean Squared Error: 43.9198

Root Mean Squared Error: 6.6270
8. What do you understand by logistic regression?
Logistic regression is a classification algorithm which can be used when the dependent variable is binary.
Let’s take an example.
Here, we are trying to determine whether it will rain or not on the basis of temperature and humidity.
 
Temperature and humidity are the independent variables, and rain would be our dependent variable.
So, logistic regression algorithm actually produces an S shape curve.
Now, let us look at another scenario:
Let’s suppose that x-axis represent the runs scored by Virat Kohli and y-axis represent the probability of team India winning the match. From this graph, we can say that if Virat Kohli scores more than 50 runs, then there is a greater probability for team India to win the match. Similarly, if he scores less than 50 runs then the probability of team India winning the match is less than 50 percent.
 
So, basically in logistic regression, the y value lies within the range of 0 and 1.
This is how logistic regression works.
9. Implement logistic regression on this ‘heart’ dataset in R where the dependent variable is ‘target’ and the independent variable is ‘age.’
 
For loading the dataset, we will use the read.csv function:
read.csv(“D:/heart.csv”)->heart

str(heart)
In the structure of this dataframe, most of the values are integers. However, since we are building a logistic regression model on top of this dataset, the final target column is supposed to be categorical. It cannot be an integer. So, we will go ahead and convert them into a factor.
Thus, we will use the as.factor function and convert these integer values into categorical data.
We will pass on heart$target column over here and store the result in heart$target as follows:
as.factor(heart$target)->heart$target
Now, we will build a logistic regression model and see the different probability values for the person to have heart disease on the basis of different age values.
To build a logistic regression model, we will use the glm function:
glm(target~age, data=heart, family=”binomial”)->log_mod1
Here, target~age indicates that the target is the dependent variable and the age is the independent variable, and we are building this model on top of the dataframe.
family=”binomial” means we are basically telling R that this is the logistic regression model, and we will store the result in log_mod1.
We will have a glance at the summary of the model that we have just built:
summary(log_mod1)
 
We can see Pr value here, and there are three stars associated with this Pr value. This basically means that we can reject the null hypothesis which states that there is no relationship between the age and the target columns. But since we have three stars over here, this null hypothesis can be rejected. There is a strong relationship between the age column and the target column.
Now, we have other parameters like null deviance and residual deviance. Lower the deviance value, the better the model.
This null deviance basically tells the deviance of the model, i.e., when we don’t have any independent variable and we are trying to predict the value of the target column with only the intercept. When that’s the case, the null deviance is 417.64.
Residual deviance is wherein we include the independent variables and try to predict the target columns. Hence, when we include the independent variable which is age, we see that the residual deviance drops. Initially, when there are no independent variables, the null deviance was 417. After we include the age column, we see that the null deviance is reduced to 401.
This basically means that there is a strong relationship between the age column and the target column and that is why the deviance is reduced.
As we have built the model, it’s time to predict some values:
predict(log_mod1, data.frame(age=30), type=”response”)

predict(log_mod1, data.frame(age=50), type=”response”)

predict(log_mod1, data.frame(age=29:77), type=”response”)

Now, we will divide this dataset into train and test sets and build a model on top of the train set and predict the values on top of the test set:
>library(caret)

Split_tag<- createDataPartition(heart$target, p=0.70, list=F)

heart[split_tag,]->train

heart[-split_tag,]->test

glm(target~age, data=train,family=”binomial”)->log_mod2

predict(log_mod2, newdata=test, type=”response”)->pred_heart

range(pred_heart)
10. What is a confusion matrix?
Confusion matrix is a table which is used to estimate the performance of a model. It tabulates the actual values and the predicted values in a 2×2 matrix.
 
True Positive (d): This denotes all of those records where the actual values are true and the predicted values are also true. So, these denote all of the true positives.
False Negative (c): This denotes all of those records where the actual values are true, but the predicted values are false.
False Positive (b): In this, the actual values are false, but the predicted values are true.
True Negative (a): Here, the actual values are false and the predicted values are also false.
So, if you want to get the correct values, then correct values would basically represent all of the true positives and the true negatives.
This is how confusion matrix works.
11. Build a confusion matrix for the model where the threshold value for the probability of predicted values is 0.6, and also find the accuracy of the model.
Accuracy is calculated as:
Accuracy = (True positives + true negatives)/(True positives+ true negatives + false positives + false negatives)
To build a confusion matrix in R, we will use the table function:
table(test$target,pred_heart>0.6)
Here, we are setting the probability threshold as 0.6. So, wherever the probability of pred_heart is greater than 0.6, it will be classified as 0, and wherever it is less than 0.6 it will be classified as 1.
Then, we calculate the accuracy by the formula for calculating Accuracy.
 
12. What do you understand by true positive rate and false positive rate?
True positive rate: In Machine Learning, true positives rates, which are also referred to as sensitivity or recall, are used to measure the percentage of actual positives which are correctly indentified.
Formula:  True Positive Rate = True Positives/Positives
False positive rate: False positive rate is basically the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positive) upon the total number of actual events.
Formula: False Positive Rate = False Positives/Negatives

13. What is ROC curve?
It stands for Receiver Operating Characteristic. It is basically a plot between a true positive rate and a false positive rate, and it helps us to find out the right tradeoff between the true positive rate and the false positive rate for different probability thresholds of the predicted values. So, the closer the curve to the upper left corner, the better the model is. In other words, whichever curve has greater area under it that would be the better model.
You can see this in the below graph:
 
 
14. Build an ROC curve for the model built.
The below code will help us in building the ROC curve:
library(ROCR)

prediction(pred_heart, test$target)-> roc_pred_heart

performance(roc_pred_heart, “tpr”, “fpr”)->roc_curve

plot(roc_curve, colorize=T)
Graph:
 
15. Build a logistic regression model on the ‘customer_churn’ dataset in Python. The dependent variable is ‘Churn’ and the independent variable is ‘MonthlyCharges.’ Find the log_loss of the model.
First, we will load the pandas dataframe and the customer_churn.csv file:
customer_churn=pd.read_csv(“customer_churn.csv”)
 
After loading this dataset, we can have a glance at the head of the dataset by using the following command:
customer_churn.head()
Now, we will separate the dependent and the independent variables into two separate objects:
x=pd.Dataframe(customer_churn[‘MonthlyCharges’])

y=customer_churn[‘ Churn’]

#Splitting the data into training and testing sets

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.3, random_state=0)
Now, we will see how to build the model and calculate log_loss.
from sklearn.linear_model, we have to import LogisticRegression

l=LogisticRegression()

l.fit(x_train,y_train)

y_pred=l.predict_proba(x_test)
As we are supposed to calculate the log_loss, we will import it from sklearn.metrics:
from sklearn.metrics import log_loss

print(log_loss(y_test,y_pred)//actual values are in y_test and predicted are in y_pred
Output:
0.5555020595194167

16. What do you understand by a decision tree?
A decision tree is a supervised learning algorithm that is used for both classification and regression. Hence, in this case, the dependent variable can be both a numerical value and a categorical value.
 
Here, each node denotes the test on an attribute, and each edge denotes the outcome of that attribute, and each leaf node holds the class label.
So, in this case, we have a series of test conditions which gives the final decision according to the condition.

17. Build a decision tree model on ‘Iris’ dataset where the dependent variable is ‘Species,’ and all other columns are independent variables. Find the accuracy of the model built.
 
To build a decision tree model, we will be loading the party package:
#party package

library(party)

#splitting the data

library(caret)

split_tag<-createDataPartition(iris$Species, p=0.65, list=F)

iris[split_tag,]->train

iris[~split_tag,]->test

#building model

mytree<-ctree(Species~.,train)
Now we will plot the model
plot(mytree)
Model:  

#predicting the values

predict(mytree,test,type=’response’)->mypred
After this, we will predict the confusion matrix and then calculate the accuracy using the table function:
table(test$Species, mypred)
 

18. What do you understand by a random forest model?
It combines multiple models together to get the final output or, to be more precise, it combines multiple decision trees together to get the final output. So, decision trees are the building blocks of the random forest model.  

19. Build a random forest model on top of this ‘CTG’ dataset, where ‘NSP’ is the dependent variable and all other columns are independent variables.
 
We will load the CTG dataset by using read.csv:
data<-read.csv(“C:/Users/intellipaat/Downloads/CTG.csv”,header=True)

str(data)
Converting the integer type to a factor
data$NSP<-as.factor(data$NSP)

table(data$NSP)

#data partition

set.seed(123)

split_tag<-createDataPartition(data$NSP, p=0.65, list=F)

data[split_tag,]->train

data[~split_tag,]->test

#random forest -1

library(randomForest)

set.seed(222)

rf<-randomForest(NSP~.,data=train)

rf

#prediction

predict(rf,test)->p1
Building confusion matrix and calculating accuracy:
table(test$NSP,p1)
 
20. How is Data modeling different from Database design?
Data Modeling: It can be considered as the first step towards the design of a database. Data modeling creates a conceptual model based on the relationship between various data models. The process involves moving from the conceptual stage to the logical model to the physical schema. It involves the systematic method of applying data modeling techniques.
Database Design: This is the process of designing the database. The database design creates an output which is a detailed data model of the database. Strictly speaking, database design includes the detailed logical model of a database but it can also include physical design choices and storage parameters.

21.In a study of emergency room waiting times, investigators consider a new and the standard triage systems. To test the systems, administrators selected 20 nights and randomly assigned the new triage system to be used on 10 nights and the standard system on the remaining 10 nights. They calculated the nightly median waiting time (MWT) to see a physician. The average MWT for the new system was 3 hours with a variance of 0.60 while the average MWT for the old system was 5 hours with a variance of 0.68. Consider the 95% confidence interval estimate for the differences of the mean MWT associated with the new system. Assume a constant variance. What is the interval? Subtract in this order (New System - Old System).
t confidence interval for the difference of the means assuming equal variances:
(new-old)±t'×sp×(1n1)2+(1n2)2-----------v(new-old)±t'×sp×(1n1)2+(1n2)2
•	t’: 97.5% quantile, with 20+10-2=28 degrees of freedom: 2.1
•	spsp: pooled variance, 0.62×9+0.682×910+10-2----------v=0.80.62×9+0.682×910+10-2=0.8
•	1/10+1/10----------v=0.441/10+1/10=0.44
•	We get [-2.75,-1.25][-2.75,-1.25]

22.To further test the hospital triage system, administrators selected 200 nights and randomly assigned a new triage system to be used on 100 nights and a standard system on the remaining 100 nights. They calculated the nightly median waiting time (MWT) to see a physician. The average MWT for the new system was 4 hours with a standard deviation of 0.5 hours while the average MWT for the old system was 6 hours with a standard deviation of 2 hours. Consider the hypothesis of a decrease in the mean MWT associated with the new treatment. What does the 95% independent group confidence interval with unequal variances suggest vis a vis this hypothesis? (Because there’s so many observations per group, just use the Z quantile instead of the T.)
•	ZZ confidence interval for the differences of the means assuming unequal variances: (new-old)±z'×sp×(1n1)2+(1n2)2-----------v(new-old)±z'×sp×(1n1)2+(1n2)2
•	Z97.5Z97.5 quantile
•	spsp: pooled variance, 0.52×99+22×99(100+100-2)----------v=1.4580.52×99+22×99(100+100-2)=1.458
•	we get: [1.6,2.4]

23. How to optimize algorithms? (parallel processing and/or faster algorithms). Provide examples for both
“Premature optimization is the root of all evil”; Donald Knuth
Parallel processing: for instance in R with a single machine.
- doParallel and foreach package
- doParallel: parallel backend, will select n-cores of the machine
- for each: assign tasks for each core
- using Hadoop on a single node
- using Hadoop on multi-node
Faster algorithm:
- In computer science: Pareto principle; 90% of the execution time is spent executing 10% of the code
- Data structure: affect performance
- Caching: avoid unnecessary work
- Improve source code level
For instance: on early C compilers, WHILE(something) was slower than FOR(;;), because WHILE evaluated “something” and then had a conditional jump which tested if it was true while FOR had unconditional jump.

24. What is the life cycle of a data science project ?
1.	Data acquisition
Acquiring data from both internal and external sources, including social media or web scraping. In a steady state, data extraction and routines should be in place, and new sources, once identified would be acquired following the established processes
2.	Data preparation
Also called data wrangling: cleaning the data and shaping it into a suitable form for later analyses. Involves exploratory data analysis and feature extraction.
3.	Hypothesis & modelling
Like in data mining but not with samples, with all the data instead. Applying machine learning techniques to all the data. A key sub-step: model selection. This involves preparing a training set for model candidates, and validation and test sets for comparing model performances, selecting the best performing model, gauging model accuracy and preventing overfitting
4.	Evaluation & interpretation
Steps 2 to 4 are repeated a number of times as needed; as the understanding of data and business becomes clearer and results from initial models and hypotheses are evaluated, further tweaks are performed. These may sometimes include step5 and be performed in a pre-production.
5.	Deployment
6.	Operations
Regular maintenance and operations. Includes performance tests to measure model performance, and can alert when performance goes beyond a certain acceptable threshold
7.	Optimization
Can be triggered by failing performance, or due to the need to add new data sources and retraining the model or even to deploy new versions of an improved model
Note: with increasing maturity and well-defined project goals, pre-defined performance can help evaluate feasibility of the data science project early enough in the data-science life cycle. This early comparison helps the team refine hypothesis, discard the project if non-viable, change approaches.

25.Imagine you have N pieces of rope in a bucket. You reach in and grab one end-piece, then reach in and grab another end-piece, and tie those two together. What is the expected value of the number of loops in the bucket?
•	There are nn entirely unattached pieces of rope in a bucket
•	A loop: any number of rope attached in a closed chain
•	Suppose the expected number of loops for n-1n-1 pieces of rope is denoted Ln-1Ln-1
•	Consider the bucket of nn pieces of rope; there are 2n2n rope ends
Pick an end of rope. Of the remaining 2n-12n-1 ends of rope, only one end creates a loop (the other end of the same piece of rope). There are then n-1n-1 untied pieces of rope. The rest of the time, two separates pieces of rope are tied together and there are effectively n-1n-1 untied pieces of rope. The recurrence is therefore:
•	Ln=12n-1+Ln-1Ln=12n-1+Ln-1
Clearly, L1=1L1=1 so:
•	Ln=?nk=112k-1=H2n-Hn2Ln=?k=1n12k-1=H2n-Hn2
•	Where HkHk is the kthkth harmonic number
Since Hk??+lnkHk??+ln?k for large-ish k, where gamma=0.57722. is the Euler-Mascheroni constant, we have:
•	Ln?ln(2n)-ln(n)2=ln2n--v
26.The probability that item an item at location A is 0.6, and 0.8 at location B. What is the probability that item would be found on Amazon website?
We need to make some assumptions about this question before we can answer it. Let’s assume that there are two possible places to purchase a particular item on Amazon and the probability of finding it at location A is 0.6 and B is 0.8. The probability of finding the item on Amazon can be explained as so:
We can reword the above as P(A) = 0.6 and P(B) = 0.8. Furthermore, let’s assume that these are independent events, meaning that the probability of one event is not impacted by the other. We can then use the formula…
P(A or B) = P(A) + P(B) — P(A and B)
P(A or B) = 0.6 + 0.8 — (0.6*0.8)
P(A or B) = 0.92

27. You randomly draw a coin from 100 coins — 1 unfair coin (head-head), 99 fair coins (head-tail) and roll it 10 times. If the result is 10 heads, what is the probability that the coin is unfair?
This can be answered using the Bayes Theorem. The extended equation for the Bayes Theorem is the following:

 
Assume that the probability of picking the unfair coin is denoted as P(A) and the probability of flipping 10 heads in a row is denoted as P(B). Then P(B|A) is equal to 1, P(B|¬A) is equal to 0.5¹°, and P(¬A) is equal to 0.99.
If you fill in the equation, then P(A|B) = 0.9118 or 91.18%.
28. Difference between convex and non-convex cost function; what does it mean when a cost function is non-convex?

 
Taken from Cho-Jui Hsieh, UCLA
A convex function is one where a line drawn between any two points on the graph lies on or above the graph. It has one minimum.
A non-convex function is one where a line drawn between any two points on the graph may intersect other points on the graph. It characterized as “wavy”.
When a cost function is non-convex, it means that there’s a likelihood that the function may find local minima instead of the global minimum, which is typically undesired in machine learning models from an optimization perspective.
29.Walk through the probability fundamentals
For this, I’m going to look at the eight rules of probability laid out here and the four different counting methods (see more here).
Eight rules of probability
•	Rule #1: For any event A, 0 = P(A) = 1; in other words, the probability of an event can range from 0 to 1.
•	Rule #2: The sum of the probabilities of all possible outcomes always equals 1.
•	Rule #3: P(not A) = 1 — P(A); This rule explains the relationship between the probability of an event and its complement event. A complement event is one that includes all possible outcomes that aren’t in A.
•	Rule #4: If A and B are disjoint events (mutually exclusive), then P(A or B) = P(A) + P(B); this is called the addition rule for disjoint events
•	Rule #5: P(A or B) = P(A) + P(B) — P(A and B); this is called the general addition rule.
•	Rule #6: If A and B are two independent events, then P(A and B) = P(A) * P(B); this is called the multiplication rule for independent events.
•	Rule #7: The conditional probability of event B given event A is P(B|A) = P(A and B) / P(A)
•	Rule #8: For any two events A and B, P(A and B) = P(A) * P(B|A); this is called the general multiplication rule
Counting Methods

 
Factorial Formula: n! = n x (n -1) x (n — 2) x … x 2 x 1
Use when the number of items is equal to the number of places available.
Eg. Find the total number of ways 5 people can sit in 5 empty seats.
= 5 x 4 x 3 x 2 x 1 = 120
Fundamental Counting Principle (multiplication)
This method should be used when repetitions are allowed and the number of ways to fill an open place is not affected by previous fills.
Eg. There are 3 types of breakfasts, 4 types of lunches, and 5 types of desserts. The total number of combinations is = 5 x 4 x 3 = 60
Permutations: P(n,r)= n! / (n-r)!
This method is used when replacements are not allowed and order of item ranking matters.
Eg. A code has 4 digits in a particular order and the digits range from 0 to 9. How many permutations are there if one digit can only be used once?
P(n,r) = 10!/(10–4)! = (10x9x8x7x6x5x4x3x2x1)/(6x5x4x3x2x1) = 5040
Combinations Formula: C(n,r)=(n!)/[(n-r)!r!]
This is used when replacements are not allowed and the order in which items are ranked does not mater.
Eg. To win the lottery, you must select the 5 correct numbers in any order from 1 to 52. What is the number of possible combinations?
C(n,r) = 52! / (52–5)!5! = 2,598,960
30. Describe Markov chains?

 
 “A Markov chain is a mathematical system that experiences transitions from one state to another according to certain probabilistic rules. The defining characteristic of a Markov chain is that no matter how the process arrived at its present state, the possible future states are fixed. In other words, the probability of transitioning to any particular state is dependent solely on the current state and time elapsed.”
31. A box has 12 red cards and 12 black cards. Another box has 24 red cards and 24 black cards. You want to draw two cards at random from one of the two boxes, one card at a time. Which box has a higher probability of getting cards of the same color and why?
The box with 24 red cards and 24 black cards has a higher probability of getting two cards of the same color. Let’s walk through each step.
Let’s say the first card you draw from each deck is a red Ace.
This means that in the deck with 12 reds and 12 blacks, there’s now 11 reds and 12 blacks. Therefore your odds of drawing another red are equal to 11/(11+12) or 11/23.
In the deck with 24 reds and 24 blacks, there would then be 23 reds and 24 blacks. Therefore your odds of drawing another red are equal to 23/(23+24) or 23/47.
Since 23/47 > 11/23, the second deck with more cards has a higher probability of getting the same two cards.
32. You are at a Casino and have two dices to play with. You win $10 every time you roll a 5. If you play till you win and then stop, what is the expected payout?

 
•	Let’s assume that it costs $5 every time you want to play.
•	There are 36 possible combinations with two dice.
•	Of the 36 combinations, there are 4 combinations that result in rolling a five (see blue). This means that there is a 4/36 or 1/9 chance of rolling a 5.
•	A 1/9 chance of winning means you’ll lose eight times and win once (theoretically).
•	Therefore, your expected payout is equal to $10.00 * 1 — $5.00 * 9= -$35.00.
33. How can you tell if a given coin is biased?
This isn’t a trick question. The answer is simply to perform a hypothesis test:
1.	The null hypothesis is that the coin is not biased and the probability of flipping heads should equal 50% (p=0.5). The alternative hypothesis is that the coin is biased and p != 0.5.
2.	Flip the coin 500 times.
3.	Calculate Z-score (if the sample is less than 30, you would calculate the t-statistics).
4.	Compare against alpha (two-tailed test so 0.05/2 = 0.025).
5.	If p-value > alpha, the null is not rejected and the coin is not biased.
If p-value < alpha, the null is rejected and the coin is biased.
34. Make an unfair coin fair
Since a coin flip is a binary outcome, you can make an unfair coin fair by flipping it twice. If you flip it twice, there are two outcomes that you can bet on: heads followed by tails or tails followed by heads.
P(heads) * P(tails) = P(tails) * P(heads)
This makes sense since each coin toss is an independent event. This means that if you get heads ? heads or tails ? tails, you would need to reflip the coin.
35.You are about to get on a plane to London, you want to know whether you have to bring an umbrella or not. You call three of your random friends and ask each one of them if it’s raining. The probability that your friend is telling the truth is 2/3 and the probability that they are playing a prank on you by lying is 1/3. If all 3 of them tell that it is raining, then what is the probability that it is actually raining in London.
You can tell that this question is related to Bayesian theory because of the last statement which essentially follows the structure, “What is the probability A is true given B is true?” Therefore we need to know the probability of it raining in London on a given day. Let’s assume it’s 25%.
P(A) = probability of it raining = 25%
P(B) = probability of all 3 friends say that it’s raining
P(A|B) probability that it’s raining given they’re telling that it is raining
P(B|A) probability that all 3 friends say that it’s raining given it’s raining = (2/3)³ = 8/27
Step 1: Solve for P(B)
P(A|B) = P(B|A) * P(A) / P(B), can be rewritten as
P(B) = P(B|A) * P(A) + P(B|not A) * P(not A)
P(B) = (2/3)³ * 0.25 + (1/3)³ * 0.75 = 0.25*8/27 + 0.75*1/27
Step 2: Solve for P(A|B)
P(A|B) = 0.25 * (8/27) / ( 0.25*8/27 + 0.75*1/27)
P(A|B) = 8 / (8 + 3) = 8/11
Therefore, if all three friends say that it’s raining, then there’s an 8/11 chance that it’s actually raining.
36. You are given 40 cards with four different colors- 10 Green cards, 10 Red Cards, 10 Blue cards, and 10 Yellow cards. The cards of each color are numbered from one to ten. Two cards are picked at random. Find out the probability that the cards picked are not of the same number and same color.
Since these events are not independent, we can use the rule:
P(A and B) = P(A) * P(B|A) ,which is also equal to
P(not A and not B) = P(not A) * P(not B | not A)
For example:
P(not 4 and not yellow) = P(not 4) * P(not yellow | not 4)
P(not 4 and not yellow) = (36/39) * (27/36)
P(not 4 and not yellow) = 0.692
Therefore, the probability that the cards picked are not the same number and the same color is 69.2%.
37. How do you assess the statistical significance of an insight?
You would perform hypothesis testing to determine statistical significance. First, you would state the null hypothesis and alternative hypothesis. Second, you would calculate the p-value, the probability of obtaining the observed results of a test assuming that the null hypothesis is true. Last, you would set the level of the significance (alpha) and if the p-value is less than the alpha, you would reject the null — in other words, the result is statistically significant.
38. Explain what a long-tailed distribution is and provide three examples of relevant phenomena that have long tails. Why are they important in classification and regression problems?



 
Example of a long tail distribution
A long-tailed distribution is a type of heavy-tailed distribution that has a tail (or tails) that drop off gradually and asymptotically.
3 practical examples include the power law, the Pareto principle (more commonly known as the 80–20 rule), and product sales (i.e. best selling products vs others).
It’s important to be mindful of long-tailed distributions in classification and regression problems because the least frequently occurring values make up the majority of the population. This can ultimately change the way that you deal with outliers, and it also conflicts with some machine learning techniques with the assumption that the data is normally distributed.
39. What is the Central Limit Theorem? Explain it. Why is it important?

 
From Wikipedia
Statistics How To provides the best definition of CLT, which is:
“The central limit theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size gets larger no matter what the shape of the population distribution.” [1]
The central limit theorem is important because it is used in hypothesis testing and also to calculate confidence intervals.
40. What is the statistical power?
‘Statistical power’ refers to the power of a binary hypothesis, which is the probability that the test rejects the null hypothesis given that the alternative hypothesis is true. [2]

 
41. Explain selection bias (with regard to a dataset, not variable selection). Why is it important? How can data management procedures such as missing data handling make it worse?
Selection bias is the phenomenon of selecting individuals, groups or data for analysis in such a way that proper randomization is not achieved, ultimately resulting in a sample that is not representative of the population.
Understanding and identifying selection bias is important because it can significantly skew results and provide false insights about a particular population group.
Types of selection bias include:
•	sampling bias: a biased sample caused by non-random sampling
•	time interval: selecting a specific time frame that supports the desired conclusion. e.g. conducting a sales analysis near Christmas.
•	exposure: includes clinical susceptibility bias, protopathic bias, indication bias. Read more here.
•	data: includes cherry-picking, suppressing evidence, and the fallacy of incomplete evidence.
•	attrition: attrition bias is similar to survivorship bias, where only those that ‘survived’ a long process are included in an analysis, or failure bias, where those that ‘failed’ are only included
•	observer selection: related to the Anthropic principle, which is a philosophical consideration that any data we collect about the universe is filtered by the fact that, in order for it to be observable, it must be compatible with the conscious and sapient life that observes it. [3]
Handling missing data can make selection bias worse because different methods impact the data in different ways. For example, if you replace null values with the mean of the data, you adding bias in the sense that you’re assuming that the data is not as spread out as it might actually be.
42.Provide a simple example of how an experimental design can help answer a question about behavior. How does experimental data contrast with observational data?
Observational data comes from observational studies which are when you observe certain variables and try to determine if there is any correlation.
Experimental data comes from experimental studies which are when you control certain variables and hold them constant to determine if there is any causality.
An example of experimental design is the following: split a group up into two. The control group lives their lives normally. The test group is told to drink a glass of wine every night for 30 days. Then research can be conducted to see how wine affects sleep.
43. Is mean imputation of missing data acceptable practice? Why or why not?
Mean imputation is the practice of replacing null values in a data set with the mean of the data.
Mean imputation is generally bad practice because it doesn’t take into account feature correlation. For example, imagine we have a table showing age and fitness score and imagine that an eighty-year-old has a missing fitness score. If we took the average fitness score from an age range of 15 to 80, then the eighty-year-old will appear to have a much higher fitness score that he actually should.
Second, mean imputation reduces the variance of the data and increases bias in our data. This leads to a less accurate model and a narrower confidence interval due to a smaller variance.
44.What is an outlier? Explain how you might screen for outliers and what would you do if you found them in your dataset. Also, explain what an inlier is and how you might screen for them and what would you do if you found them in your dataset.
An outlier is a data point that differs significantly from other observations.
Depending on the cause of the outlier, they can be bad from a machine learning perspective because they can worsen the accuracy of a model. If the outlier is caused by a measurement error, it’s important to remove them from the dataset. There are a couple of ways to identify outliers:
Z-score/standard deviations: if we know that 99.7% of data in a data set lie within three standard deviations, then we can calculate the size of one standard deviation, multiply it by 3, and identify the data points that are outside of this range. Likewise, we can calculate the z-score of a given point, and if it’s equal to +/- 3, then it’s an outlier.
Note: that there are a few contingencies that need to be considered when using this method; the data must be normally distributed, this is not applicable for small data sets, and the presence of too many outliers can throw off z-score.

 
Interquartile Range (IQR): IQR, the concept used to build boxplots, can also be used to identify outliers. The IQR is equal to the difference between the 3rd quartile and the 1st quartile. You can then identify if a point is an outlier if it is less than Q1–1.5*IRQ or greater than Q3 + 1.5*IQR. This comes to approximately 2.698 standard deviations.

 
Photo from Michael Galarnyk
Other methods include DBScan clustering, Isolation Forests, and Robust Random Cut Forests.
An inlier is a data observation that lies within the rest of the dataset and is unusual or an error. Since it lies in the dataset, it is typically harder to identify than an outlier and requires external data to identify them. Should you identify any inliers, you can simply remove them from the dataset to address them.
45. How do you handle missing data? What imputation techniques do you recommend?
There are several ways to handle missing data:
•	Delete rows with missing data
•	Mean/Median/Mode imputation
•	Assigning a unique value
•	Predicting the missing values
•	Using an algorithm which supports missing values, like random forests
The best method is to delete rows with missing data as it ensures that no bias or variance is added or removed, and ultimately results in a robust and accurate model. However, this is only recommended if there’s a lot of data to start with and the percentage of missing values is low.
46. You have data on the duration of calls to a call center. Generate a plan for how you would code and analyze these data. Explain a plausible scenario for what the distribution of these durations might look like. How could you test, even graphically, whether your expectations are borne out?
First I would conduct EDA — Exploratory Data Analysis to clean, explore, and understand my data. See my article on EDA here. As part of my EDA, I could compose a histogram of the duration of calls to see the underlying distribution.
My guess is that the duration of calls would follow a lognormal distribution (see below). The reason that I believe it’s positively skewed is because the lower end is limited to 0 since a call can’t be negative seconds. However, on the upper end, it’s likely for there to be a small proportion of calls that are extremely long relatively.

 
Lognormal Distribution Example
You could use a QQ plot to confirm whether the duration of calls follows a lognormal distribution or not
47.Explain likely differences between administrative datasets and datasets gathered from experimental studies. What are likely problems encountered with administrative data? How do experimental methods help alleviate these problems? What problem do they bring?
Administrative datasets are typically datasets used by governments or other organizations for non-statistical reasons.
Administrative datasets are usually larger and more cost-efficient than experimental studies. They are also regularly updated assuming that the organization associated with the administrative dataset is active and functioning. At the same time, administrative datasets may not capture all of the data that one may want and may not be in the desired format either. It is also prone to quality issues and missing entries.
48.You are compiling a report for user content uploaded every month and notice a spike in uploads in October. In particular, a spike in picture uploads. What might you think is the cause of this, and how would you test it?
There are a number of potential reasons for a spike in photo uploads:
1.	A new feature may have been implemented in October which involves uploading photos and gained a lot of traction by users. For example, a feature that gives the ability to create photo albums.
2.	Similarly, it’s possible that the process of uploading photos before was not intuitive and was improved in the month of October.
3.	There may have been a viral social media movement that involved uploading photos that lasted for all of October. Eg. Movember but something more scalable.
4.	It’s possible that the spike is due to people posting pictures of themselves in costumes for Halloween.
The method of testing depends on the cause of the spike, but you would conduct hypothesis testing to determine if the inferred cause is the actual cause.
49.Give examples of data that does not have a Gaussian distribution, nor log-normal.
•	Any type of categorical data won’t have a gaussian distribution or lognormal distribution.
•	Exponential distributions — eg. the amount of time that a car battery lasts or the amount of time until an earthquake occurs.
50.What is root cause analysis? How to identify a cause vs. a correlation? Give examples
Root cause analysis: a method of problem-solving used for identifying the root cause(s) of a problem [5]
Correlation measures the relationship between two variables, range from -1 to 1. Causation is when a first event appears to have caused a second event. Causation essentially looks at direct relationships while correlation can look at both direct and indirect relationships.
Example: a higher crime rate is associated with higher sales in ice cream in Canada, aka they are positively correlated. However, this doesn’t mean that one causes another. Instead, it’s because both occur more when it’s warmer outside.
You can test for causation using hypothesis testing or A/B testing.
51.Give an example where the median is a better measure than the mean
When there are a number of outliers that positively or negatively skew the data.
52.Given two fair dices, what is the probability of getting scores that sum to 4? to 8?
There are 4 combinations of rolling a 4 (1+3, 3+1, 2+2):
P(rolling a 4) = 3/36 = 1/12
There are combinations of rolling an 8 (2+6, 6+2, 3+5, 5+3, 4+4):
P(rolling an 8) = 5/36
53. What is the Law of Large Numbers?
The Law of Large Numbers is a theory that states that as the number of trials increases, the average of the result will become closer to the expected value.
Eg. flipping heads from fair coin 100,000 times should be closer to 0.5 than 100 times.
54. How do you calculate the needed sample size?

 
Formula for margin of error
You can use the margin of error (ME) formula to determine the desired sample size.
•	t/z = t/z score used to calculate the confidence interval
•	ME = the desired margin of error
•	S = sample standard deviation
55. When you sample, what bias are you inflicting?
Potential biases include the following:
•	Sampling bias: a biased sample caused by non-random sampling
•	Under coverage bias: sampling too few observations
•	Survivorship bias: error of overlooking observations that did not make it past a form of selection process.

56. How can you avoid overfitting?
Overfitting happens when a machine has an inadequate dataset and it tries to learn from it. So, overfitting is inversely proportional to the amount of data.
For small databases, we can bypass overfitting by the cross-validation method. In this approach, we will divide the dataset into two sections. These two sections will comprise testing and training sets. To train the model, we will use the training dataset and, for testing the model for new inputs, we will use the testing dataset.
This is how we can avoid overfitting.
57. Why do we need a validation set and a test set?
We split the data into three different categories while creating a model:
1.	Training set: We use the training set for building the model and adjusting the model’s variables. But, we cannot rely on the correctness of the model build on top of the training set. The model might give incorrect outputs on feeding new inputs.
2.	Validation set: We use a validation set to look into the model’s response on top of the samples that don’t exist in the training dataset. Then, we will tune hyperparameters on the basis of the estimated benchmark of the validation data.
When we are evaluating the model’s response using the validation set, we are indirectly training the model with the validation set. This may lead to the overfitting of the model to specific data. So, this model won’t be strong enough to give the desired response to the real-world data.
3.	Test set: The test dataset is the subset of the actual dataset, which is not yet used to train the model. The model is unaware of this dataset. So, by using the test dataset, we can compute the response of the created model on hidden data. We evaluate the model’s performance on the basis of the test dataset.
Note: We always expose the model to the test dataset after tuning the hyperparameters on top of the validation set.
As we know, the evaluation of the model on the basis of the validation set would not be enough. Thus, we use a test set for computing the efficiency of the model.
58. What is a Decision Tree?
A decision tree is used to explain the sequence of actions that must be performed to get the desired output. It is a hierarchical diagram that shows the actions.
 
We can create an algorithm for a decision tree on the basis of the hierarchy of actions that we have set.
In the above decision tree diagram, we have made a sequence of actions for driving a vehicle with/without a license.
59. Explain the difference between KNN and K-means Clustering.
K-nearest neighbors: It is a supervised Machine Learning algorithm. In KNN, we give the identified (labeled) data to the model. Then, the model matches the points based on the distance from the closest points.
 
K-means clustering: It is an unsupervised Machine Learning algorithm. In this, we give the unidentified (unlabeled) data to the model. Then, the algorithm creates batches of points based on the average of the distances between distinct points.
 
60. What is Dimensionality Reduction?
In the real world, we build Machine Learning models on top of features and parameters. These features can be multi-dimensional and large in number. Sometimes, the features may be irrelevant and it becomes a difficult task to visualize them.
Here, we use dimensionality reduction to cut down the irrelevant and redundant features with the help of principal variables. These principal variables are the subgroup of the parent variables that conserve the feature of the parent variables.

61. Both being tree-based algorithms, how is Random Forest different from Gradient Boosting Algorithm (GBM)?
The main difference between a random forest and GBM is the use of techniques. Random forest advances predictions using a technique called ‘bagging.’ On the other hand, GBM advances predictions with the help of a technique called ‘boosting.’
•	Bagging: In bagging, we apply arbitrary sampling and we divide the dataset into N After that, we build a model by employing a single training algorithm. Following, we combine the final predictions by polling. Bagging helps increase the efficiency of the model by decreasing the variance to eschew overfitting.
•	Boosting: In boosting, the algorithm tries to review and correct the inadmissible predictions at the initial iteration. After that, the algorithm’s sequence of iterations for correction continues until we get the desired prediction. Boosting assists in reducing bias and variance, both, for making the weak learners strong.
62. Suppose, you found that your model is suffering from high variance. Which algorithm do you think could handle this situation and why?
Handling High Variance
•	For handling issues of high variance, we should use the bagging algorithm.
•	Bagging algorithm would split data into sub-groups with replicated sampling of random data.
•	Once the algorithm splits the data, we use random data to create rules using a particular training algorithm.
•	After that, we use polling for combining the predictions of the model.
63. What is ROC curve and what does it represent?
ROC stands for ‘Receiver Operating Characteristic.’ We use ROC curves to represent the trade-off between True and False positive rates, graphically.
In ROC, AUC (Area Under the Curve) gives us an idea about the accuracy of the model.
 
The above graph shows an ROC curve. Greater the Area Under the Curve better the performance of the model.
Next, we would be looking at Machine Learning Interview Questions on Rescaling, Binarizing, and Standardizing.
64. What is Rescaling of data and how is it done?
In real-world scenarios, the attributes present in data will be in a varying pattern. So, rescaling of the characteristics to a common scale gives benefit to algorithms to process the data efficiently.
We can rescale the data using Scikit-learn. The code for rescaling the data using MinMaxScaler is as follows:
#Rescaling data
import pandas
import scipy
import numpy
from sklearn.preprocessing import MinMaxScaler
names = ['Abhi', 'Piyush', 'Pranay', 'Sourav', 'Sid', 'Mike', 'pedi', 'Jack', 'Tim']
Dataframe = pandas.read_csv(url, names=names)
Array = dataframe.values
# Splitting the array into input and output
X = array[:,0:8]
Y = array[:,8]
Scaler = MinMaxScaler(feature_range=(0, 1))
rescaledX = scaler.fit_transform(X)
# Summarizing the modified data
numpy.set_printoptions(precision=3)
print(rescaledX[0:5,:])
65. What is Binarizing of data? How to Binarize?
In most of the Machine Learning Interviews, apart from theoretical questions, interviewers focus on the implementation part. So, this ML Interview Questions in focused on the implementation of the theoretical concepts.
Converting data into binary values on the basis of threshold values is known as the binarizing of data. The values that are less than the threshold are set to 0 and the values that are greater than the threshold are set to 1. This process is useful when we have to perform feature engineering, and we can also use it for adding unique features.
We can binarize data using Scikit-learn. The code for binarizing the data using Binarizer is as follows:
from sklearn.preprocessing import Binarizer
import pandas
import numpy
names = ['Abhi', 'Piyush', 'Pranay', 'Sourav', 'Sid', 'Mike', 'pedi', 'Jack', 'Tim']
dataframe = pandas.read_csv(url, names=names)
array = dataframe.values
# Splitting the array into input and output
X = array[:,0:8]
Y = array[:,8]
binarizer = Binarizer(threshold=0.0).fit(X)
binaryX = binarizer.transform(X)
# Summarizing the modified data
numpy.set_printoptions(precision=3)
print(binaryX[0:5,:])
66. How to Standardize data?
Standardization is the method that is used for rescaling data attributes. The attributes would likely have a value of mean as 0 and the value of standard deviation as 1. The main objective of standardization is to prompt the mean and standard deviation for the attributes.
We can standardize the data using Scikit-learn. The code for standardizing the data using StandardScaler is as follows:
# Python code to Standardize data (0 mean, 1 stdev)
from sklearn.preprocessing import StandardScaler
import pandas
import numpy
names = ['Abhi', 'Piyush', 'Pranay', 'Sourav', 'Sid', 'Mike', 'pedi', 'Jack', 'Tim']
dataframe = pandas.read_csv(url, names=names)
array = dataframe.values
# Separate the array into input and output components
X = array[:,0:8]
Y = array[:,8]
scaler = StandardScaler().fit(X)
rescaledX = scaler.transform(X)
# Summarize the transformed data
numpy.set_printoptions(precision=3)
print(rescaledX[0:5,:])
67. Executing a binary classification tree algorithm is a simple task. But, how does a tree splitting take place? How does the tree determine which variable to break at the root node and which at its child nodes?
Gini index and Node Entropy assist the binary classification tree to take decisions. Basically, the tree algorithm determines the feasible feature that is used to distribute data into the most genuine child nodes.
According to Gini index, if we arbitrarily pick a pair of objects from a group, then they should be of identical class and the possibility for this event should be 1.
To compute the Gini index, we should do the following:
1.	Compute Gini for sub-nodes with the formula: The sum of the square of probability for success and failure (p^2 + q^2)
2.	Compute Gini for split by weighted Gini rate of every node of the split
Now, Entropy is the degree of indecency that is given by the following:
where a and b are the probabilities of success and failure of the node
When Entropy = 0, the node is homogenous
When Entropy is high, both groups are present at 50–50 percent in the node.
Finally, to determine the suitability of the node as a root node, the entropy should be very low.
68. What is SVM (Support Vector Machines)?
SVM is a Machine Learning algorithm that is majorly used for classification. It is used on top of the high dimensionality of the characteristic vector.
Below is the code for the SVM classifier:
# Introducing required libraries
from sklearn import datasets
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
# Stacking the Iris dataset
iris = datasets.load_iris()
# A -> features and B -> label
A = iris.data
B = iris.target
# Breaking A and B into train and test data
A_train, A_test, B_train, B_test = train_test_split(A, B, random_state = 0)
# Training a linear SVM classifier
from sklearn.svm import SVC
svm_model_linear = SVC(kernel = 'linear', C = 1).fit(A_train, B_train)
svm_predictions = svm_model_linear.predict(A_test)
# Model accuracy for A_test
accuracy = svm_model_linear.score(A_test, B_test)
# Creating a confusion matrix
cm = confusion_matrix(B_test, svm_predictions)
69. Implement the KNN classification algorithm.
We will use the Iris dataset for implementing the KNN classification algorithm.
# KNN classification algorithm
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier
import numpy as np
from sklearn.model_selection import train_test_split
iris_dataset=load_iris()
A_train, A_test, B_train, B_test = train_test_split(iris_dataset["data"], iris_dataset["target"], random_state=0)
kn = KNeighborsClassifier(n_neighbors=1) 
kn.fit(A_train, B_train)
A_new = np.array([[8, 2.5, 1, 1.2]])
prediction = kn.predict(A_new)
print("Predicted target value: {}\n".format(prediction))
print("Predicted feature name: {}\n".format
(iris_dataset["target_names"][prediction]))
print("Test score: {:.2f}".format(kn.score(A_test, B_test)))
Output:
Predicted Target Name: [0]
Predicted Feature Name: [‘ Setosa’]
Test Score: 0.92








